{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "asa-kyn-001494-dev-eus-001"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"AzureDataLakeStorage2_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage2'"
		},
		"AzureSqlDatabase1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase1'"
		},
		"asa-kyn-001494-dev-eus-001-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'asa-kyn-001494-dev-eus-001-WorkspaceDefaultSqlServer'"
		},
		"asdb_etlhub_confirmed_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'asdb_etlhub_confirmed'"
		},
		"linkedService1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'linkedService1'"
		},
		"ln_rfs_pgmp_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ln_rfs_pgmp'"
		},
		"ls_adls_project_dimension_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'ls_adls_project_dimension'"
		},
		"ls_db2_esa_kyndryl_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ls_db2_esa_kyndryl'"
		},
		"ls_pgmp_rfs_db_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ls_pgmp_rfs_db'"
		},
		"tgt_geo_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'tgt_geo'"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adlskyn001494deveus001.dfs.core.windows.net"
		},
		"AzureDataLakeStorage2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adls4fsoetlhubdevuseast.dfs.core.windows.net"
		},
		"asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adls4fsoetlhubdevuseast.dfs.core.windows.net"
		},
		"ls_adls_project_dimension_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adls4fsoetlhubdevuseast.dfs.core.windows.net"
		},
		"tgt_geo_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adls4fsoetlhubdevuseast.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/BAL_GEO_DIM001')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "TMF_GEO",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "with\n\t-- This sub query gets the limits of the data that will be processed\n\tFIL as (\n\t\tselect\n\t\t\tETL_EXCTN_ID\n\t\tfrom\n\t\t\tPGMPDM.[ZAUX_ETL_EXCTN]\n\t\twhere\n\t\t\tIS_CURR_IND = 'Y'\n\t),\n\t\n\t-- This subquery gets the JOB ID. Returns -1 is the value is unknown\n\tJOB as (\n\t\tselect\n\t\t\tcoalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\n\t\tfrom\n\t\t\tPGMPDM.ZAUX_ETL_JOBS\n\t\twhere\n\t\t\tETL_JOB_NM = '#DSJobName#'\t\t\n\t),\n\t\n\t-- This subquery gets the SRC_SYS_ID. Returns -1 is the value is unknown\n\tSYS as (\n\t\tselect\n\t\t\tcoalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\n\t\tfrom\n\t\t\tPGMPDM.SRC_SYS_DIM\n\t\twhere\n\t\t\tSRC_SYS_CD = 'TMF'\t\t\t\n\t)\n\t\n\t\n\n\nselect\n\tG.GEO_DIM_UID,\n\tG.IOT_RGN_CD,\n\tG.IOT_RGN_NM,\n\tG.IMT_RGN_CD,\n\tG.IMT_SRGN_NM,\n\tG.SRGN_CD,\n\tG.SRGNCTRY_CD,\n\tG.SRGNCTRY_NM,\n\tG.IOTSORT_NUM,\n\tG.GEO_CD,\n\tG.GEO_TYP_CD,\n\tG.GMR_RGN_CD,\n\tG.GMR_RGN_NM,\n\tG.IOT_SHORT_NM,\n\tG.IMT_SHORT_NM,\n\tJOB.ETL_JOB_ID,\n\tFIL.ETL_EXCTN_ID,\n\tSYS.SRC_SYS_DIM_UID\nfrom\n\tPGMPDM.GEO_DIM_TMF G\n\tleft join\n\tJOB\n\ton 1 = 1\n\tleft join\n\tFIL\n\ton 1 = 1\n\tleft join\n\tSYS\n\ton 1 = 1",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "AzureSqlTable3",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "dl_tgt_geo",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-06-08T10:03:41Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AzureSqlTable3')]",
				"[concat(variables('workspaceId'), '/datasets/dl_tgt_geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CP_SUMMRY_SEQ')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "CP_CNTRCT_CHNG_RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "CP_CNTRCT_CHNG_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"Srccntrchnagrpt": {},
									"tgtcntrctchngrpt": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "PGMP"
				},
				"annotations": [],
				"lastPublishTime": "2022-06-16T16:51:44Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/CP_CNTRCT_CHNG_RPT')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PgMP_Dimension_Pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "GEO DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Df_BAL0001_GEO_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcgeo": {},
									"srtdtgtgeolkp": {},
									"Tgtupdgeodimtable": {},
									"InsTgtGeoDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "ACCTRPTS DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "GEO DIM",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0010_ACCTRPTS_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "ACCTSPFDIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "ACCTRPTS DIM",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0020ACCTSPF_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"appfunacctspcf": {},
									"LkpTgtAcctspcfDim": {},
									"TgtAcctspfDimupd": {},
									"TgtpgmdmacctspfDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "PGMP"
				},
				"annotations": [],
				"lastPublishTime": "2022-06-18T13:17:10Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Df_BAL0001_GEO_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0010_ACCTRPTS_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0020ACCTSPF_DIM')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook_R",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook_R",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					},
					{
						"name": "Notebook_R_copy1",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Notebook_R",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook_R",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-05-23T06:29:14Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Notebook_R')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline _Df_GEO_DIM')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "pipeline_Geo_dim",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Df_BAL0001_GEO_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcgeo": {},
									"srtdtgtgeolkp": {},
									"Tgtupdgeodimtable": {},
									"InsTgtGeoDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "PGMP"
				},
				"annotations": [],
				"lastPublishTime": "2022-06-13T16:27:15Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Df_BAL0001_GEO_DIM')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PipelineTest')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "select * from DBXDH.DHT_CUSTOMER\nwhere CUSTOMER_NO='9549530'",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "SqlPoolSink",
								"preCopyScript": "DELETE FROM DBXDH.DHT_CUSTOMER WHERE CUSTOMER_NO='9549530'",
								"allowCopyCommand": true,
								"copyCommandSettings": {}
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "linkedService1",
									"type": "LinkedServiceReference"
								}
							}
						},
						"inputs": [
							{
								"referenceName": "AzureSqlTable2",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "SqlPoolTable1",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-05-16T11:57:44Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AzureSqlTable2')]",
				"[concat(variables('workspaceId'), '/datasets/SqlPoolTable1')]",
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sample_pl')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow_copy1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-04-28T05:18:48Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Dataflow_copy1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sample_pl_sell_cycle')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_SELL_CYCLE",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-04-29T09:42:05Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/df_SELL_CYCLE')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/customer_dimension_load')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "scdType2_Generict_Dimension_Load",
								"type": "DataFlowReference",
								"parameters": {
									"NaturalKey": {
										"value": "'@{pipeline().parameters.NaturalKey}'",
										"type": "Expression"
									},
									"NonkeyColumns": {
										"value": "'@{pipeline().parameters.NonKeyColumns}'",
										"type": "Expression"
									},
									"EXTRACT_DT": "currentTimestamp()",
									"REC_START_DT": "currentTimestamp()",
									"SurrogateKey": {
										"value": "'@{pipeline().parameters.Key}'",
										"type": "Expression"
									},
									"DimTableName": "'DHT_CUSTOMER'"
								},
								"datasetParameters": {
									"genericInput": {
										"FolderName": {
											"value": "@pipeline().parameters.folder_name",
											"type": "Expression"
										},
										"fileName": {
											"value": "@pipeline().parameters.file_name",
											"type": "Expression"
										}
									},
									"genericDimensionTable": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4ChangedRecords": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Insert4ChangedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"SinkInsert4NewRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4SoftDeletedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"NaturalKey": {
						"type": "string",
						"defaultValue": "CUSTOMER_NO"
					},
					"NonKeyColumns": {
						"type": "string",
						"defaultValue": "FINANCIAL_COUNTRY_CD,CUSTOMER_DESC,GBG_ID"
					},
					"Key": {
						"type": "string",
						"defaultValue": "CUSTOMER_KEY"
					},
					"DimTableName": {
						"type": "string",
						"defaultValue": "DHT_CUSTOMER"
					},
					"folder_name": {
						"type": "string",
						"defaultValue": "customer"
					},
					"file_name": {
						"type": "string",
						"defaultValue": "customer_data.csv"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-05-16T11:43:55Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/scdType2_Generict_Dimension_Load')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/customer_load_woversion')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "scdType2_Generict_Dimension_Load_WOVersion",
								"type": "DataFlowReference",
								"parameters": {
									"NaturalKey": "'CUSTOMER_NO'",
									"NonkeyColumns": "'FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME'",
									"EXTRACT_DT": "currentTimestamp()",
									"REC_START_DT": "currentTimestamp()",
									"DimTableName": "'DHT_CUSTOMER1'",
									"UpdateKey": {
										"value": "'@{pipeline().parameters.Key1}'",
										"type": "Expression"
									},
									"SurrogateKey": "'CUSTOMER_KEY'"
								},
								"datasetParameters": {
									"genericInput": {
										"FolderName": {
											"value": "@pipeline().parameters.Folder_Name",
											"type": "Expression"
										},
										"fileName": {
											"value": "@pipeline().parameters.File_Name",
											"type": "Expression"
										}
									},
									"genericDimensionTable": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": {
											"value": "@pipeline().parameters.Key1",
											"type": "Expression"
										}
									},
									"Update4ChangedRecords": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": {
											"value": "@pipeline().parameters.Key1",
											"type": "Expression"
										}
									},
									"Insert4ChangedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"SinkInsert4NewRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4SoftDeletedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": "'key'"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"Key1": {
						"type": "string",
						"defaultValue": "CUSTOMER_KEY"
					},
					"Folder_Name": {
						"type": "string",
						"defaultValue": "customer"
					},
					"Dimension_Table_Name": {
						"type": "string",
						"defaultValue": "DHT_CUSTOMER1"
					},
					"File_Name": {
						"type": "string",
						"defaultValue": "customer_data.csv"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-05-04T18:52:53Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/scdType2_Generict_Dimension_Load_WOVersion')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmp_pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Account reports Job",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "00:30:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0010_ACCTRPTS_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-06-14T10:15:21Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0010_ACCTRPTS_DIM')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pl_deltalake_customer_dimension')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "xfrm_deltalake_Customer_Dimension",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sPoolKyn001494",
								"type": "BigDataPoolReference"
							}
						}
					},
					{
						"name": "Notebook2",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Notebook1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook 9",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sPoolKyn001494",
								"type": "BigDataPoolReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-06-03T12:05:38Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/xfrm_deltalake_Customer_Dimension')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sPoolKyn001494')]",
				"[concat(variables('workspaceId'), '/notebooks/Notebook 9')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/project_dim_no_parameters')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "scdType2_Project_Dimension_Latest",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "bkp_scdType2_Project_Dimension_Latest",
								"type": "DataFlowReference",
								"parameters": {
									"NaturalKey": "'PROJECT_ID'",
									"NonkeyColumns": "'FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'",
									"EXTRACT_DT": "currentTimestamp()",
									"REC_START_DT": "currentTimestamp()",
									"SurrogateKey": "'PROJECT_KEY'",
									"DimTableName": "'DHT_PROJECT_SIV'"
								},
								"datasetParameters": {
									"genericInput": {
										"FolderName": "project"
									},
									"genericDimensionTable": {
										"DimTableName1": "DHT_PROJECT_SIV"
									},
									"Update4ChangedRecords": {
										"DimTableName1": "DHT_PROJECT_SIV"
									},
									"Insert4ChangedRows": {
										"DimTableName1": "DHT_PROJECT_SIV"
									},
									"SinkInsert4NewRows": {
										"DimTableName1": "DHT_PROJECT_SIV"
									},
									"sink1": {
										"DimTableName1": "DHT_PROJECT_SIV"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-05-04T18:14:02Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/bkp_scdType2_Project_Dimension_Latest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/project_dimension_load')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "scdType2_Generict_Dimension_Load",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "scdType2_Generict_Dimension_Load",
								"type": "DataFlowReference",
								"parameters": {
									"NaturalKey": {
										"value": "'@{pipeline().parameters.NaturalKey}'",
										"type": "Expression"
									},
									"NonkeyColumns": {
										"value": "'@{pipeline().parameters.NonkeyColumns}'",
										"type": "Expression"
									},
									"EXTRACT_DT": "currentTimestamp()",
									"REC_START_DT": "currentTimestamp()",
									"SurrogateKey": {
										"value": "'@{pipeline().parameters.SurrogateKey}'",
										"type": "Expression"
									},
									"DimTableName": {
										"value": "'@{pipeline().parameters.Table_Name1}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"genericInput": {
										"FolderName": {
											"value": "@pipeline().parameters.FolderName",
											"type": "Expression"
										},
										"fileName": "EnterDimensionFileName"
									},
									"genericDimensionTable": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Table_Name1",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4ChangedRecords": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Table_Name1",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Insert4ChangedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Table_Name1",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"SinkInsert4NewRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Table_Name1",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4SoftDeletedRows": {
										"DimTableName1": "EnterDimensionTableName",
										"Key1": "'key'"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"SurrogateKey": {
						"type": "string",
						"defaultValue": "PROJECT_KEY"
					},
					"NaturalKey": {
						"type": "string",
						"defaultValue": "PROJECT_ID"
					},
					"NonkeyColumns": {
						"type": "string",
						"defaultValue": "FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE"
					},
					"FolderName": {
						"type": "string",
						"defaultValue": "project"
					},
					"Table_Name1": {
						"type": "string",
						"defaultValue": "DHT_PROJECT_SIV"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-05-23T07:30:09Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/scdType2_Generict_Dimension_Load')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sanple_proj_pl')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-04-28T06:27:25Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Dataflow1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHT_SELL_CYCLE"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "CUSTOMER_KEY",
						"type": "int",
						"precision": 10
					},
					{
						"name": "VERSION",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CUSTOMER_NO",
						"type": "varchar"
					},
					{
						"name": "FINANCIAL_COUNTRY_CD",
						"type": "varchar"
					},
					{
						"name": "GBG_ID",
						"type": "varchar"
					},
					{
						"name": "CUSTOMER_NAME",
						"type": "varchar"
					},
					{
						"name": "CURRENT_IND",
						"type": "varchar"
					},
					{
						"name": "EXTRACT_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_START_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_END_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "SOURCE_SYSTEM",
						"type": "varchar"
					},
					{
						"name": "REC_CHECKSUM",
						"type": "varchar"
					},
					{
						"name": "REC_STATUS",
						"type": "varchar"
					},
					{
						"name": "IMG_LST_UPD_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "IMG_CREATED_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "DATA_IND",
						"type": "varchar"
					},
					{
						"name": "ACTIVE_IN_SOURCE_IND",
						"type": "char"
					}
				],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHT_CUSTOMER"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable3')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_pgmp_rfs_db",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DBXDH_DHTS_SELL_CYCLE')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHTS_SELL_CYCLE"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DBXDH_DHT_PROJECT_SIV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "PROJECT_KEY",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PROJECT_VERSION",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PROJECT_ID",
						"type": "varchar"
					},
					{
						"name": "FINANCIAL_COUNTRY_CD",
						"type": "varchar"
					},
					{
						"name": "LEDGER_CD",
						"type": "varchar"
					},
					{
						"name": "OFFERING_COMPONENT_CD",
						"type": "varchar"
					},
					{
						"name": "OPPORTUNITY_NUM",
						"type": "varchar"
					},
					{
						"name": "PROJECT_DESC",
						"type": "varchar"
					},
					{
						"name": "SIGNINGS_CD",
						"type": "varchar"
					},
					{
						"name": "SIGNINGS_DESC",
						"type": "varchar"
					},
					{
						"name": "BUSINESS_TYPE_CD",
						"type": "varchar"
					},
					{
						"name": "BUSINESS_TYPE_DESC",
						"type": "varchar"
					},
					{
						"name": "PROJECT_STATUS_CD",
						"type": "varchar"
					},
					{
						"name": "PROJECT_STATUS_DESC",
						"type": "varchar"
					},
					{
						"name": "PROJECT_CUSTOMER_NO",
						"type": "varchar"
					},
					{
						"name": "PROJECT_CREATION_DATE",
						"type": "date"
					},
					{
						"name": "ACCOUTNING_DIVISION",
						"type": "varchar"
					},
					{
						"name": "RESPONSIBLE_SERV_OFFICE",
						"type": "varchar"
					},
					{
						"name": "CURRENT_IND",
						"type": "varchar"
					},
					{
						"name": "EXTRACT_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_START_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_END_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "SOURCE_SYSTEM",
						"type": "varchar"
					},
					{
						"name": "REC_CHECKSUM",
						"type": "varchar"
					},
					{
						"name": "REC_STATUS",
						"type": "varchar"
					},
					{
						"name": "IMG_LST_UPD_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "IMG_CREATED_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "DATA_IND",
						"type": "varchar"
					},
					{
						"name": "ACTIVE_IN_SOURCE_IND",
						"type": "char"
					}
				],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHT_PROJECT_SIV"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DBXDH_DHT_SELL_CYCLE')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHTS_SELL_CYCLE"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataset')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_project_dimension",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "sell_cycle.csv",
						"fileSystem": "project"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "SIEBEL_SALES_STAGE_CODE",
						"type": "String"
					},
					{
						"name": "SIEBEL_SALES_STAGE_NAME",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NO",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NAME",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_project_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_project_dimension",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "sell_cycle.csv",
						"fileSystem": "project"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "SIEBEL_SALES_STAGE_CODE",
						"type": "String"
					},
					{
						"name": "SIEBEL_SALES_STAGE_NAME",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NO",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NAME",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_project_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_project_dimension",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "sell_cycle_tgt.csv",
						"fileSystem": "project"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "SIEBEL_SALES_STAGE_CODE",
						"type": "String"
					},
					{
						"name": "SIEBEL_SALES_STAGE_NAME",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NO",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NAME",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_project_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText3')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "tgt_geo",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/tgt_geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/P_ACCTRPTS_DIM')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_pgmp_rfs_db",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "ACCTRPTS_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PRJCT_ID",
						"type": "varchar"
					},
					{
						"name": "ACCTRPTS_REM_TXT",
						"type": "varchar"
					},
					{
						"name": "SRC_CRETD_TMS",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "SRC_CRETD_USER_ID",
						"type": "varchar"
					},
					{
						"name": "SRC_UPDTD_TMS",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "SRC_UPDTD_USER_ID",
						"type": "varchar"
					},
					{
						"name": "ROW_STAT_CD",
						"type": "char"
					},
					{
						"name": "SRC_SYS_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ETL_JOB_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ETL_EXCTN_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "DM_CRETD_TMS",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "DM_CRETD_USER_ID",
						"type": "varchar"
					},
					{
						"name": "DM_UPDTD_TMS",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "DM_UPDTD_USER_ID",
						"type": "varchar"
					},
					{
						"name": "ORIG_ORG",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "PGMPDM",
					"table": "ACCTRPTS_DIM"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/P_ZAUX_DATE_TRIGGERS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_pgmp_rfs_db",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "PROC_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CRETD_DT",
						"type": "date"
					},
					{
						"name": "RCVD_DT",
						"type": "date"
					},
					{
						"name": "PRPSL_SENT_TO_CLNT_DT",
						"type": "date"
					},
					{
						"name": "PRPSL_DSPSN_DT",
						"type": "date"
					},
					{
						"name": "PRPSL_ACCPTD_DT",
						"type": "date"
					},
					{
						"name": "IMPLMTN_READY_DT",
						"type": "date"
					},
					{
						"name": "IMPLMTN_CLOSE_OUT_DT",
						"type": "date"
					}
				],
				"typeProperties": {
					"schema": "PGMPDM",
					"table": "ZAUX_DATE_TRIGGERS"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RISK_RPT')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_pgmp_rfs_db",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "CNTRCT_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CONTRACT",
						"type": "varchar"
					},
					{
						"name": "UNIQUE_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CLIENT_REFERENCE",
						"type": "varchar"
					},
					{
						"name": "DISPLAY_ID",
						"type": "varchar"
					},
					{
						"name": "GLBL_BUY_GRP_ID",
						"type": "varchar"
					},
					{
						"name": "PARENT_UNIQUE_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PARENT_DISPLAY_ID",
						"type": "varchar"
					},
					{
						"name": "PARENT_TITLE",
						"type": "varchar"
					},
					{
						"name": "PROC_TITLE",
						"type": "varchar"
					},
					{
						"name": "WKFLW_DEF_ID",
						"type": "varchar"
					},
					{
						"name": "INIT_WKFLW_DEF_ID_DESCR",
						"type": "varchar"
					},
					{
						"name": "PROC_DESCRIPTION",
						"type": "varchar"
					},
					{
						"name": "CURRENT_WORKFLOW",
						"type": "varchar"
					},
					{
						"name": "CURRENT_WKFLW_STEP",
						"type": "varchar"
					},
					{
						"name": "CURRENT_STEP_SEQ",
						"type": "int",
						"precision": 10
					},
					{
						"name": "STEP_DESCRIPTION",
						"type": "varchar"
					},
					{
						"name": "STATE_DESC",
						"type": "varchar"
					},
					{
						"name": "STATUS_DESC",
						"type": "varchar"
					},
					{
						"name": "ASSIGNED_TO",
						"type": "varchar"
					},
					{
						"name": "AUDIENCE",
						"type": "varchar"
					},
					{
						"name": "PRIORITY_TXT",
						"type": "varchar"
					},
					{
						"name": "CONDITION",
						"type": "varchar"
					},
					{
						"name": "REGION",
						"type": "varchar"
					},
					{
						"name": "REQUESTOR",
						"type": "varchar"
					},
					{
						"name": "CREATED_BY",
						"type": "varchar"
					},
					{
						"name": "CREATED_DATE",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "LAST_UPDATED",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "RISK_RESPONSE_DUE_DATE",
						"type": "date"
					},
					{
						"name": "REVISED_RISK_RESPONSE_DUE_DATE",
						"type": "date"
					},
					{
						"name": "DUE_IN_DAY_CNT",
						"type": "int",
						"precision": 10
					},
					{
						"name": "OVERDUE",
						"type": "varchar"
					},
					{
						"name": "COMPLETION_DATE",
						"type": "date"
					},
					{
						"name": "COMPLETION_REASON",
						"type": "varchar"
					},
					{
						"name": "COUNTRY",
						"type": "varchar"
					},
					{
						"name": "RISK_SOURCE",
						"type": "varchar"
					},
					{
						"name": "REMARKS",
						"type": "varchar"
					},
					{
						"name": "ORIGINATING_ORG",
						"type": "varchar"
					},
					{
						"name": "RISK_OWNER",
						"type": "varchar"
					},
					{
						"name": "PROBABILITY",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PROBABILITY_SMPL",
						"type": "varchar"
					},
					{
						"name": "IMPACT",
						"type": "char"
					},
					{
						"name": "RESPONSE_PLAN",
						"type": "varchar"
					},
					{
						"name": "LOCAL_CURRENCY",
						"type": "varchar"
					},
					{
						"name": "IMPACT_AMOUNT_LOCAL_CURRENCY",
						"type": "decimal",
						"precision": 15,
						"scale": 2
					},
					{
						"name": "GS_RISK_ID",
						"type": "varchar"
					},
					{
						"name": "BUSINESS_CONTROLS_RISK",
						"type": "varchar"
					},
					{
						"name": "WWBCIT_REFERENCE",
						"type": "varchar"
					},
					{
						"name": "RISK_ANLYSS_DUE_DT",
						"type": "date"
					},
					{
						"name": "RVSD_RISK_ANLYSS_DUE_DT",
						"type": "date"
					},
					{
						"name": "RISK_RSPNS_TYPE_CD",
						"type": "varchar"
					},
					{
						"name": "RISK_OCCURRED_CD",
						"type": "char"
					},
					{
						"name": "RISK_CLOSE_RSN_TXT",
						"type": "varchar"
					},
					{
						"name": "SRC_SYS_CD",
						"type": "varchar"
					},
					{
						"name": "LEGACY_UNIQUE_ID",
						"type": "varchar"
					},
					{
						"name": "LEGACY_DISPLAYED_ID",
						"type": "varchar"
					},
					{
						"name": "LATEST_IBM_ONLY_NOTE",
						"type": "varchar"
					},
					{
						"name": "LATEST_IBM_AND_CLIENT_NOTE",
						"type": "varchar"
					},
					{
						"name": "CNTRCT_TYPE_DESC",
						"type": "varchar"
					},
					{
						"name": "PROC_TYPE_ID",
						"type": "varchar"
					},
					{
						"name": "PROC_TYPE_DESC",
						"type": "varchar"
					},
					{
						"name": "ON_HOLD_CD",
						"type": "varchar"
					},
					{
						"name": "WITHDRWN_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "WITHDRWN_DT",
						"type": "date"
					},
					{
						"name": "ORGNZN_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "IBM_ONLY_TEXT_1",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_2",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_3",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_4",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_5",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_6",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_DATE_1",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_2",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_3",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_4",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_5",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_6",
						"type": "date"
					},
					{
						"name": "PUBLIC_TEXT_1",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_2",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_3",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_4",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_5",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_6",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_DATE_1",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_2",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_3",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_4",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_5",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_6",
						"type": "date"
					},
					{
						"name": "STATE_OF_ORGANIZATION",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "PGMPDM",
					"table": "RISK_RPT"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlPoolTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "CUSTOMER_KEY",
						"type": "int",
						"precision": 10
					},
					{
						"name": "VERSION",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CUSTOMER_NO",
						"type": "varchar"
					},
					{
						"name": "FINANCIAL_COUNTRY_CD",
						"type": "varchar"
					},
					{
						"name": "GBG_ID",
						"type": "varchar"
					},
					{
						"name": "CUSTOMER_NAME",
						"type": "varchar"
					},
					{
						"name": "CURRENT_IND",
						"type": "varchar"
					},
					{
						"name": "EXTRACT_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_START_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_END_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "SOURCE_SYSTEM",
						"type": "varchar"
					},
					{
						"name": "REC_CHECKSUM",
						"type": "varchar"
					},
					{
						"name": "REC_STATUS",
						"type": "varchar"
					},
					{
						"name": "IMG_LST_UPD_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "IMG_CREATED_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "DATA_IND",
						"type": "varchar"
					},
					{
						"name": "ACTIVE_IN_SOURCE_IND",
						"type": "char"
					}
				],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHT_CUSTOMER"
				},
				"sqlPool": {
					"referenceName": "dsqlpoolKyn001494DevEtlHubEUS001",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/dsqlpoolKyn001494DevEtlHubEUS001')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_lookup')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "output-lookup",
						"fileSystem": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_source')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "output-source",
						"fileSystem": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/appfun_acctrpts')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ACCTRPTS_202205311910.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					},
					{
						"name": "CNTRY",
						"type": "String"
					},
					{
						"name": "PROJECT_ID",
						"type": "String"
					},
					{
						"name": "DUE_DATE",
						"type": "String"
					},
					{
						"name": "REVISD_DUE_DATE",
						"type": "String"
					},
					{
						"name": "REMARKS",
						"type": "String"
					},
					{
						"name": "ORIG_ORG",
						"type": "String"
					},
					{
						"name": "CREATED_TS",
						"type": "String"
					},
					{
						"name": "CREATED_USERID",
						"type": "String"
					},
					{
						"name": "UPDATED_TS",
						"type": "String"
					},
					{
						"name": "UPDATED_USERID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dl_tgt_geo')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "tgt_geo",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "GEO1.csv",
						"folderPath": "Output",
						"fileSystem": "prasad"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/tgt_geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsAzureSqlDBEtlhubGenericDimension')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"DimTableName1": {
						"type": "String",
						"defaultValue": "EnterDimensionTableName"
					},
					"Key1": {
						"type": "string",
						"defaultValue": "'key'"
					}
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "DBXDH",
					"table": {
						"value": "@dataset().DimTableName1",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsProjectDimensionRaw')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_project_dimension",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"FolderName": {
						"type": "string",
						"defaultValue": "EnterDimensionFolderName"
					},
					"fileName": {
						"type": "String",
						"defaultValue": "EnterDimensionFileName"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().fileName",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().FolderName",
							"type": "Expression"
						}
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_project_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/output_ar')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "new-ar",
						"fileSystem": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_acctrpts_dim')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "pgmp-account_reports"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ACCTRPTS_DIM_202205301234.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ACCTRPTS_DIM_UID",
						"type": "String"
					},
					{
						"name": "PRJCT_ID",
						"type": "String"
					},
					{
						"name": "ACCTRPTS_REM_TXT",
						"type": "String"
					},
					{
						"name": "SRC_CRETD_TMS",
						"type": "String"
					},
					{
						"name": "SRC_CRETD_USER_ID",
						"type": "String"
					},
					{
						"name": "SRC_UPDTD_TMS",
						"type": "String"
					},
					{
						"name": "SRC_UPDTD_USER_ID",
						"type": "String"
					},
					{
						"name": "ROW_STAT_CD",
						"type": "String"
					},
					{
						"name": "SRC_SYS_DIM_UID",
						"type": "String"
					},
					{
						"name": "ETL_JOB_ID",
						"type": "String"
					},
					{
						"name": "ETL_EXCTN_ID",
						"type": "String"
					},
					{
						"name": "DM_CRETD_TMS",
						"type": "String"
					},
					{
						"name": "DM_CRETD_USER_ID",
						"type": "String"
					},
					{
						"name": "DM_UPDTD_TMS",
						"type": "String"
					},
					{
						"name": "DM_UPDTD_USER_ID",
						"type": "String"
					},
					{
						"name": "ORIG_ORG",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_src_sys_dim')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "SRC_SYS_DIM_202205311909.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "SRC_SYS_DIM_UID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_date_triggers')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "pgmp-account_reports"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_DATE_TRIGGERS_202205301233.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					},
					{
						"name": "CRETD_DT",
						"type": "String"
					},
					{
						"name": "RCVD_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_SENT_TO_CLNT_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_DSPSN_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_ACCPTD_DT",
						"type": "String"
					},
					{
						"name": "IMPLMTN_READY_DT",
						"type": "String"
					},
					{
						"name": "IMPLMTN_CLOSE_OUT_DT",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_date_triggers_src')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_DATE_TRIGGERS_202205311911.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					},
					{
						"name": "CRETD_DT",
						"type": "String"
					},
					{
						"name": "RCVD_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_SENT_TO_CLNT_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_DSPSN_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_ACCPTD_DT",
						"type": "String"
					},
					{
						"name": "IMPLMTN_READY_DT",
						"type": "String"
					},
					{
						"name": "IMPLMTN_CLOSE_OUT_DT",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_deld_proc')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_DELD_PROC_ID_202205311912.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_etl_exctn')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_ETL_EXCTN_202205311906.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ETL_EXCTN_ID",
						"type": "String"
					},
					{
						"name": "ETL_PARAM_START_TMS",
						"type": "String"
					},
					{
						"name": "ETL_PARAM_END_TMS",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_etl_jobs')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_ETL_JOBS_202205311908.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ETL_JOB_ID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/risk_op')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					},
					{
						"name": "CNTRY",
						"type": "String"
					},
					{
						"name": "PROJECT_ID",
						"type": "String"
					},
					{
						"name": "DUE_DATE",
						"type": "String"
					},
					{
						"name": "REVISD_DUE_DATE",
						"type": "String"
					},
					{
						"name": "REMARKS",
						"type": "String"
					},
					{
						"name": "ORIG_ORG",
						"type": "String"
					},
					{
						"name": "CREATED_TS",
						"type": "String"
					},
					{
						"name": "CREATED_USERID",
						"type": "String"
					},
					{
						"name": "UPDATED_TS",
						"type": "String"
					},
					{
						"name": "UPDATED_USERID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage2_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage2_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-kyn-001494-dev-eus-001-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('asa-kyn-001494-dev-eus-001-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asdb_etlhub_confirmed')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('asdb_etlhub_confirmed_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/linkedService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('linkedService1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ln_rfs_pgmp')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('ln_rfs_pgmp_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_adls_project_dimension')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ls_adls_project_dimension_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('ls_adls_project_dimension_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_db2_esa_kyndryl')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Linked Service for ESA Kyndryl DB2 database",
				"annotations": [],
				"type": "Db2",
				"typeProperties": {
					"connectionString": "[parameters('ls_db2_esa_kyndryl_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_pgmp_rfs_db')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('ls_pgmp_rfs_db_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tgt_geo')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('tgt_geo_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('tgt_geo_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tr_customer_dimension_load')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "pl_deltalake_customer_dimension",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 2,
						"startTime": "2022-05-31T16:58:00",
						"endTime": "2022-05-31T17:03:00",
						"timeZone": "India Standard Time"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/pl_deltalake_customer_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tr_pgmp_dimensions_pipeline')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "PgMP_Dimension_Pipeline",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 2,
						"startTime": "2022-06-14T21:00:00",
						"timeZone": "India Standard Time"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/PgMP_Dimension_Pipeline')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tr_pgmp_pipeline_account_reports')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "pgmp_pipeline",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2022-06-01T06:35:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/pgmp_pipeline')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CP_CNTRCT_CHNG_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:05-06-2022\nJob name: Df_BAL0001_GEO_DIM\nCreatedBy: Varaprasad",
				"folder": {
					"name": "PGMP"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Srccntrchnagrpt"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "tgtcntrctchngrpt"
						}
					],
					"transformations": [
						{
							"name": "AlterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          CNTRCT_DIM_UID as integer,",
						"          CONTRACT as string,",
						"          UNIQUE_ID as integer,",
						"          DISPLAY_ID as string,",
						"          GLBL_BUY_GRP_ID as string,",
						"          PARENT_UNIQUE_ID as integer,",
						"          PARENT_DISPLAY_ID as string,",
						"          PARENT_TITLE as string,",
						"          CLIENT_REFERENCE as string,",
						"          PROC_TITLE as string,",
						"          PROC_DESCRIPTION as string,",
						"          CURRENT_WORKFLOW as string,",
						"          CURRENT_WKFLW_STEP as string,",
						"          CURRENT_STEP_SEQ as integer,",
						"          STEP_DESCRIPTION as string,",
						"          STATE_DESC as string,",
						"          STATUS_DESC as string,",
						"          ON_HOLD_DESC as string,",
						"          ON_HOLD_DT as date,",
						"          ON_HOLD_RSN_TXT as string,",
						"          ADTNL_DTLS_FOR_WITHDRW_HOLD_TXT as string,",
						"          AUDIENCE as string,",
						"          PRIORITY_TXT as string,",
						"          CONDITION as string,",
						"          REGION as string,",
						"          REQUESTOR as string,",
						"          COUNTRY as string,",
						"          AGREEMENT_REFERENCE_DETAILS as string,",
						"          ORIGINATING_ORG as string,",
						"          CREATED_DATE as timestamp,",
						"          LAST_UPDATED as timestamp,",
						"          DUE_DT as date,",
						"          RVSD_DUE_DT as date,",
						"          IBM_APPRVL_SIGNED_DT as date,",
						"          CLNT_APPRVL_SIGNED_DT as date,",
						"          CNTRCT_EX_DT as date,",
						"          COMPLETION_DATE as date,",
						"          CMPLTD_RSN_TXT as string,",
						"          CNTRCT_DELVRBL_CNFRMN_IND as string,",
						"          FRML_AMNDMNT_NBR_TXT as string,",
						"          CNTRCT_BSLN_CNFRMN_IND as string,",
						"          STAFFG_SKILL_CNFRMN_IND as string,",
						"          FIN_CNFRMN_IND as string,",
						"          RJCT_RSN_ID as integer,",
						"          RJCT_EXPLNN_TXT as string,",
						"          CNTRCT_REM_TXT as string,",
						"          RJCT_RSN_TXT as string,",
						"          SRC_SYS_CD as string,",
						"          LEGACY_UNIQUE_ID as string,",
						"          LEGACY_DISPLAYED_ID as string,",
						"          LATEST_IBM_ONLY_NOTE as string,",
						"          LATEST_IBM_AND_CLIENT_NOTE as string,",
						"          CNTRCT_TYPE_DESC as string,",
						"          PROC_TYPE_ID as string,",
						"          PROC_TYPE_DESC as string,",
						"          ON_HOLD_CD as string,",
						"          WITHDRWN_DIM_UID as integer,",
						"          WITHDRWN_DT as date,",
						"          ORGNZN_DIM_UID as integer,",
						"          IBM_ONLY_TEXT_1 as string,",
						"          IBM_ONLY_TEXT_2 as string,",
						"          IBM_ONLY_TEXT_3 as string,",
						"          IBM_ONLY_TEXT_4 as string,",
						"          IBM_ONLY_TEXT_5 as string,",
						"          IBM_ONLY_TEXT_6 as string,",
						"          IBM_ONLY_DATE_1 as date,",
						"          IBM_ONLY_DATE_2 as date,",
						"          IBM_ONLY_DATE_3 as date,",
						"          IBM_ONLY_DATE_4 as date,",
						"          IBM_ONLY_DATE_5 as date,",
						"          IBM_ONLY_DATE_6 as date,",
						"          PUBLIC_TEXT_1 as string,",
						"          PUBLIC_TEXT_2 as string,",
						"          PUBLIC_TEXT_3 as string,",
						"          PUBLIC_TEXT_4 as string,",
						"          PUBLIC_TEXT_5 as string,",
						"          PUBLIC_TEXT_6 as string,",
						"          PUBLIC_DATE_1 as date,",
						"          PUBLIC_DATE_2 as date,",
						"          PUBLIC_DATE_3 as date,",
						"          PUBLIC_DATE_4 as date,",
						"          PUBLIC_DATE_5 as date,",
						"          PUBLIC_DATE_6 as date,",
						"          STATE_OF_ORGANIZATION as string,",
						"          SCTR_NM as string,",
						"          CLIENT_UNIT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT \\nC.CNTRCT_DIM_UID,\\nC.CNTRCT_NM AS CONTRACT,\\nH.PROC_DIM_UID AS UNIQUE_ID,\\nP.PROC_DSPLY_ID AS DISPLAY_ID,\\nZ.GLBL_BUY_GRP_ID,\\nPC.PROC_DIM_UID AS PARENT_UNIQUE_ID,  \\nPC.PROC_DSPLY_ID AS PARENT_DISPLAY_ID,\\nPC.TITLE_TXT AS PARENT_TITLE,\\nP.CLNT_REF_NUM AS CLIENT_REFERENCE,\\n--I.WKFLW_DEF_ID,\\n--I.\"DESC\"  as  INIT_WKFLW_DEF_ID_DESCR,\\nP.TITLE_TXT AS PROC_TITLE,\\nP.PROC_DESC AS PROC_DESCRIPTION,\\nW.WKFLW_DEF_ID AS CURRENT_WORKFLOW,\\nWS.WKFLW_STEP_DEF_ID AS CURRENT_WKFLW_STEP,\\nWS.WKFLW_STEP_SEQ_NUM AS CURRENT_STEP_SEQ,\\nWS.WKFLW_STEP_DESC AS STEP_DESCRIPTION,\\nST.STATE_TITLE_TXT STATE_DESC,\\nS.STAT_IBM_DESC AS STATUS_DESC,\\nO.ON_HOLD_DESC, \\nH.ON_HOLD_DT,\\nHR.ON_HOLD_RSN_TXT,\\nP.ADTNL_DTLS_FOR_WITHDRW_HOLD_TXT, \\n--decrypt_char(UD.CONCAT_NM_LOGIN,MR.MIS_REP_REF_CD) AS ASSIGNED_TO,\\nAU.ADNC_TXT AS AUDIENCE,\\nPD.PRIORITY_TXT as PRIORITY_TXT,\\nCN.COND_TXT AS CONDITION,\\nCOALESCE(RGN.REGION,GEO.KYNDRYL_RGN) AS REGION,\\n--decrypt_char(CR.CONCAT_NM_LOGIN,MR.MIS_REP_REF_CD) AS CREATED_BY,\\n--decrypt_char(P.RQSTR_TXT,\\nMR.MIS_REP_REF_CD AS REQUESTOR,\\nGEO.SRGNCTRY_NM AS COUNTRY,                                                                                                                                  \\nCC.CNTRCT_REF_TXT AS AGREEMENT_REFERENCE_DETAILS,\\nCASE CC.ORIG_ORG WHEN \\'C\\' THEN \\'Client\\' WHEN \\'I\\' THEN \\'IBM\\' WHEN \\'R\\' THEN \\'RFS\\' ELSE \\'\\' END AS ORIGINATING_ORG,\\n---decrypt_char(CC.IBM_APPRVR_TXT,MR.MIS_REP_REF_CD)  AS IBM_APPRVR_TXT,\\n--decrypt_char(CC.CLNT_APPRVR_TXT,MR.MIS_REP_REF_CD) AS CLNT_APPRVR_TXT ,     \\nH.SRC_CRETD_TMS AS CREATED_DATE,\\nH.SRC_UPDTD_TMS AS LAST_UPDATED,\\nF.DUE_DT,\\nF.RVSD_DUE_DT,\\n/*DAYS(NVL(F.DUE_DT,F.RVSD_DUE_DT)) - DAYS((CURRENT_TIMESTAMP - CURRENT_TIMEZONE) + AF.TZ HOURS) AS DUE_IN_DAY_CNT,\\nCASE WHEN DAYS(NVL(F.RVSD_DUE_DT,F.DUE_DT)) - DAYS((CURRENT_TIMESTAMP - CURRENT_TIMEZONE) + AF.TZ HOURS)  < 0 THEN \\'Y\\' ELSE \\'N\\' END AS OVERDUE,*/      \\nF.IBM_APPRVL_SIGNED_DT,\\nF.CLNT_APPRVL_SIGNED_DT,\\nF.CNTRCT_EX_DT,\\nH.CMPLTD_DT AS COMPLETION_DATE,\\nP.CMPLTD_RSN_TXT,\\nCC.CNTRCT_DELVRBL_CNFRMN_IND,\\nCC.FRML_AMNDMNT_NBR_TXT,\\nCC.CNTRCT_BSLN_CNFRMN_IND,\\nCC.STAFFG_SKILL_CNFRMN_IND,\\nCC.FIN_CNFRMN_IND,\\nCC.RJCT_RSN_ID,\\nCC.RJCT_EXPLNN_TXT,\\nCC.CNTRCT_REM_TXT,\\nCC.RJCT_RSN_TXT, \\nSR.SRC_SYS_CD,\\n\\' \\' AS LEGACY_UNIQUE_ID,\\n\\' \\' AS LEGACY_DISPLAYED_ID,\\nTRIM(SUBSTRING(ND.NOTES_DESC,1,50)) AS LATEST_IBM_ONLY_NOTE,\\nTRIM(SUBSTRING(PN.NOTES_DESC,1,50)) AS LATEST_IBM_AND_CLIENT_NOTE,\\nCT.CNTRCT_TYPE_DESC,\\nPT.PROC_TYPE_ID,\\nPT.PROC_TYPE_DESC,\\nO.ON_HOLD_CD,\\nH.WITHDRWN_DIM_UID,\\nH.WITHDRWN_DT,\\nH.ORGNZN_DIM_UID,\\nDI.IBM_ONLY_TEXT_1,\\nDI.IBM_ONLY_TEXT_2,\\nDI.IBM_ONLY_TEXT_3,\\nDI.IBM_ONLY_TEXT_4,\\nDI.IBM_ONLY_TEXT_5,\\nDI.IBM_ONLY_TEXT_6,\\nDI.IBM_ONLY_DATE_1,\\nDI.IBM_ONLY_DATE_2,\\nDI.IBM_ONLY_DATE_3,\\nDI.IBM_ONLY_DATE_4,\\nDI.IBM_ONLY_DATE_5,\\nDI.IBM_ONLY_DATE_6,\\nDP.PUBLIC_TEXT_1,\\nDP.PUBLIC_TEXT_2,\\nDP.PUBLIC_TEXT_3,\\nDP.PUBLIC_TEXT_4,\\nDP.PUBLIC_TEXT_5,\\nDP.PUBLIC_TEXT_6,\\nDP.PUBLIC_DATE_1,\\nDP.PUBLIC_DATE_2,\\nDP.PUBLIC_DATE_3,\\nDP.PUBLIC_DATE_4,\\nDP.PUBLIC_DATE_5,\\nDP.PUBLIC_DATE_6,\\nSO.STAT_OF_ORGNZN_DESC AS STATE_OF_ORGANIZATION,\\nSD.SCTR_NM,\\nZ.CLIENT_UNIT_NM AS CLIENT_UNIT\\nFROM PGMPDM.PROC_HEADR_FCT H\\n\\nINNER JOIN PGMPDM.PROC_TYPE_DIM PT ON PT.PROC_TYPE_DIM_UID = H.PROC_TYPE_DIM_UID AND PT.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PROC_DIM P ON P.PROC_DIM_UID = H.PROC_DIM_UID AND P.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = P.CNTRCT_DIM_UID AND C.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.CNTRCT_TYPE_DIM CT ON CT.CNTRCT_TYPE_DIM_UID = C.CNTRCT_TYPE_DIM_UID AND CT.ROW_STAT_CD <> \\'D\\'\\n--INNER JOIN PGMPDM.WKFLW_DEF_DIM I ON I.WKFLW_DEF_DIM_UID = H.INIT_WKFLW_DEF_DIM_UID AND I.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.WKFLW_DEF_DIM W ON W.WKFLW_DEF_DIM_UID = H.WKFLW_DEF_DIM_UID AND W.ROW_STAT_CD <> \\'D\\'  \\n\\nINNER JOIN PGMPDM.WKFLW_STEP_DEF_DIM WS ON WS.WKFLW_STEP_DEF_DIM_UID = H.WKFLW_STEP_DEF_DIM_UID AND WS.ROW_STAT_CD <> \\'D\\' \\nINNER JOIN PGMPDM.STATE_DIM ST ON ST.STATE_DIM_UID = H.STATE_DIM_UID AND ST.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PROC_STEP_DAT_DIM PS ON P.PROC_DIM_UID = PS.PROC_DIM_UID AND P.CURR_PROC_STEP_DAT_DIM_UID = PS.PROC_STEP_DAT_DIM_UID AND PS.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.GEO_DIM GEO ON 1=1\\n-- GEO.GEO_DIM_UID = H.GEO_DIM_UID AND GEO.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.MISC_FCT F ON F.PROC_HEADR_FCT_UID = H.PROC_HEADR_FCT_UID AND F.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.CNTRCT_CHG_DIM CC ON CC.CNTRCT_CHG_DIM_UID = P.PROC_DIM_UID AND CC.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.SRC_SYS_DIM SR ON SR.SRC_SYS_DIM_UID = H.SRC_SYS_DIM_UID AND SR.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.SCTR_DIM SD ON SD.SCTR_DIM_UID = C.SCTR_DIM_UID AND SD.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.STAT_DIM S ON S.STAT_DIM_UID = H.STAT_DIM_UID AND S.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.MISC_REP_REF MR ON MIS_REP_REF_UID = 3\\n\\nLEFT OUTER JOIN PGMPDM.USER_DIM UD ON UD.USER_ID = PS.ASSGN_TO_NUM AND UD.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.ADNC_DIM AU ON AU.ADNC_CD = P.ADNC_CD\\nLEFT OUTER JOIN PGMPDM.PRIORITY_DIM PD ON PD.PRIORITY_NUM = P.PRIORTY_NUM\\n\\nLEFT OUTER JOIN PGMPDM.COND_DIM CN ON CN.COND_CD = P.CNDTN_CD\\nLEFT OUTER JOIN APPFUN.PROC_REGION_V RGN ON RGN.PROC_ID = P.PROC_DIM_UID AND RGN.INTERNAL_VAL <> \\'NONE\\'\\nLEFT OUTER JOIN PGMPDM.USER_DIM CR ON CR.USER_DIM_UID = H.CRETD_BY_USER_DIM_UID AND CR.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.NOTES_DIM ND ON ND.PROC_DIM_UID = P.PROC_DIM_UID AND ND.ROW_STAT_CD <> \\'D\\' AND ND.NOTE_TYPE_CD = \\'I\\' AND ND.LATEST_NOTE_IND = \\'Y\\'\\nLEFT OUTER JOIN PGMPDM.NOTES_DIM PN ON PN.PROC_DIM_UID = P.PROC_DIM_UID AND PN.ROW_STAT_CD <> \\'D\\' AND PN.NOTE_TYPE_CD = \\'P\\' AND PN.LATEST_NOTE_IND = \\'Y\\'\\nLEFT OUTER JOIN PGMPDM.ON_HOLD_DIM O ON O.ON_HOLD_DIM_UID = H.ON_HOLD_DIM_UID AND O.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.ON_HOLD_RSN_DIM HR ON HR.ON_HOLD_RSN_ID = P.ON_HOLD_RSN_ID AND HR.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_DIM PC ON PC.PROC_DIM_UID = P.PARNT_PROC_ID AND PC.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP ON DP.PROC_DIM_UID = P.PROC_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DI ON DI.PROC_DIM_UID = P.PROC_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ORG_CNTRCT_MAP_DIM OM ON OM.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ORGNZN_DIM OD ON OD.ORGNZN_DIM_UID = OM.ORGNZN_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.STAT_OF_ORGNZN_DIM SO ON SO.STAT_OF_ORGNZN_DIM_UID = OD.STAT_OF_ORGNZN_DIM_UID\\n--LEFT OUTER JOIN PGMPDM.CNTRCT_TZ_V AF ON C.CNTRCT_DIM_UID = AF.CNTRCT_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ZAUX_CNTRCT_GLBL_BUY_GRP_MAP Z ON Z.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID \\n\\nWHERE H.ROW_STAT_CD <> \\'D\\' AND PT.PROC_TYPE_ID = \\'CONTRCHG\\' AND P.ADNC_ACCSS_CD <> \\'C\\' AND H.DELD_DT IS NULL AND SR.SRC_SYS_CD = \\'PGMP\\'\\n\\n\\n\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> Srccntrchnagrpt",
						"Srccntrchnagrpt alterRow(upsertIf(true())) ~> AlterRow1",
						"AlterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PGMPDM.CONTRACT_CHANGE_RPT',",
						"     insertable: false,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: true,",
						"     keys:['UNIQUE_ID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> tgtcntrctchngrpt"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0006_CP_RISK_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          CNTRCT_DIM_UID as integer,",
						"          CONTRACT as string,",
						"          UNIQUE_ID as integer,",
						"          CLIENT_REFERENCE as string,",
						"          DISPLAY_ID as string,",
						"          GLBL_BUY_GRP_ID as string,",
						"          PARENT_UNIQUE_ID as integer,",
						"          PARENT_DISPLAY_ID as string,",
						"          PARENT_TITLE as string,",
						"          PROC_TITLE as string,",
						"          WKFLW_DEF_ID as string,",
						"          INIT_WKFLW_DEF_ID_DESCR as string,",
						"          PROC_DESCRIPTION as string,",
						"          CURRENT_WORKFLOW as string,",
						"          CURRENT_WKFLW_STEP as string,",
						"          CURRENT_STEP_SEQ as integer,",
						"          STEP_DESCRIPTION as string,",
						"          STATE_DESC as string,",
						"          STATUS_DESC as string,",
						"          ASSIGNED_TO as binary,",
						"          AUDIENCE as string,",
						"          PRIORITY_TXT as string,",
						"          CONDITION as string,",
						"          REGION as string,",
						"          REQUESTOR as binary,",
						"          CREATED_BY as binary,",
						"          CREATED_DATE as timestamp,",
						"          LAST_UPDATED as timestamp,",
						"          RISK_RESPONSE_DUE_DATE as date,",
						"          REVISED_RISK_RESPONSE_DUE_DATE as date,",
						"          DUE_IN_DAY_CNT as integer,",
						"          OVERDUE as string,",
						"          COMPLETION_DATE as date,",
						"          COMPLETION_REASON as string,",
						"          COUNTRY as string,",
						"          RISK_SOURCE as string,",
						"          REMARKS as string,",
						"          ORIGINATING_ORG as string,",
						"          RISK_OWNER as binary,",
						"          PROBABILITY as integer,",
						"          PROBABILITY_SMPL as string,",
						"          IMPACT as string,",
						"          RESPONSE_PLAN as string,",
						"          LOCAL_CURRENCY as string,",
						"          IMPACT_AMOUNT_LOCAL_CURRENCY as decimal(15,2),",
						"          GS_RISK_ID as string,",
						"          BUSINESS_CONTROLS_RISK as string,",
						"          WWBCIT_REFERENCE as string,",
						"          RISK_ANLYSS_DUE_DT as date,",
						"          RVSD_RISK_ANLYSS_DUE_DT as date,",
						"          RISK_RSPNS_TYPE_CD as string,",
						"          RISK_OCCURRED_CD as string,",
						"          RISK_CLOSE_RSN_TXT as string,",
						"          SRC_SYS_CD as string,",
						"          LEGACY_UNIQUE_ID as string,",
						"          LEGACY_DISPLAYED_ID as string,",
						"          LATEST_IBM_ONLY_NOTE as string,",
						"          LATEST_IBM_AND_CLIENT_NOTE as string,",
						"          CNTRCT_TYPE_DESC as string,",
						"          PROC_TYPE_ID as string,",
						"          PROC_TYPE_DESC as string,",
						"          ON_HOLD_CD as string,",
						"          WITHDRWN_DIM_UID as integer,",
						"          WITHDRWN_DT as date,",
						"          ORGNZN_DIM_UID as integer,",
						"          IBM_ONLY_TEXT_1 as string,",
						"          IBM_ONLY_TEXT_2 as string,",
						"          IBM_ONLY_TEXT_3 as string,",
						"          IBM_ONLY_TEXT_4 as string,",
						"          IBM_ONLY_TEXT_5 as string,",
						"          IBM_ONLY_TEXT_6 as string,",
						"          IBM_ONLY_DATE_1 as date,",
						"          IBM_ONLY_DATE_2 as date,",
						"          IBM_ONLY_DATE_3 as date,",
						"          IBM_ONLY_DATE_4 as date,",
						"          IBM_ONLY_DATE_5 as date,",
						"          IBM_ONLY_DATE_6 as date,",
						"          PUBLIC_TEXT_1 as string,",
						"          PUBLIC_TEXT_2 as string,",
						"          PUBLIC_TEXT_3 as string,",
						"          PUBLIC_TEXT_4 as string,",
						"          PUBLIC_TEXT_5 as string,",
						"          PUBLIC_TEXT_6 as string,",
						"          PUBLIC_DATE_1 as date,",
						"          PUBLIC_DATE_2 as date,",
						"          PUBLIC_DATE_3 as date,",
						"          PUBLIC_DATE_4 as date,",
						"          PUBLIC_DATE_5 as date,",
						"          PUBLIC_DATE_6 as date,",
						"          STATE_OF_ORGANIZATION as string,",
						"          SCTR_NM as string,",
						"          CLIENT_UNIT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT \\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM AS CONTRACT,\\n     HFCT.PROC_DIM_UID AS UNIQUE_ID,\\n     PROCS.CLNT_REF_NUM AS CLIENT_REFERENCE,\\n     PROCS.PROC_DSPLY_ID AS DISPLAY_ID,\\n     ZACNTRCT.GLBL_BUY_GRP_ID,\\n     PARENT_PROC.PROC_DIM_UID AS PARENT_UNIQUE_ID,\\n     PARENT_PROC.PROC_DSPLY_ID AS PARENT_DISPLAY_ID,\\n     PARENT_PROC.TITLE_TXT AS PARENT_TITLE,\\n     PROCS.TITLE_TXT AS PROC_TITLE,\\n     INITWDD.WKFLW_DEF_ID,\\n     INITWDD.\"DESC\" AS INIT_WKFLW_DEF_ID_DESCR,\\n     PROCS.PROC_DESC AS PROC_DESCRIPTION,\\n     WKFLW.WKFLW_DEF_ID AS CURRENT_WORKFLOW,\\n     WKFLW_STEP.WKFLW_STEP_DEF_ID AS CURRENT_WKFLW_STEP,\\n     WKFLW_STEP.WKFLW_STEP_SEQ_NUM AS CURRENT_STEP_SEQ,\\n     WKFLW_STEP.WKFLW_STEP_DESC AS STEP_DESCRIPTION,\\n     ST.STATE_TITLE_TXT STATE_DESC,\\n     STAT.STAT_IBM_DESC AS STATUS_DESC,\\n     ASSGNTO.CONCAT_NM_LOGIN AS ASSIGNED_TO, --decrypt_char(ASSGNTO.CONCAT_NM_LOGIN,MIS_REP_REF.MIS_REP_REF_CD) AS ASSIGNED_TO,\\n     AUDIENCE.ADNC_TXT AS AUDIENCE,\\n     P.PRIORITY_TXT,\\n     COND.COND_TXT AS CONDITION,\\n     COALESCE(RGN.REGION,GEO.KYNDRYL_RGN) AS REGION,\\n     PROCS.RQSTR_TXT AS REQUESTOR, --decrypt_char(PROCS.RQSTR_TXT,MIS_REP_REF.MIS_REP_REF_CD) AS REQUESTOR,\\n     CREATOR.CONCAT_NM_LOGIN AS CREATED_BY, --decrypt_char(CREATOR.CONCAT_NM_LOGIN,MIS_REP_REF.MIS_REP_REF_CD) AS CREATED_BY,\\n     HFCT.SRC_CRETD_TMS AS CREATED_DATE,\\n     HFCT.SRC_UPDTD_TMS AS LAST_UPDATED,\\n     FCT.RISK_RSPNS_DUE_DT AS RISK_RESPONSE_DUE_DATE,\\n     FCT.RVSD_RISK_RSPNS_DUE_DT AS REVISED_RISK_RESPONSE_DUE_DATE,\\n     DATEDIFF(day, \\'2000/01/01\\', ISNULL(FCT.RVSD_RISK_RSPNS_DUE_DT,FCT.RISK_RSPNS_DUE_DT)) - DATEDIFF(day, \\'2000/01/01\\', DATEADD(hh, AF.TZ, CURRENT_TIMESTAMP)) AS DUE_IN_DAY_CNT,\\n     CASE \\n          WHEN DATEDIFF(day, \\'2000/01/01\\', ISNULL(FCT.RVSD_RISK_RSPNS_DUE_DT,FCT.RISK_RSPNS_DUE_DT)) - DATEDIFF(day, \\'2000/01/01\\', DATEADD(hh, AF.TZ, CURRENT_TIMESTAMP)) < 0 THEN \\'Y\\' \\n          ELSE \\'N\\' \\n     END AS OVERDUE,\\n     HFCT.CMPLTD_DT AS COMPLETION_DATE,\\n     PROCS.CMPLTD_RSN_TXT AS COMPLETION_REASON,\\n     GEO.SRGNCTRY_NM AS COUNTRY,\\n     RISKSRC.RISK_SRC_TXT AS RISK_SOURCE,\\n     RISK.RISK_REM_TXT AS REMARKS,\\n     RISK.ORGNTG_ORGNZN_TXT AS ORIGINATING_ORG,\\n     RISK.RISK_OWNR_TXT AS RISK_OWNER, --decrypt_char(RISK.RISK_OWNR_TXT,MIS_REP_REF.MIS_REP_REF_CD) AS RISK_OWNER,\\n     FCT.PRBBLTY_NUM AS PROBABILITY,\\n     CASE \\n          WHEN FCT.PROBABILITY_SMPL =  \\'Z\\' THEN \\'10\\' \\n          WHEN FCT.PROBABILITY_SMPL =  \\'D\\' THEN \\'25\\' \\n          WHEN FCT.PROBABILITY_SMPL =  \\'C\\' THEN \\'50\\' \\n          WHEN FCT.PROBABILITY_SMPL =  \\'S\\' THEN \\'75\\' \\n     END AS PROBABILITY_SMPL ,\\n     FCT.IMPCT_CD AS IMPACT,\\n     RISK.MTGTN_RSPNS_PLAN_TXT AS RESPONSE_PLAN,\\n     CRNCY.CRNCY_NM AS LOCAL_CURRENCY,\\n     FCT.IMPCT_AMT AS IMPACT_AMOUNT_LOCAL_CURRENCY,\\n     RISK.GS_RISK_ID ,\\n     CASE \\n          WHEN RISK.BUS_CNTRL_RISK_IND=\\'Y\\' THEN \\'YES\\' \\n          WHEN RISK.BUS_CNTRL_RISK_IND=\\'N\\' THEN \\'NO\\' \\n     END AS BUSINESS_CONTROLS_RISK,\\n     RISK.WWRADB_REF_TXT AS WWBCIT_REFERENCE,\\n     FCT.RISK_ANLYSS_DUE_DT,\\n     FCT.RVSD_RISK_ANLYSS_DUE_DT,\\n     CASE \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'ACC\\' THEN \\'Accept/Retain\\' \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'AVO\\' THEN \\'Avoid\\' \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'CON\\' THEN \\'Contain/Reduce\\' \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'INS\\' THEN \\'Use Insurance\\'  \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'XFR\\' THEN \\'Transfer\\' \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'RES\\' THEN \\'Use Risk Reserve\\' \\n          ELSE RISK.RISK_RSPNS_TYPE_CD \\n     END AS RISK_RSPNS_TYPE_CD,\\n     RISK.RISK_OCCURRED_CD,\\n     RISK.RISK_CLOSE_RSN_TXT,\\n     SRC.SRC_SYS_CD,\\n     \\'\\' AS LEGACY_UNIQUE_ID,\\n     \\'\\' AS LEGACY_DISPLAYED_ID,\\n     TRIM(INOTES.NOTES_DESC) AS LATEST_IBM_ONLY_NOTE,\\n     TRIM(PNOTES.NOTES_DESC) AS LATEST_IBM_AND_CLIENT_NOTE,\\n     CTYPE.CNTRCT_TYPE_DESC,\\n     PTYPE.PROC_TYPE_ID,\\n     PTYPE.PROC_TYPE_DESC,\\n     ONHOLD.ON_HOLD_CD,\\n     HFCT.WITHDRWN_DIM_UID,\\n     HFCT.WITHDRWN_DT,\\n     HFCT.ORGNZN_DIM_UID,\\n     DATAIBM.IBM_ONLY_TEXT_1,\\n     DATAIBM.IBM_ONLY_TEXT_2,\\n     DATAIBM.IBM_ONLY_TEXT_3,\\n     DATAIBM.IBM_ONLY_TEXT_4,\\n     DATAIBM.IBM_ONLY_TEXT_5,\\n     DATAIBM.IBM_ONLY_TEXT_6,\\n     DATAIBM.IBM_ONLY_DATE_1,\\n     DATAIBM.IBM_ONLY_DATE_2,\\n     DATAIBM.IBM_ONLY_DATE_3,\\n     DATAIBM.IBM_ONLY_DATE_4,\\n     DATAIBM.IBM_ONLY_DATE_5,\\n     DATAIBM.IBM_ONLY_DATE_6,\\n     DATAPBLIC.PUBLIC_TEXT_1,\\n     DATAPBLIC.PUBLIC_TEXT_2,\\n     DATAPBLIC.PUBLIC_TEXT_3,\\n     DATAPBLIC.PUBLIC_TEXT_4,\\n     DATAPBLIC.PUBLIC_TEXT_5,\\n     DATAPBLIC.PUBLIC_TEXT_6,\\n     DATAPBLIC.PUBLIC_DATE_1,\\n     DATAPBLIC.PUBLIC_DATE_2,\\n     DATAPBLIC.PUBLIC_DATE_3,\\n     DATAPBLIC.PUBLIC_DATE_4,\\n     DATAPBLIC.PUBLIC_DATE_5,\\n     DATAPBLIC.PUBLIC_DATE_6,\\n     SOO.STAT_OF_ORGNZN_DESC AS STATE_OF_ORGANIZATION,\\n     SCTR.SCTR_NM,\\n     ZACNTRCT.CLIENT_UNIT_NM AS CLIENT_UNIT\\n\\nFROM \\n     PGMPDM.RISK_DIM RISK\\n     INNER JOIN PGMPDM.PROC_HEADR_FCT HFCT ON HFCT.PROC_DIM_UID = RISK.RISK_DIM_UID AND HFCT.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.PROC_TYPE_DIM PTYPE ON PTYPE.PROC_TYPE_DIM_UID = HFCT.PROC_TYPE_DIM_UID AND PTYPE.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.PROC_DIM PROCS ON PROCS.PROC_DIM_UID = HFCT.PROC_DIM_UID AND PROCS.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID AND C.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.CNTRCT_TYPE_DIM CTYPE ON CTYPE.CNTRCT_TYPE_DIM_UID = C.CNTRCT_TYPE_DIM_UID AND CTYPE.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.WKFLW_DEF_DIM INITWDD ON INITWDD.WKFLW_DEF_DIM_UID = HFCT.INIT_WKFLW_DEF_DIM_UID AND INITWDD.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.WKFLW_DEF_DIM WKFLW ON WKFLW.WKFLW_DEF_DIM_UID = HFCT.WKFLW_DEF_DIM_UID AND WKFLW.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.WKFLW_STEP_DEF_DIM WKFLW_STEP ON WKFLW_STEP.WKFLW_STEP_DEF_DIM_UID = HFCT.WKFLW_STEP_DEF_DIM_UID AND WKFLW_STEP.ROW_STAT_CD <> \\'D\\' \\n     INNER JOIN PGMPDM.STATE_DIM ST ON ST.STATE_DIM_UID = HFCT.STATE_DIM_UID AND ST.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.PROC_STEP_DAT_DIM PSD ON PROCS.PROC_DIM_UID = PSD.PROC_DIM_UID AND PROCS.CURR_PROC_STEP_DAT_DIM_UID = PSD.PROC_STEP_DAT_DIM_UID AND PSD.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.GEO_DIM GEO ON GEO.GEO_DIM_UID = HFCT.GEO_DIM_UID -- AND GEO.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.MISC_FCT FCT ON FCT.PROC_HEADR_FCT_UID = HFCT.PROC_HEADR_FCT_UID AND FCT.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.SRC_SYS_DIM SRC ON SRC.SRC_SYS_DIM_UID = HFCT.SRC_SYS_DIM_UID AND SRC.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.SCTR_DIM SCTR ON SCTR.SCTR_DIM_UID = C.SCTR_DIM_UID AND SCTR.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.STAT_DIM STAT ON STAT.STAT_DIM_UID = HFCT.STAT_DIM_UID AND STAT.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.MISC_REP_REF MIS_REP_REF ON MIS_REP_REF_UID = 3\\n     LEFT OUTER JOIN PGMPDM.USER_DIM ASSGNTO ON ASSGNTO.USER_ID = PSD.ASSGN_TO_NUM AND ASSGNTO.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.ADNC_DIM AUDIENCE ON AUDIENCE.ADNC_CD = PROCS.ADNC_CD\\n     LEFT OUTER JOIN PGMPDM.PRIORITY_DIM P ON P.PRIORITY_NUM = PROCS.PRIORTY_NUM\\n     LEFT OUTER JOIN PGMPDM.COND_DIM COND ON COND.COND_CD = PROCS.CNDTN_CD\\n     LEFT OUTER JOIN APPFUN.PROC_REGION_V RGN ON RGN.PROC_ID = PROCS.PROC_DIM_UID AND RGN.INTERNAL_VAL <> \\'NONE\\'\\n     LEFT OUTER JOIN PGMPDM.USER_DIM CREATOR ON CREATOR.USER_DIM_UID = HFCT.CRETD_BY_USER_DIM_UID AND CREATOR.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.NOTES_DIM INOTES ON INOTES.PROC_DIM_UID = PROCS.PROC_DIM_UID AND INOTES.ROW_STAT_CD <> \\'D\\' AND INOTES.NOTE_TYPE_CD = \\'I\\' AND INOTES.LATEST_NOTE_IND = \\'Y\\'\\n     LEFT OUTER JOIN PGMPDM.NOTES_DIM PNOTES ON PNOTES.PROC_DIM_UID = PROCS.PROC_DIM_UID AND PNOTES.ROW_STAT_CD <> \\'D\\' AND PNOTES.NOTE_TYPE_CD = \\'P\\' AND PNOTES.LATEST_NOTE_IND = \\'Y\\'\\n     LEFT OUTER JOIN PGMPDM.ON_HOLD_DIM ONHOLD ON ONHOLD.ON_HOLD_DIM_UID = HFCT.ON_HOLD_DIM_UID AND ONHOLD.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.ON_HOLD_RSN_DIM HOLDRSN ON HOLDRSN.ON_HOLD_RSN_ID = PROCS.ON_HOLD_RSN_ID AND HOLDRSN.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.CRNCY_DIM CRNCY ON CRNCY.CRNCY_DIM_UID = FCT.LOCAL_CRNCY_DIM_UID AND CRNCY.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.PROC_DIM PARENT_PROC ON PARENT_PROC.PROC_DIM_UID = PROCS.PARNT_PROC_ID AND PARENT_PROC.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.RISK_SRC_DIM RISKSRC ON RISKSRC.RISK_SRC_DIM_CD = RISK.RISK_SRC_IND AND RISKSRC.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DATAPBLIC ON DATAPBLIC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DATAIBM ON DATAIBM.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.ORG_CNTRCT_MAP_DIM ORG_MAP ON ORG_MAP.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.ORGNZN_DIM ORG ON ORG.ORGNZN_DIM_UID = ORG_MAP.ORGNZN_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.STAT_OF_ORGNZN_DIM SOO ON SOO.STAT_OF_ORGNZN_DIM_UID = ORG.STAT_OF_ORGNZN_DIM_UID\\n     --LEFT OUTER JOIN PGMPDM.CNTRCT_TZ_V AF ON C.CNTRCT_DIM_UID = AF.CNTRCT_DIM_UID\\n     LEFT OUTER JOIN (\\n          SELECT DISTINCT \\n               C.CNTRCT_DIM_UID,\\n              CASE\\n                  WHEN B.TMZONE_DESC LIKE \\'(GMT)%\\' THEN 0\\n                  WHEN B.TMZONE_DESC LIKE \\'(GMT+%\\' THEN CAST(SUBSTRING(B.TMZONE_DESC, 6, 2) AS INT)\\n                  WHEN B.TMZONE_DESC LIKE \\'(GMT-%\\' THEN CAST(SUBSTRING(B.TMZONE_DESC, 6, 2) AS INT) * -1\\n                  ELSE 0\\n              END TZ\\n          FROM\\n              PGMPDM.ORGNZN_DIM A,\\n              PGMPDM.TMZONE_DIM B,\\n              PGMPDM.CNTRCT_DIM C,\\n              PGMPDM.ORG_CNTRCT_MAP_DIM D\\n          WHERE\\n              A.TMZONE_DIM_UID = B.TMZONE_DIM_UID\\n              AND A.ORGNZN_DIM_UID = D.ORGNZN_DIM_UID\\n              AND C.CNTRCT_DIM_UID = D.CNTRCT_DIM_UID \\n      ) AF ON C.CNTRCT_DIM_UID = AF.CNTRCT_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.ZAUX_CNTRCT_GLBL_BUY_GRP_MAP ZACNTRCT ON ZACNTRCT.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID \\n\\nWHERE \\n     RISK.ROW_STAT_CD <> \\'D\\' \\n     AND PTYPE.PROC_TYPE_ID IN (\\'RISK\\', \\'RISKNEW\\') \\n     AND PROCS.ADNC_ACCSS_CD <> \\'C\\' \\n     AND HFCT.DELD_DT IS NULL \\n     AND SRC.SRC_SYS_CD = \\'PGMP\\'',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'RISK_RPT',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     recreate: true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0010_ACCTRPTS_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ACCTRPTS_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ACCTRPTS_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     ACC.PROC_ID as ACCTRPTS_DIM_UID,\\n     ACC.PROJECT_ID as PRJCT_ID,\\n     cast(ACC.REMARKS as varchar(1024)) as ACCTRPTS_REM_TXT,\\n     ACC.CREATED_TS as SRC_CRETD_TMS,\\n     ACC.CREATED_USERID as SRC_CRETD_USER_ID,\\n     ACC.UPDATED_TS as SRC_UPDTD_TMS,\\n     ACC.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nfrom\\n     APPFUN.ACCTRPTS ACC\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on ACC.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on ACC.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'BALD0010_ACCTRPTS_DIM\\'          \\n     ) as JOB\\n     on \\'1\\' = \\'1\\'\\n     left join\\n     (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\'\\n     ) as FIL\\n     on \\'1\\' = \\'1\\'\\n     left join\\n     (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\'               \\n     ) as SYS\\n     on \\'1\\' = \\'1\\'\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_ACCTRPTS_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_ACCTRPTS_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.ACCTRPTS_DIM_UID as LKP_ACCTRPTS_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.ACCTRPTS_REM_TXT as LKP_ACCTRPTS_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nfrom\\n     PGMPDM.ACCTRPTS_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.ACCTRPTS_DIM_UID = ZDT.PROC_ID     \\n     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_ACCTRPTS_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(ACCTRPTS_DIM_UID==LKP_ACCTRPTS_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(ACCTRPTS_DIM_UID == LKP_ACCTRPTS_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          ACCTRPTS_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = \"dsdam\",",
						"          DM_UPDTD_USER_ID = \"dsdam\",",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = \"dsdam\",",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = \"dsdam\",",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          ACCTRPTS_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTRPTS_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['ACCTRPTS_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTRPTS_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0020ACCTSPF_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:05-06-2022\nJob name: Df_BALD0020ACCTSPF_DIM\nCreatedBy: Varaprasad",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "appfunacctspcf"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "LkpTgtAcctspcfDim"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtAcctspfDimupd"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtpgmdmacctspfDim"
						}
					],
					"transformations": [
						{
							"name": "acctspfmrgd"
						},
						{
							"name": "SrtTgtacctspfdim"
						},
						{
							"name": "mergdAcctspf"
						},
						{
							"name": "Addrowstat"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ACCTSPFC_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ACCTSPFC_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     ACCTS.PROC_ID as ACCTSPFC_DIM_UID,\\n     ACCTS.PROJECT_ID as PRJCT_ID,\\n     cast(ACCTS.REMARKS as varchar(4096)) as ACCTSPFC_REM_TXT,\\n     ACCTS.CREATED_TS as SRC_CRETD_TMS,\\n     ACCTS.CREATED_USERID as SRC_CRETD_USER_ID,\\n     ACCTS.UPDATED_TS as SRC_UPDTD_TMS,\\n     ACCTS.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nfrom\\n     APPFUN.ACCTSPFC ACCTS\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on ACCTS.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on ACCTS.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\')JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\') FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\')SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> appfunacctspcf",
						"source(output(",
						"          LKP_ACCTSPFC_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_ as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     S.ACCTSPFC_DIM_UID as LKP_ACCTSPFC_DIM_UID,\\n     S.PRJCT_ID AS LKP_PRJCT_ID,\\n     S.ACCTSPFC_REM_TXT AS LKP_,\\n     S.SRC_CRETD_TMS AS LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID AS LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS AS LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID AS LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID AS LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID AS LKP_ETL_JOB_ID\\nfrom\\n     PGMPDM.ACCTSPFC_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.ACCTSPFC_DIM_UID = ZDT.PROC_ID\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> LkpTgtAcctspcfDim",
						"appfunacctspcf, SrtTgtacctspfdim join(ACCTSPFC_DIM_UID == LKP_ACCTSPFC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> acctspfmrgd",
						"LkpTgtAcctspcfDim sort(asc(LKP_ACCTSPFC_DIM_UID, true)) ~> SrtTgtacctspfdim",
						"acctspfmrgd split(ACCTSPFC_DIM_UID==LKP_ACCTSPFC_DIM_UID,",
						"     disjoint: false) ~> mergdAcctspf@(datatobeupdated, datatobeinserted)",
						"mergdAcctspf@datatobeupdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> Addrowstat",
						"Addrowstat select(mapColumn(",
						"          ACCTSPFC_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTSPFC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          LKP_ACCTSPFC_DIM_UID,",
						"          LKP_PRJCT_ID,",
						"          LKP_,",
						"          LKP_SRC_CRETD_TMS,",
						"          LKP_SRC_CRETD_USER_ID,",
						"          LKP_SRC_UPDTD_TMS,",
						"          LKP_SRC_UPDTD_USER_ID,",
						"          LKP_SRC_SYS_DIM_UID,",
						"          LKP_ETL_JOB_ID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"mergdAcctspf@datatobeinserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> derivedColumn1",
						"derivedColumn1 select(mapColumn(",
						"          ACCTSPFC_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTSPFC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          LKP_ACCTSPFC_DIM_UID,",
						"          LKP_PRJCT_ID,",
						"          LKP_,",
						"          LKP_SRC_CRETD_TMS,",
						"          LKP_SRC_CRETD_USER_ID,",
						"          LKP_SRC_UPDTD_TMS,",
						"          LKP_SRC_UPDTD_USER_ID,",
						"          LKP_SRC_SYS_DIM_UID,",
						"          LKP_ETL_JOB_ID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTSPFC_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['ACCTSPFC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> TgtAcctspfDimupd",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTSPFC_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> TgtpgmdmacctspfDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD1090_CNTRCT_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						}
					],
					"scriptLines": [
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as long,",
						"          CNTRCT_ID as integer,",
						"          CLNT_DIM_UID as integer,",
						"          SO_NONSO_DIM_UID as integer,",
						"          CNTRCT_STATE_CD as string,",
						"          CNTCT_TXT as string,",
						"          CNTRCT_NUM as string,",
						"          CNTRCT_NM as string,",
						"          CNTRCT_ATTCHMT_LCTN_TXT as string,",
						"          START_DT as date,",
						"          END_DT as date,",
						"          LINE_OF_BUS_TYPE_CD as string,",
						"          INDSTRY_TXT as string,",
						"          INDSTRY_SRC_TXT as string,",
						"          VRFYD_DT as date,",
						"          INDSTRY_OVERRDEN_IND as string,",
						"          GEO_DIM_UID as integer,",
						"          SCTR_DIM_UID as integer,",
						"          CNTRCT_TYPE_DIM_UID as integer,",
						"          PRVT_IND as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          CLIENT_UNIT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     (Select \\n          coalesce(max(CNTRCT_DIM_UID), 0) \\n      From PGMPDM.CNTRCT_DIM\\n     ) as MAX_ID,\\n     ROW_NUMBER() over (order by C.CNTRCT_ID) as ROW_NUM,\\n     C.CNTRCT_ID as CNTRCT_ID,\\n     C.CLIENT_ID as CLNT_DIM_UID, -- The CLIENT_ID is reused as CLNT_DIM_UID, not look up required\\n     SND.SO_NONSO_DIM_UID as SO_NONSO_DIM_UID,\\n     coalesce(C.STATUS, PC.STATUS) as CNTRCT_STATE_CD,\\n     coalesce(C.CNTCT, PC.CNTCT) as CNTCT_TXT,\\n     C.CNTRCT_NUMBER as CNTRCT_NUM,\\n     C.NAME as CNTRCT_NM,\\n     C.CNTRCT_ATTCHMNT_LOC as CNTRCT_ATTCHMT_LCTN_TXT,\\n     C.CNTRCT_START_DATE     as START_DT,\\n     C.CNTRCT_END_DATE as END_DT,\\n     PC.LINE_OF_BUSINESS_TYPE as LINE_OF_BUS_TYPE_CD,\\n     PC.INDUSTRY_TXT as INDSTRY_TXT,\\n     PC.INDUSTRY_SOURCE_TXT as INDSTRY_SRC_TXT,\\n     PC.VERIFIED_DATE as VRFYD_DT,\\n     PC.INDUSTRY_OVERRIDDEN_FLAG as INDSTRY_OVERRDEN_IND,\\n     G.GEO_DIM_UID as GEO_DIM_UID,\\n     coalesce(SD.SCTR_DIM_UID, -1) as SCTR_DIM_UID,\\n     coalesce(CT.CNTRCT_TYPE_DIM_UID, -1) as CNTRCT_TYPE_DIM_UID,\\n     case \\n          when PRIVATE_CONTRACTS.CNTRCT_ID is null then cast(\\'N\\' as char(1)) \\n          else cast(\\'Y\\' as char(1)) \\n     end as PRVT_IND,\\n     C.CREATED_TS as SRC_CRETD_TMS,\\n     C.CREATED_USERID as SRC_CRETD_USER_ID,\\n     C.UPDATED_TS as SRC_UPDTD_TMS,\\n     C.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n    C.CLIENT_UNIT as CLIENT_UNIT\\nFrom\\n     APPFUN.CNTRCT C\\n     left join\\n     APPFUN.PST_CNTRCT PC\\n     on C.CNTRCT_ID = PC.PST_CNTRCT_ID\\n     -- SO_NONSO_DIM_UID: Find out if the contract is defined as Non-SO\\n     left join\\n     APPFUN.NON_SO_CNTRCT_TMF SOTMF\\n     on C.CNTRCT_ID = SOTMF.CNTRCT_ID\\n     left join\\n     PGMPDM.SO_NONSO_DIM SND\\n     -- The case validates if the TMF record for the contract is null\\n     -- If the value is null, the contract is NOT considered Non-SO, which means \"SO\"\\n     -- Otherwise, if the contract id is found in the tmf table, \\n     -- it implies that the record should be treated as \"NON_SO\" \\n     on case when SOTMF.CNTRCT_ID is null then \\'SO\\' else \\'NON_SO\\' end = SND.SO_NONSO_CD\\n     -- GEO_DIM_UID: find out the corresponding GEO record\\n    left join\\n    APPFUN.ORG_CNTRCT_MAP OCM \\n    on C.CNTRCT_ID = OCM.CNTRCT_ID\\n    left join\\n    APPFUN.PST_ORG O \\n    on OCM.ORG_ID = O.PST_ORG_ID\\n    left join\\n    PGMPDM.GEO_DIM G\\n    on O.CNTRY = G.SRGN_CD     \\n    -- SCTR_DIM_UID: find out the corresponding sector\\n     left join\\n     PGMPDM.SCTR_DIM SD\\n     on PC.INDUSTRY_TXT = SD.ISU_INDSTRY_CD         \\n     -- CNTRCT_TYPE_DIM_UID: Logic to find out the corresponding contract type\\n     left join \\n     PGMPDM.CNTRCT_TYPE_DIM CT\\n     on \\n          case\\n            when O.TST_ACCNT_IND = \\'Y\\' AND C.CNTRCT_ID = 1203950002 then \\'Production Contract\\'  -- Jira PGMP-931\\n               when /*O.PST_ORG_STATUS = \\'O\\' and */ O.ORG_ACTV_FLAG = \\'N\\' then \\'Inactive Contract\\'   ---  795819\\n               when O.TST_ACCNT_IND =\\'Y\\' then \\'Test Contract\\'\\n            when O.TST_ACCNT_IND =\\'I\\' then \\'Inactive Contract\\'    ------ added as part of 553286\\n               when C.STATUS = \\'A\\' and lower(C.NAME) like \\'test%\\' then \\'Test Contract\\'          \\n               else \\'Production Contract\\'\\n        end = CT.CNTRCT_TYPE_DESC\\n     -- Determination of the private contracts    \\n    left join (\\n          Select \\n               OCM.CNTRCT_ID\\n          From \\n               (\\n                    Select\\n                         ORG_ID\\n                    From\\n                         APPFUN.ORG R\\n                    Where\\n                         R.PARENT_ORG_ID is null\\n                         and upper(R.TITLE) like \\'%PRIVATE%\\'                               \\n               ) R\\n               left join\\n               APPFUN.ORG C1\\n               on R.ORG_ID = C1.PARENT_ORG_ID\\n               left join\\n               APPFUN.ORG C2\\n               on C1.ORG_ID = C2.PARENT_ORG_ID\\n               left join\\n               APPFUN.ORG C3\\n               on C2.ORG_ID = C3.PARENT_ORG_ID\\n               left join\\n               APPFUN.ORG_CNTRCT_MAP OCM\\n               on \\n                    C1.ORG_ID = OCM.ORG_ID\\n                    or C2.ORG_ID = OCM.ORG_ID\\n                    or C3.ORG_ID = OCM.ORG_ID\\n          Where\\n               OCM.CNTRCT_ID is not null     \\n     ) PRIVATE_CONTRACTS\\n    on C.CNTRCT_ID = PRIVATE_CONTRACTS.CNTRCT_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD1090_CNTRCT_DIM\\'          \\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID\\n          From\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'               \\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_CNTRCT_ID as integer,",
						"          LKP_CLNT_DIM_UID as integer,",
						"          LKP_SO_NONSO_DIM_UID as integer,",
						"          LKP_CNTRCT_STATE_CD as string,",
						"          LKP_CNTCT_TXT as string,",
						"          LKP_CNTRCT_NUM as string,",
						"          LKP_CNTRCT_NM as string,",
						"          LKP_CNTRCT_ATTCHMT_LCTN_TXT as string,",
						"          LKP_START_DT as date,",
						"          LKP_END_DT as date,",
						"          LKP_LINE_OF_BUS_TYPE_CD as string,",
						"          LKP_INDSTRY_TXT as string,",
						"          LKP_INDSTRY_SRC_TXT as string,",
						"          LKP_VRFYD_DT as date,",
						"          LKP_INDSTRY_OVERRDEN_IND as string,",
						"          LKP_GEO_DIM_UID as integer,",
						"          LKP_SCTR_DIM_UID as integer,",
						"          LKP_CNTRCT_TYPE_DIM_UID as integer,",
						"          LKP_PRVT_IND as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_CLIENT_UNIT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CNTRCT_ID as LKP_CNTRCT_ID,\\n     CLNT_DIM_UID as LKP_CLNT_DIM_UID,\\n     SO_NONSO_DIM_UID as LKP_SO_NONSO_DIM_UID,\\n     CNTRCT_STATE_CD as LKP_CNTRCT_STATE_CD,\\n     CNTCT_TXT as LKP_CNTCT_TXT,\\n     CNTRCT_NUM as LKP_CNTRCT_NUM,\\n     CNTRCT_NM as LKP_CNTRCT_NM,\\n     CNTRCT_ATTCHMT_LCTN_TXT as LKP_CNTRCT_ATTCHMT_LCTN_TXT,\\n     START_DT as LKP_START_DT,\\n     END_DT as LKP_END_DT,\\n     LINE_OF_BUS_TYPE_CD as LKP_LINE_OF_BUS_TYPE_CD,\\n     INDSTRY_TXT as LKP_INDSTRY_TXT,\\n     INDSTRY_SRC_TXT as LKP_INDSTRY_SRC_TXT,\\n     VRFYD_DT as LKP_VRFYD_DT,\\n     INDSTRY_OVERRDEN_IND as LKP_INDSTRY_OVERRDEN_IND,\\n     GEO_DIM_UID as LKP_GEO_DIM_UID,\\n     SCTR_DIM_UID as LKP_SCTR_DIM_UID,\\n     CNTRCT_TYPE_DIM_UID as LKP_CNTRCT_TYPE_DIM_UID,\\n     PRVT_IND as LKP_PRVT_IND,\\n     SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     ETL_JOB_ID as LKP_ETL_JOB_ID,\\n     SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n    \\'\\' as LKP_CLIENT_UNIT -- CLIENT_UNIT as LKP_CLIENT_UNIT\\nFrom\\n     PGMPDM.CNTRCT_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_CNTRCT_ID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"CDCval split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CNTRCT_ID == LKP_CNTRCT_ID,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CNTRCT_ID,",
						"          CLNT_DIM_UID,",
						"          SO_NONSO_DIM_UID,",
						"          CNTRCT_STATE_CD,",
						"          CNTCT_TXT,",
						"          CNTRCT_NUM,",
						"          CNTRCT_NM,",
						"          CNTRCT_ATTCHMT_LCTN_TXT,",
						"          START_DT,",
						"          END_DT,",
						"          LINE_OF_BUS_TYPE_CD,",
						"          INDSTRY_TXT,",
						"          INDSTRY_SRC_TXT,",
						"          VRFYD_DT,",
						"          INDSTRY_OVERRDEN_IND,",
						"          GEO_DIM_UID,",
						"          SCTR_DIM_UID,",
						"          CNTRCT_TYPE_DIM_UID,",
						"          PRVT_IND,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          CLIENT_UNIT,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = \"dsdam\",",
						"          DM_UPDTD_USER_ID = \"dsdam\",",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(CNTRCT_ID = MAX_ID+ROW_NUM,",
						"          DM_CRETD_USER_ID = \"dsdam\",",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = \"dsdam\",",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CNTRCT_ID,",
						"          CLNT_DIM_UID,",
						"          SO_NONSO_DIM_UID,",
						"          CNTRCT_STATE_CD,",
						"          CNTCT_TXT,",
						"          CNTRCT_NUM,",
						"          CNTRCT_NM,",
						"          CNTRCT_ATTCHMT_LCTN_TXT,",
						"          START_DT,",
						"          END_DT,",
						"          LINE_OF_BUS_TYPE_CD,",
						"          INDSTRY_TXT,",
						"          INDSTRY_SRC_TXT,",
						"          VRFYD_DT,",
						"          INDSTRY_OVERRDEN_IND,",
						"          GEO_DIM_UID,",
						"          SCTR_DIM_UID,",
						"          CNTRCT_TYPE_DIM_UID,",
						"          PRVT_IND,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          CLIENT_UNIT,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(CNTRCT_ID == toInteger(null()) && LKP_CNTRCT_ID != toInteger(null()), 'D', case(CNTRCT_ID != toInteger(null()) && LKP_CNTRCT_ID == toInteger(null()), 'I', 'U'))) ~> CDCval",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CNTRCT_ID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_CSTM_CNTRY_KOAF')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedSourceData"
						},
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          CNTRCT_DIM_UID as integer,",
						"          CNTRCT_NM as string,",
						"          KYNDRYL_ONLY_TEXT_1 as string,",
						"          UTS_KoT1 as timestamp,",
						"          UUID_KoT1 as binary,",
						"          KYNDRYL_ONLY_TEXT_2 as string,",
						"          UTS_KoT2 as timestamp,",
						"          UUID_KoT2 as binary,",
						"          KYNDRYL_ONLY_TEXT_3 as string,",
						"          UTS_KoT3 as timestamp,",
						"          UUID_KoT3 as binary,",
						"          KYNDRYL_ONLY_TEXT_4 as string,",
						"          UTS_KoT4 as timestamp,",
						"          UUID_KoT4 as binary,",
						"          KYNDRYL_ONLY_TEXT_5 as string,",
						"          UTS_KoT5 as timestamp,",
						"          UUID_KoT5 as binary,",
						"          KYNDRYL_ONLY_TEXT_6 as string,",
						"          UTS_KoT6 as timestamp,",
						"          UUID_KoT6 as binary,",
						"          KYNDRYL_ONLY_TEXT_7 as string,",
						"          UTS_KoT7 as timestamp,",
						"          UUID_KoT7 as binary,",
						"          KYNDRYL_ONLY_TEXT_8 as string,",
						"          UTS_KoT8 as timestamp,",
						"          UUID_KoT8 as binary,",
						"          KYNDRYL_ONLY_TEXT_9 as string,",
						"          UTS_KoT9 as timestamp,",
						"          UUID_KoT9 as binary,",
						"          KYNDRYL_ONLY_TEXT_10 as string,",
						"          UTS_KoT10 as timestamp,",
						"          UUID_KoT10 as binary,",
						"          KYNDRYL_ONLY_TEXT_11 as string,",
						"          UTS_KoT11 as timestamp,",
						"          UUID_KoT11 as binary,",
						"          KYNDRYL_ONLY_TEXT_12 as string,",
						"          UTS_KoT12 as timestamp,",
						"          UUID_KoT12 as binary,",
						"          KYNDRYL_ONLY_TEXT_13 as string,",
						"          UTS_KoT13 as timestamp,",
						"          UUID_KoT13 as binary,",
						"          KYNDRYL_ONLY_TEXT_14 as string,",
						"          UTS_KoT14 as timestamp,",
						"          UUID_KoT14 as binary,",
						"          KYNDRYL_ONLY_TEXT_15 as string,",
						"          UTS_KoT15 as timestamp,",
						"          UUID_KoT15 as binary,",
						"          KYNDRYL_ONLY_TEXT_16 as string,",
						"          UTS_KoT16 as timestamp,",
						"          UUID_KoT16 as binary,",
						"          KYNDRYL_ONLY_TEXT_17 as string,",
						"          UTS_KoT17 as timestamp,",
						"          UUID_KoT17 as binary,",
						"          KYNDRYL_ONLY_TEXT_18 as string,",
						"          UTS_KoT18 as timestamp,",
						"          UUID_KoT18 as binary,",
						"          KYNDRYL_ONLY_TEXT_19 as string,",
						"          UTS_KoT19 as timestamp,",
						"          UUID_KoT19 as binary,",
						"          KYNDRYL_ONLY_TEXT_20 as string,",
						"          UTS_KoT20 as timestamp,",
						"          UUID_KoT20 as binary,",
						"          KYNDRYL_ONLY_TEXT_21 as string,",
						"          UTS_KoT21 as timestamp,",
						"          UUID_KoT21 as binary,",
						"          KYNDRYL_ONLY_TEXT_22 as string,",
						"          UTS_KoT22 as timestamp,",
						"          UUID_KoT22 as binary,",
						"          KYNDRYL_ONLY_TEXT_23 as string,",
						"          UTS_KoT23 as timestamp,",
						"          UUID_KoT23 as binary,",
						"          KYNDRYL_ONLY_TEXT_24 as string,",
						"          UTS_KoT24 as timestamp,",
						"          UUID_KoT24 as binary,",
						"          KYNDRYL_ONLY_TEXT_25 as string,",
						"          UTS_KoT25 as timestamp,",
						"          UUID_KoT25 as binary,",
						"          KYNDRYL_ONLY_TEXT_26 as string,",
						"          UTS_KoT26 as timestamp,",
						"          UUID_KoT26 as string,",
						"          KYNDRYL_ONLY_TEXT_27 as string,",
						"          UTS_KoT27 as timestamp,",
						"          UUID_KoT27 as string,",
						"          KYNDRYL_ONLY_DATE_1 as date,",
						"          UTS_KoD1 as timestamp,",
						"          UUID_KoD1 as binary,",
						"          KYNDRYL_ONLY_DATE_2 as date,",
						"          UTS_KoD2 as timestamp,",
						"          UUID_KoD2 as binary,",
						"          KYNDRYL_ONLY_DATE_3 as date,",
						"          UTS_KoD3 as timestamp,",
						"          UUID_KoD3 as binary,",
						"          KYNDRYL_ONLY_DATE_4 as date,",
						"          UTS_KoD4 as timestamp,",
						"          UUID_KoD4 as binary,",
						"          KYNDRYL_ONLY_DATE_5 as date,",
						"          UTS_KoD5 as timestamp,",
						"          UUID_KoD5 as binary,",
						"          KYNDRYL_ONLY_DATE_6 as date,",
						"          UTS_KoD6 as timestamp,",
						"          UUID_KoD6 as binary,",
						"          KYNDRYL_ONLY_DATE_7 as date,",
						"          UTS_KoD7 as timestamp,",
						"          UUID_KoD7 as binary,",
						"          KYNDRYL_ONLY_DATE_8 as date,",
						"          UTS_KoD8 as timestamp,",
						"          UUID_KoD8 as binary,",
						"          KYNDRYL_ONLY_DATE_9 as date,",
						"          UTS_KoD9 as timestamp,",
						"          UUID_KoD9 as binary,",
						"          KYNDRYL_ONLY_DATE_10 as date,",
						"          UTS_KoD10 as timestamp,",
						"          UUID_KoD10 as binary,",
						"          KYNDRYL_ONLY_DATE_11 as date,",
						"          UTS_KoD11 as timestamp,",
						"          UUID_KoD11 as binary,",
						"          KYNDRYL_ONLY_DATE_12 as date,",
						"          UTS_KoD12 as timestamp,",
						"          UUID_KoD12 as binary,",
						"          KYNDRYL_ONLY_DATE_13 as date,",
						"          UTS_KoD13 as timestamp,",
						"          UUID_KoD13 as binary,",
						"          KYNDRYL_ONLY_DATE_14 as date,",
						"          UTS_KoD14 as timestamp,",
						"          UUID_KoD14 as binary,",
						"          KYNDRYL_ONLY_DATE_15 as date,",
						"          UTS_KoD15 as timestamp,",
						"          UUID_KoD15 as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select distinct\\n     PROCS.PROC_DIM_UID,\\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM,\\n    KYNDRYL_ONLY_TEXT_1.KYNDRYL_ONLY_TEXT_1,\\n     KYNDRYL_ONLY_TEXT_1.UPDATED_TIMESTAMP AS UTS_KoT1,\\n     KYNDRYL_ONLY_TEXT_1.USER_ID AS UUID_KoT1,\\n    KYNDRYL_ONLY_TEXT_2.KYNDRYL_ONLY_TEXT_2,\\n     KYNDRYL_ONLY_TEXT_2.UPDATED_TIMESTAMP AS UTS_KoT2,\\n     KYNDRYL_ONLY_TEXT_2.USER_ID AS UUID_KoT2,\\n    KYNDRYL_ONLY_TEXT_3.KYNDRYL_ONLY_TEXT_3,\\n     KYNDRYL_ONLY_TEXT_3.UPDATED_TIMESTAMP AS UTS_KoT3,\\n     KYNDRYL_ONLY_TEXT_3.USER_ID AS UUID_KoT3,\\n    KYNDRYL_ONLY_TEXT_4.KYNDRYL_ONLY_TEXT_4,\\n     KYNDRYL_ONLY_TEXT_4.UPDATED_TIMESTAMP AS UTS_KoT4,\\n     KYNDRYL_ONLY_TEXT_4.USER_ID AS UUID_KoT4,\\n    KYNDRYL_ONLY_TEXT_5.KYNDRYL_ONLY_TEXT_5,\\n     KYNDRYL_ONLY_TEXT_5.UPDATED_TIMESTAMP AS UTS_KoT5,\\n     KYNDRYL_ONLY_TEXT_5.USER_ID AS UUID_KoT5,\\n    KYNDRYL_ONLY_TEXT_6.KYNDRYL_ONLY_TEXT_6,\\n     KYNDRYL_ONLY_TEXT_6.UPDATED_TIMESTAMP AS UTS_KoT6,\\n     KYNDRYL_ONLY_TEXT_6.USER_ID AS UUID_KoT6,\\n    KYNDRYL_ONLY_TEXT_7.KYNDRYL_ONLY_TEXT_7,\\n     KYNDRYL_ONLY_TEXT_7.UPDATED_TIMESTAMP AS UTS_KoT7,\\n     KYNDRYL_ONLY_TEXT_7.USER_ID AS UUID_KoT7,\\n    KYNDRYL_ONLY_TEXT_8.KYNDRYL_ONLY_TEXT_8,\\n     KYNDRYL_ONLY_TEXT_8.UPDATED_TIMESTAMP AS UTS_KoT8,\\n     KYNDRYL_ONLY_TEXT_8.USER_ID AS UUID_KoT8,\\n    KYNDRYL_ONLY_TEXT_9.KYNDRYL_ONLY_TEXT_9,\\n     KYNDRYL_ONLY_TEXT_9.UPDATED_TIMESTAMP AS UTS_KoT9,\\n     KYNDRYL_ONLY_TEXT_9.USER_ID AS UUID_KoT9,\\n    KYNDRYL_ONLY_TEXT_10.KYNDRYL_ONLY_TEXT_10,\\n     KYNDRYL_ONLY_TEXT_10.UPDATED_TIMESTAMP AS UTS_KoT10,\\n     KYNDRYL_ONLY_TEXT_10.USER_ID AS UUID_KoT10,\\n    KYNDRYL_ONLY_TEXT_11.KYNDRYL_ONLY_TEXT_11,\\n     KYNDRYL_ONLY_TEXT_11.UPDATED_TIMESTAMP AS UTS_KoT11,\\n     KYNDRYL_ONLY_TEXT_11.USER_ID AS UUID_KoT11,\\n    KYNDRYL_ONLY_TEXT_12.KYNDRYL_ONLY_TEXT_12,\\n     KYNDRYL_ONLY_TEXT_12.UPDATED_TIMESTAMP AS UTS_KoT12,\\n     KYNDRYL_ONLY_TEXT_12.USER_ID AS UUID_KoT12,\\n    KYNDRYL_ONLY_TEXT_13.KYNDRYL_ONLY_TEXT_13,\\n     KYNDRYL_ONLY_TEXT_13.UPDATED_TIMESTAMP AS UTS_KoT13,\\n     KYNDRYL_ONLY_TEXT_13.USER_ID AS UUID_KoT13,\\n    KYNDRYL_ONLY_TEXT_14.KYNDRYL_ONLY_TEXT_14,\\n     KYNDRYL_ONLY_TEXT_14.UPDATED_TIMESTAMP AS UTS_KoT14,\\n     KYNDRYL_ONLY_TEXT_14.USER_ID AS UUID_KoT14,\\n    KYNDRYL_ONLY_TEXT_15.KYNDRYL_ONLY_TEXT_15,\\n     KYNDRYL_ONLY_TEXT_15.UPDATED_TIMESTAMP AS UTS_KoT15,\\n     KYNDRYL_ONLY_TEXT_15.USER_ID AS UUID_KoT15,\\n    KYNDRYL_ONLY_TEXT_16.KYNDRYL_ONLY_TEXT_16,\\n     KYNDRYL_ONLY_TEXT_16.UPDATED_TIMESTAMP AS UTS_KoT16,\\n     KYNDRYL_ONLY_TEXT_16.USER_ID AS UUID_KoT16,\\n    KYNDRYL_ONLY_TEXT_17.KYNDRYL_ONLY_TEXT_17,\\n     KYNDRYL_ONLY_TEXT_17.UPDATED_TIMESTAMP AS UTS_KoT17,\\n     KYNDRYL_ONLY_TEXT_17.USER_ID AS UUID_KoT17,\\n    KYNDRYL_ONLY_TEXT_18.KYNDRYL_ONLY_TEXT_18,\\n     KYNDRYL_ONLY_TEXT_18.UPDATED_TIMESTAMP AS UTS_KoT18,\\n     KYNDRYL_ONLY_TEXT_18.USER_ID AS UUID_KoT18,\\n    KYNDRYL_ONLY_TEXT_19.KYNDRYL_ONLY_TEXT_19,\\n     KYNDRYL_ONLY_TEXT_19.UPDATED_TIMESTAMP AS UTS_KoT19,\\n     KYNDRYL_ONLY_TEXT_19.USER_ID AS UUID_KoT19,\\n    KYNDRYL_ONLY_TEXT_20.KYNDRYL_ONLY_TEXT_20,\\n     KYNDRYL_ONLY_TEXT_20.UPDATED_TIMESTAMP AS UTS_KoT20,\\n     KYNDRYL_ONLY_TEXT_20.USER_ID AS UUID_KoT20,\\n    KYNDRYL_ONLY_TEXT_21.KYNDRYL_ONLY_TEXT_21,\\n     KYNDRYL_ONLY_TEXT_21.UPDATED_TIMESTAMP AS UTS_KoT21,\\n     KYNDRYL_ONLY_TEXT_21.USER_ID AS UUID_KoT21,\\n    KYNDRYL_ONLY_TEXT_22.KYNDRYL_ONLY_TEXT_22,\\n     KYNDRYL_ONLY_TEXT_22.UPDATED_TIMESTAMP AS UTS_KoT22,\\n     KYNDRYL_ONLY_TEXT_22.USER_ID AS UUID_KoT22,\\n    KYNDRYL_ONLY_TEXT_23.KYNDRYL_ONLY_TEXT_23,\\n     KYNDRYL_ONLY_TEXT_23.UPDATED_TIMESTAMP AS UTS_KoT23,\\n     KYNDRYL_ONLY_TEXT_23.USER_ID AS UUID_KoT23,\\n    KYNDRYL_ONLY_TEXT_24.KYNDRYL_ONLY_TEXT_24,\\n     KYNDRYL_ONLY_TEXT_24.UPDATED_TIMESTAMP AS UTS_KoT24,\\n     KYNDRYL_ONLY_TEXT_24.USER_ID AS UUID_KoT24,\\n    KYNDRYL_ONLY_TEXT_25.KYNDRYL_ONLY_TEXT_25,\\n     KYNDRYL_ONLY_TEXT_25.UPDATED_TIMESTAMP AS UTS_KoT25,\\n     KYNDRYL_ONLY_TEXT_25.USER_ID AS UUID_KoT25,\\n    \\'\\' AS KYNDRYL_ONLY_TEXT_26, --KYNDRYL_ONLY_TEXT_26.KYNDRYL_ONLY_TEXT_26,\\n     CURRENT_TIMESTAMP AS UTS_KoT26, --KYNDRYL_ONLY_TEXT_26.UPDATED_TIMESTAMP AS UTS_KoT26,\\n     \\'\\' AS UUID_KoT26, --KYNDRYL_ONLY_TEXT_26.USER_ID AS UUID_KoT26,\\n    \\'\\' AS KYNDRYL_ONLY_TEXT_27, --KYNDRYL_ONLY_TEXT_27.KYNDRYL_ONLY_TEXT_27,\\n     CURRENT_TIMESTAMP AS UTS_KoT27, --KYNDRYL_ONLY_TEXT_27.UPDATED_TIMESTAMP AS UTS_KoT27,\\n     \\'\\' AS UUID_KoT27, --KYNDRYL_ONLY_TEXT_27.USER_ID AS UUID_KoT27,\\n    KYNDRYL_ONLY_DATE_1.KYNDRYL_ONLY_DATE_1,\\n     KYNDRYL_ONLY_DATE_1.UPDATED_TIMESTAMP AS UTS_KoD1,\\n     KYNDRYL_ONLY_DATE_1.USER_ID AS UUID_KoD1,\\n    KYNDRYL_ONLY_DATE_2.KYNDRYL_ONLY_DATE_2,\\n     KYNDRYL_ONLY_DATE_2.UPDATED_TIMESTAMP AS UTS_KoD2,\\n     KYNDRYL_ONLY_DATE_2.USER_ID AS UUID_KoD2,\\n    KYNDRYL_ONLY_DATE_3.KYNDRYL_ONLY_DATE_3,\\n     KYNDRYL_ONLY_DATE_3.UPDATED_TIMESTAMP AS UTS_KoD3,\\n     KYNDRYL_ONLY_DATE_3.USER_ID AS UUID_KoD3,\\n    KYNDRYL_ONLY_DATE_4.KYNDRYL_ONLY_DATE_4,\\n     KYNDRYL_ONLY_DATE_4.UPDATED_TIMESTAMP AS UTS_KoD4,\\n     KYNDRYL_ONLY_DATE_4.USER_ID AS UUID_KoD4,\\n    KYNDRYL_ONLY_DATE_5.KYNDRYL_ONLY_DATE_5,\\n     KYNDRYL_ONLY_DATE_5.UPDATED_TIMESTAMP AS UTS_KoD5,\\n     KYNDRYL_ONLY_DATE_5.USER_ID AS UUID_KoD5,\\n    KYNDRYL_ONLY_DATE_6.KYNDRYL_ONLY_DATE_6,\\n     KYNDRYL_ONLY_DATE_6.UPDATED_TIMESTAMP AS UTS_KoD6,\\n     KYNDRYL_ONLY_DATE_6.USER_ID AS UUID_KoD6,\\n    KYNDRYL_ONLY_DATE_7.KYNDRYL_ONLY_DATE_7,\\n     KYNDRYL_ONLY_DATE_7.UPDATED_TIMESTAMP AS UTS_KoD7,\\n     KYNDRYL_ONLY_DATE_7.USER_ID AS UUID_KoD7,\\n    KYNDRYL_ONLY_DATE_8.KYNDRYL_ONLY_DATE_8,\\n     KYNDRYL_ONLY_DATE_8.UPDATED_TIMESTAMP AS UTS_KoD8,\\n     KYNDRYL_ONLY_DATE_8.USER_ID AS UUID_KoD8,\\n    KYNDRYL_ONLY_DATE_9.KYNDRYL_ONLY_DATE_9,\\n     KYNDRYL_ONLY_DATE_9.UPDATED_TIMESTAMP AS UTS_KoD9,\\n     KYNDRYL_ONLY_DATE_9.USER_ID AS UUID_KoD9,\\n    KYNDRYL_ONLY_DATE_10.KYNDRYL_ONLY_DATE_10,\\n     KYNDRYL_ONLY_DATE_10.UPDATED_TIMESTAMP AS UTS_KoD10,\\n     KYNDRYL_ONLY_DATE_10.USER_ID AS UUID_KoD10,\\n    KYNDRYL_ONLY_DATE_11.KYNDRYL_ONLY_DATE_11,\\n     KYNDRYL_ONLY_DATE_11.UPDATED_TIMESTAMP AS UTS_KoD11,\\n     KYNDRYL_ONLY_DATE_11.USER_ID AS UUID_KoD11,\\n    KYNDRYL_ONLY_DATE_12.KYNDRYL_ONLY_DATE_12,\\n     KYNDRYL_ONLY_DATE_12.UPDATED_TIMESTAMP AS UTS_KoD12,\\n     KYNDRYL_ONLY_DATE_12.USER_ID AS UUID_KoD12,\\n    KYNDRYL_ONLY_DATE_13.KYNDRYL_ONLY_DATE_13,\\n     KYNDRYL_ONLY_DATE_13.UPDATED_TIMESTAMP AS UTS_KoD13,\\n     KYNDRYL_ONLY_DATE_13.USER_ID AS UUID_KoD13,\\n    KYNDRYL_ONLY_DATE_14.KYNDRYL_ONLY_DATE_14,\\n     KYNDRYL_ONLY_DATE_14.UPDATED_TIMESTAMP AS UTS_KoD14,\\n     KYNDRYL_ONLY_DATE_14.USER_ID AS UUID_KoD14,\\n    KYNDRYL_ONLY_DATE_15.KYNDRYL_ONLY_DATE_15,\\n     KYNDRYL_ONLY_DATE_15.UPDATED_TIMESTAMP AS UTS_KoD15,\\n     KYNDRYL_ONLY_DATE_15.USER_ID AS UUID_KoD15\\nFrom\\n     PGMPDM.PROC_DIM PROCS\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID\\n     INNER JOIN (\\n          Select distinct\\n               PROC_DIM_UID\\n          From     \\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM\\n          Where     \\n               COALESCE(\\n                    IBM_ONLY_TEXT_1,\\n                    IBM_ONLY_TEXT_2,\\n                    IBM_ONLY_TEXT_3,\\n                    IBM_ONLY_TEXT_4,\\n                    IBM_ONLY_TEXT_5,\\n                    IBM_ONLY_TEXT_6,\\n                    IBM_ONLY_TEXT_7,\\n                    IBM_ONLY_TEXT_8,\\n                    IBM_ONLY_TEXT_9,\\n                    IBM_ONLY_TEXT_10,\\n                    IBM_ONLY_TEXT_11,\\n                    IBM_ONLY_TEXT_12,\\n                    IBM_ONLY_TEXT_13,\\n                    IBM_ONLY_TEXT_14,\\n                    IBM_ONLY_TEXT_15,\\n                    IBM_ONLY_TEXT_16,\\n                    IBM_ONLY_TEXT_17,\\n                    IBM_ONLY_TEXT_18,\\n                    IBM_ONLY_TEXT_19,\\n                    IBM_ONLY_TEXT_20,\\n                    IBM_ONLY_TEXT_21,\\n                    IBM_ONLY_TEXT_22,\\n                    IBM_ONLY_TEXT_23,\\n                    IBM_ONLY_TEXT_24,\\n                    IBM_ONLY_TEXT_25,\\n                    --IBM_ONLY_TEXT_26,\\n                    --IBM_ONLY_TEXT_27,\\n                    CAST(IBM_ONLY_DATE_1 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_2 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_3 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_4 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_5 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_6 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_7 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_8 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_9 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_10 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_11 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_12 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_13 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_14 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_15 AS VARCHAR)\\n               ) NOT IN (\\'\\', \\'NULL\\')\\n     ) KoPROC ON KoPROC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_1 AS KYNDRYL_ONLY_TEXT_1,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_1                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_1 = DKo.IBM_ONLY_TEXT_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_1 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_1 ON KYNDRYL_ONLY_TEXT_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_2 AS KYNDRYL_ONLY_TEXT_2,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_2                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_2 = DKo.IBM_ONLY_TEXT_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_2 not in (\\'\\') \\n     )KYNDRYL_ONLY_TEXT_2 ON KYNDRYL_ONLY_TEXT_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_3 AS KYNDRYL_ONLY_TEXT_3,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_3                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_3 = DKo.IBM_ONLY_TEXT_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_3 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_3 ON KYNDRYL_ONLY_TEXT_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_4 AS KYNDRYL_ONLY_TEXT_4,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP               \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_4                              \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_4 = DKo.IBM_ONLY_TEXT_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_4 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_4 ON KYNDRYL_ONLY_TEXT_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_5 AS KYNDRYL_ONLY_TEXT_5,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_5               \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_5 = DKo.IBM_ONLY_TEXT_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_5 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_5 ON KYNDRYL_ONLY_TEXT_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_6 AS KYNDRYL_ONLY_TEXT_6,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_6                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_6 = DKo.IBM_ONLY_TEXT_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_6 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_6 ON KYNDRYL_ONLY_TEXT_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_7 AS KYNDRYL_ONLY_TEXT_7,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_7                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_7 = DKo.IBM_ONLY_TEXT_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_7 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_7 ON KYNDRYL_ONLY_TEXT_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_8 AS KYNDRYL_ONLY_TEXT_8,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_8                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_8 = DKo.IBM_ONLY_TEXT_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_8 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_8 ON KYNDRYL_ONLY_TEXT_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_9 AS KYNDRYL_ONLY_TEXT_9,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_9                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_9 = DKo.IBM_ONLY_TEXT_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_9 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_9 ON KYNDRYL_ONLY_TEXT_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_10 AS KYNDRYL_ONLY_TEXT_10,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_10                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_10 = DKo.IBM_ONLY_TEXT_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_10 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_10 ON KYNDRYL_ONLY_TEXT_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_11 AS KYNDRYL_ONLY_TEXT_11,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_11                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_11 = DKo.IBM_ONLY_TEXT_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_11 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_11 ON KYNDRYL_ONLY_TEXT_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_12 AS KYNDRYL_ONLY_TEXT_12,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_12                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_12 = DKo.IBM_ONLY_TEXT_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_12 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_12 ON KYNDRYL_ONLY_TEXT_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_13 AS KYNDRYL_ONLY_TEXT_13,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_13                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_13 = DKo.IBM_ONLY_TEXT_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_13 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_13 ON KYNDRYL_ONLY_TEXT_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_14 AS KYNDRYL_ONLY_TEXT_14,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_14\\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_14 = DKo.IBM_ONLY_TEXT_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_14 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_14 ON KYNDRYL_ONLY_TEXT_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_15 AS KYNDRYL_ONLY_TEXT_15,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_15                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_15 = DKo.IBM_ONLY_TEXT_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_15 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_15 ON KYNDRYL_ONLY_TEXT_15.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_16 AS KYNDRYL_ONLY_TEXT_16,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_16,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_16                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_16 = DKo.IBM_ONLY_TEXT_16)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_16 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_16 ON KYNDRYL_ONLY_TEXT_16.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_17 AS KYNDRYL_ONLY_TEXT_17,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_17,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_17                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_17 = DKo.IBM_ONLY_TEXT_17)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_17 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_17 ON KYNDRYL_ONLY_TEXT_17.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_18 AS KYNDRYL_ONLY_TEXT_18,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_18,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_18                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_18 = DKo.IBM_ONLY_TEXT_18)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_18 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_18 ON KYNDRYL_ONLY_TEXT_18.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_19 AS KYNDRYL_ONLY_TEXT_19,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_19,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_19                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_19 = DKo.IBM_ONLY_TEXT_19)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_19 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_19 ON KYNDRYL_ONLY_TEXT_19.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_20 AS KYNDRYL_ONLY_TEXT_20,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_20,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_20                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_20 = DKo.IBM_ONLY_TEXT_20)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_20 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_20 ON KYNDRYL_ONLY_TEXT_20.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_21 AS KYNDRYL_ONLY_TEXT_21,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_21,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_21                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_21 = DKo.IBM_ONLY_TEXT_21)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_21 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_21 ON KYNDRYL_ONLY_TEXT_21.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_22 AS KYNDRYL_ONLY_TEXT_22,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_22,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_22                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_22 = DKo.IBM_ONLY_TEXT_22)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_22 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_22 ON KYNDRYL_ONLY_TEXT_22.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_23 AS KYNDRYL_ONLY_TEXT_23,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_23,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_23                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_23 = DKo.IBM_ONLY_TEXT_23)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_23 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_23 ON KYNDRYL_ONLY_TEXT_23.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_24 AS KYNDRYL_ONLY_TEXT_24,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_24,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_24                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_24 = DKo.IBM_ONLY_TEXT_24)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_24 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_24 ON KYNDRYL_ONLY_TEXT_24.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_25 AS KYNDRYL_ONLY_TEXT_25,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_25,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_25                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_25 = DKo.IBM_ONLY_TEXT_25)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_25 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_25 ON KYNDRYL_ONLY_TEXT_25.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    /*\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_26 AS KYNDRYL_ONLY_TEXT_26,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_26,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_26                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_26 = DKo.IBM_ONLY_TEXT_26)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_26 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_26 ON KYNDRYL_ONLY_TEXT_26.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_27 AS KYNDRYL_ONLY_TEXT_27,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_27,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_27               \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_27 = DKo.IBM_ONLY_TEXT_27)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_27 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_27 ON KYNDRYL_ONLY_TEXT_27.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     */\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_1 AS KYNDRYL_ONLY_DATE_1,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_1                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_1 = DKo.IBM_ONLY_DATE_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_1 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_1 ON KYNDRYL_ONLY_DATE_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_2 AS KYNDRYL_ONLY_DATE_2,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_2                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_2 = DKo.IBM_ONLY_DATE_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_2 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_2 ON KYNDRYL_ONLY_DATE_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_3 AS KYNDRYL_ONLY_DATE_3,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_3                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_3 = DKo.IBM_ONLY_DATE_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_3 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_3 ON KYNDRYL_ONLY_DATE_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_4 AS KYNDRYL_ONLY_DATE_4,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP     \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_4                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_4 = DKo.IBM_ONLY_DATE_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_4 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_4 ON KYNDRYL_ONLY_DATE_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_5 AS KYNDRYL_ONLY_DATE_5,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_5                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_5 = DKo.IBM_ONLY_DATE_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_5 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_5 ON KYNDRYL_ONLY_DATE_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_6 AS KYNDRYL_ONLY_DATE_6,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_6                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_6 = DKo.IBM_ONLY_DATE_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_6 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_6 ON KYNDRYL_ONLY_DATE_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_7 AS KYNDRYL_ONLY_DATE_7,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_7                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_7 = DKo.IBM_ONLY_DATE_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_7 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_7 ON KYNDRYL_ONLY_DATE_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_8 AS KYNDRYL_ONLY_DATE_8,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_8                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_8 = DKo.IBM_ONLY_DATE_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_8 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_8 ON KYNDRYL_ONLY_DATE_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_9 AS KYNDRYL_ONLY_DATE_9,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_9                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_9 = DKo.IBM_ONLY_DATE_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_9 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_9 ON KYNDRYL_ONLY_DATE_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_10 AS KYNDRYL_ONLY_DATE_10,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_10                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_10 = DKo.IBM_ONLY_DATE_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_10 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_10 ON KYNDRYL_ONLY_DATE_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_11 AS KYNDRYL_ONLY_DATE_11,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_11                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_11 = DKo.IBM_ONLY_DATE_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_11 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_11 ON KYNDRYL_ONLY_DATE_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_12 AS KYNDRYL_ONLY_DATE_12,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_12                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_12 = DKo.IBM_ONLY_DATE_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_12 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_12 ON KYNDRYL_ONLY_DATE_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_13 AS KYNDRYL_ONLY_DATE_13,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_13                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_13 = DKo.IBM_ONLY_DATE_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_13 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_13 ON KYNDRYL_ONLY_DATE_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_14 AS KYNDRYL_ONLY_DATE_14,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_14                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_14 = DKo.IBM_ONLY_DATE_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_14 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_14 ON KYNDRYL_ONLY_DATE_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_15 AS KYNDRYL_ONLY_DATE_15,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_15                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_15 = DKo.IBM_ONLY_DATE_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_15 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_15 ON KYNDRYL_ONLY_DATE_15.PROC_DIM_UID = PROCS.PROC_DIM_UID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData sort(asc(PROC_DIM_UID, false)) ~> sortedSourceData",
						"sortedSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTRPTS_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_CSTM_CNTRY_PAF')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedSourceData"
						},
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          CNTRCT_DIM_UID as integer,",
						"          CNTRCT_NM as string,",
						"          KYNDRYL_ONLY_TEXT_1 as string,",
						"          UTS_KoT1 as timestamp,",
						"          UUID_KoT1 as binary,",
						"          KYNDRYL_ONLY_TEXT_2 as string,",
						"          UTS_KoT2 as timestamp,",
						"          UUID_KoT2 as binary,",
						"          KYNDRYL_ONLY_TEXT_3 as string,",
						"          UTS_KoT3 as timestamp,",
						"          UUID_KoT3 as binary,",
						"          KYNDRYL_ONLY_TEXT_4 as string,",
						"          UTS_KoT4 as timestamp,",
						"          UUID_KoT4 as binary,",
						"          KYNDRYL_ONLY_TEXT_5 as string,",
						"          UTS_KoT5 as timestamp,",
						"          UUID_KoT5 as binary,",
						"          KYNDRYL_ONLY_TEXT_6 as string,",
						"          UTS_KoT6 as timestamp,",
						"          UUID_KoT6 as binary,",
						"          KYNDRYL_ONLY_TEXT_7 as string,",
						"          UTS_KoT7 as timestamp,",
						"          UUID_KoT7 as binary,",
						"          KYNDRYL_ONLY_TEXT_8 as string,",
						"          UTS_KoT8 as timestamp,",
						"          UUID_KoT8 as binary,",
						"          KYNDRYL_ONLY_TEXT_9 as string,",
						"          UTS_KoT9 as timestamp,",
						"          UUID_KoT9 as binary,",
						"          KYNDRYL_ONLY_TEXT_10 as string,",
						"          UTS_KoT10 as timestamp,",
						"          UUID_KoT10 as binary,",
						"          KYNDRYL_ONLY_TEXT_11 as string,",
						"          UTS_KoT11 as timestamp,",
						"          UUID_KoT11 as binary,",
						"          KYNDRYL_ONLY_TEXT_12 as string,",
						"          UTS_KoT12 as timestamp,",
						"          UUID_KoT12 as binary,",
						"          KYNDRYL_ONLY_TEXT_13 as string,",
						"          UTS_KoT13 as timestamp,",
						"          UUID_KoT13 as binary,",
						"          KYNDRYL_ONLY_TEXT_14 as string,",
						"          UTS_KoT14 as timestamp,",
						"          UUID_KoT14 as binary,",
						"          KYNDRYL_ONLY_TEXT_15 as string,",
						"          UTS_KoT15 as timestamp,",
						"          UUID_KoT15 as binary,",
						"          KYNDRYL_ONLY_TEXT_16 as string,",
						"          UTS_KoT16 as timestamp,",
						"          UUID_KoT16 as binary,",
						"          KYNDRYL_ONLY_TEXT_17 as string,",
						"          UTS_KoT17 as timestamp,",
						"          UUID_KoT17 as binary,",
						"          KYNDRYL_ONLY_TEXT_18 as string,",
						"          UTS_KoT18 as timestamp,",
						"          UUID_KoT18 as binary,",
						"          KYNDRYL_ONLY_TEXT_19 as string,",
						"          UTS_KoT19 as timestamp,",
						"          UUID_KoT19 as binary,",
						"          KYNDRYL_ONLY_TEXT_20 as string,",
						"          UTS_KoT20 as timestamp,",
						"          UUID_KoT20 as binary,",
						"          KYNDRYL_ONLY_TEXT_21 as string,",
						"          UTS_KoT21 as timestamp,",
						"          UUID_KoT21 as binary,",
						"          KYNDRYL_ONLY_TEXT_22 as string,",
						"          UTS_KoT22 as timestamp,",
						"          UUID_KoT22 as binary,",
						"          KYNDRYL_ONLY_TEXT_23 as string,",
						"          UTS_KoT23 as timestamp,",
						"          UUID_KoT23 as binary,",
						"          KYNDRYL_ONLY_TEXT_24 as string,",
						"          UTS_KoT24 as timestamp,",
						"          UUID_KoT24 as binary,",
						"          KYNDRYL_ONLY_TEXT_25 as string,",
						"          UTS_KoT25 as timestamp,",
						"          UUID_KoT25 as binary,",
						"          KYNDRYL_ONLY_TEXT_26 as string,",
						"          UTS_KoT26 as timestamp,",
						"          UUID_KoT26 as string,",
						"          KYNDRYL_ONLY_TEXT_27 as string,",
						"          UTS_KoT27 as timestamp,",
						"          UUID_KoT27 as string,",
						"          KYNDRYL_ONLY_DATE_1 as date,",
						"          UTS_KoD1 as timestamp,",
						"          UUID_KoD1 as binary,",
						"          KYNDRYL_ONLY_DATE_2 as date,",
						"          UTS_KoD2 as timestamp,",
						"          UUID_KoD2 as binary,",
						"          KYNDRYL_ONLY_DATE_3 as date,",
						"          UTS_KoD3 as timestamp,",
						"          UUID_KoD3 as binary,",
						"          KYNDRYL_ONLY_DATE_4 as date,",
						"          UTS_KoD4 as timestamp,",
						"          UUID_KoD4 as binary,",
						"          KYNDRYL_ONLY_DATE_5 as date,",
						"          UTS_KoD5 as timestamp,",
						"          UUID_KoD5 as binary,",
						"          KYNDRYL_ONLY_DATE_6 as date,",
						"          UTS_KoD6 as timestamp,",
						"          UUID_KoD6 as binary,",
						"          KYNDRYL_ONLY_DATE_7 as date,",
						"          UTS_KoD7 as timestamp,",
						"          UUID_KoD7 as binary,",
						"          KYNDRYL_ONLY_DATE_8 as date,",
						"          UTS_KoD8 as timestamp,",
						"          UUID_KoD8 as binary,",
						"          KYNDRYL_ONLY_DATE_9 as date,",
						"          UTS_KoD9 as timestamp,",
						"          UUID_KoD9 as binary,",
						"          KYNDRYL_ONLY_DATE_10 as date,",
						"          UTS_KoD10 as timestamp,",
						"          UUID_KoD10 as binary,",
						"          KYNDRYL_ONLY_DATE_11 as date,",
						"          UTS_KoD11 as timestamp,",
						"          UUID_KoD11 as binary,",
						"          KYNDRYL_ONLY_DATE_12 as date,",
						"          UTS_KoD12 as timestamp,",
						"          UUID_KoD12 as binary,",
						"          KYNDRYL_ONLY_DATE_13 as date,",
						"          UTS_KoD13 as timestamp,",
						"          UUID_KoD13 as binary,",
						"          KYNDRYL_ONLY_DATE_14 as date,",
						"          UTS_KoD14 as timestamp,",
						"          UUID_KoD14 as binary,",
						"          KYNDRYL_ONLY_DATE_15 as date,",
						"          UTS_KoD15 as timestamp,",
						"          UUID_KoD15 as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select distinct\\n     PROCS.PROC_DIM_UID,\\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM,\\n    KYNDRYL_ONLY_TEXT_1.KYNDRYL_ONLY_TEXT_1,\\n     KYNDRYL_ONLY_TEXT_1.UPDATED_TIMESTAMP AS UTS_KoT1,\\n     KYNDRYL_ONLY_TEXT_1.USER_ID AS UUID_KoT1,\\n    KYNDRYL_ONLY_TEXT_2.KYNDRYL_ONLY_TEXT_2,\\n     KYNDRYL_ONLY_TEXT_2.UPDATED_TIMESTAMP AS UTS_KoT2,\\n     KYNDRYL_ONLY_TEXT_2.USER_ID AS UUID_KoT2,\\n    KYNDRYL_ONLY_TEXT_3.KYNDRYL_ONLY_TEXT_3,\\n     KYNDRYL_ONLY_TEXT_3.UPDATED_TIMESTAMP AS UTS_KoT3,\\n     KYNDRYL_ONLY_TEXT_3.USER_ID AS UUID_KoT3,\\n    KYNDRYL_ONLY_TEXT_4.KYNDRYL_ONLY_TEXT_4,\\n     KYNDRYL_ONLY_TEXT_4.UPDATED_TIMESTAMP AS UTS_KoT4,\\n     KYNDRYL_ONLY_TEXT_4.USER_ID AS UUID_KoT4,\\n    KYNDRYL_ONLY_TEXT_5.KYNDRYL_ONLY_TEXT_5,\\n     KYNDRYL_ONLY_TEXT_5.UPDATED_TIMESTAMP AS UTS_KoT5,\\n     KYNDRYL_ONLY_TEXT_5.USER_ID AS UUID_KoT5,\\n    KYNDRYL_ONLY_TEXT_6.KYNDRYL_ONLY_TEXT_6,\\n     KYNDRYL_ONLY_TEXT_6.UPDATED_TIMESTAMP AS UTS_KoT6,\\n     KYNDRYL_ONLY_TEXT_6.USER_ID AS UUID_KoT6,\\n    KYNDRYL_ONLY_TEXT_7.KYNDRYL_ONLY_TEXT_7,\\n     KYNDRYL_ONLY_TEXT_7.UPDATED_TIMESTAMP AS UTS_KoT7,\\n     KYNDRYL_ONLY_TEXT_7.USER_ID AS UUID_KoT7,\\n    KYNDRYL_ONLY_TEXT_8.KYNDRYL_ONLY_TEXT_8,\\n     KYNDRYL_ONLY_TEXT_8.UPDATED_TIMESTAMP AS UTS_KoT8,\\n     KYNDRYL_ONLY_TEXT_8.USER_ID AS UUID_KoT8,\\n    KYNDRYL_ONLY_TEXT_9.KYNDRYL_ONLY_TEXT_9,\\n     KYNDRYL_ONLY_TEXT_9.UPDATED_TIMESTAMP AS UTS_KoT9,\\n     KYNDRYL_ONLY_TEXT_9.USER_ID AS UUID_KoT9,\\n    KYNDRYL_ONLY_TEXT_10.KYNDRYL_ONLY_TEXT_10,\\n     KYNDRYL_ONLY_TEXT_10.UPDATED_TIMESTAMP AS UTS_KoT10,\\n     KYNDRYL_ONLY_TEXT_10.USER_ID AS UUID_KoT10,\\n    KYNDRYL_ONLY_TEXT_11.KYNDRYL_ONLY_TEXT_11,\\n     KYNDRYL_ONLY_TEXT_11.UPDATED_TIMESTAMP AS UTS_KoT11,\\n     KYNDRYL_ONLY_TEXT_11.USER_ID AS UUID_KoT11,\\n    KYNDRYL_ONLY_TEXT_12.KYNDRYL_ONLY_TEXT_12,\\n     KYNDRYL_ONLY_TEXT_12.UPDATED_TIMESTAMP AS UTS_KoT12,\\n     KYNDRYL_ONLY_TEXT_12.USER_ID AS UUID_KoT12,\\n    KYNDRYL_ONLY_TEXT_13.KYNDRYL_ONLY_TEXT_13,\\n     KYNDRYL_ONLY_TEXT_13.UPDATED_TIMESTAMP AS UTS_KoT13,\\n     KYNDRYL_ONLY_TEXT_13.USER_ID AS UUID_KoT13,\\n    KYNDRYL_ONLY_TEXT_14.KYNDRYL_ONLY_TEXT_14,\\n     KYNDRYL_ONLY_TEXT_14.UPDATED_TIMESTAMP AS UTS_KoT14,\\n     KYNDRYL_ONLY_TEXT_14.USER_ID AS UUID_KoT14,\\n    KYNDRYL_ONLY_TEXT_15.KYNDRYL_ONLY_TEXT_15,\\n     KYNDRYL_ONLY_TEXT_15.UPDATED_TIMESTAMP AS UTS_KoT15,\\n     KYNDRYL_ONLY_TEXT_15.USER_ID AS UUID_KoT15,\\n    KYNDRYL_ONLY_TEXT_16.KYNDRYL_ONLY_TEXT_16,\\n     KYNDRYL_ONLY_TEXT_16.UPDATED_TIMESTAMP AS UTS_KoT16,\\n     KYNDRYL_ONLY_TEXT_16.USER_ID AS UUID_KoT16,\\n    KYNDRYL_ONLY_TEXT_17.KYNDRYL_ONLY_TEXT_17,\\n     KYNDRYL_ONLY_TEXT_17.UPDATED_TIMESTAMP AS UTS_KoT17,\\n     KYNDRYL_ONLY_TEXT_17.USER_ID AS UUID_KoT17,\\n    KYNDRYL_ONLY_TEXT_18.KYNDRYL_ONLY_TEXT_18,\\n     KYNDRYL_ONLY_TEXT_18.UPDATED_TIMESTAMP AS UTS_KoT18,\\n     KYNDRYL_ONLY_TEXT_18.USER_ID AS UUID_KoT18,\\n    KYNDRYL_ONLY_TEXT_19.KYNDRYL_ONLY_TEXT_19,\\n     KYNDRYL_ONLY_TEXT_19.UPDATED_TIMESTAMP AS UTS_KoT19,\\n     KYNDRYL_ONLY_TEXT_19.USER_ID AS UUID_KoT19,\\n    KYNDRYL_ONLY_TEXT_20.KYNDRYL_ONLY_TEXT_20,\\n     KYNDRYL_ONLY_TEXT_20.UPDATED_TIMESTAMP AS UTS_KoT20,\\n     KYNDRYL_ONLY_TEXT_20.USER_ID AS UUID_KoT20,\\n    KYNDRYL_ONLY_TEXT_21.KYNDRYL_ONLY_TEXT_21,\\n     KYNDRYL_ONLY_TEXT_21.UPDATED_TIMESTAMP AS UTS_KoT21,\\n     KYNDRYL_ONLY_TEXT_21.USER_ID AS UUID_KoT21,\\n    KYNDRYL_ONLY_TEXT_22.KYNDRYL_ONLY_TEXT_22,\\n     KYNDRYL_ONLY_TEXT_22.UPDATED_TIMESTAMP AS UTS_KoT22,\\n     KYNDRYL_ONLY_TEXT_22.USER_ID AS UUID_KoT22,\\n    KYNDRYL_ONLY_TEXT_23.KYNDRYL_ONLY_TEXT_23,\\n     KYNDRYL_ONLY_TEXT_23.UPDATED_TIMESTAMP AS UTS_KoT23,\\n     KYNDRYL_ONLY_TEXT_23.USER_ID AS UUID_KoT23,\\n    KYNDRYL_ONLY_TEXT_24.KYNDRYL_ONLY_TEXT_24,\\n     KYNDRYL_ONLY_TEXT_24.UPDATED_TIMESTAMP AS UTS_KoT24,\\n     KYNDRYL_ONLY_TEXT_24.USER_ID AS UUID_KoT24,\\n    KYNDRYL_ONLY_TEXT_25.KYNDRYL_ONLY_TEXT_25,\\n     KYNDRYL_ONLY_TEXT_25.UPDATED_TIMESTAMP AS UTS_KoT25,\\n     KYNDRYL_ONLY_TEXT_25.USER_ID AS UUID_KoT25,\\n    \\'\\' AS KYNDRYL_ONLY_TEXT_26, --KYNDRYL_ONLY_TEXT_26.KYNDRYL_ONLY_TEXT_26,\\n     CURRENT_TIMESTAMP AS UTS_KoT26, --KYNDRYL_ONLY_TEXT_26.UPDATED_TIMESTAMP AS UTS_KoT26,\\n     \\'\\' AS UUID_KoT26, --KYNDRYL_ONLY_TEXT_26.USER_ID AS UUID_KoT26,\\n    \\'\\' AS KYNDRYL_ONLY_TEXT_27, --KYNDRYL_ONLY_TEXT_27.KYNDRYL_ONLY_TEXT_27,\\n     CURRENT_TIMESTAMP AS UTS_KoT27, --KYNDRYL_ONLY_TEXT_27.UPDATED_TIMESTAMP AS UTS_KoT27,\\n     \\'\\' AS UUID_KoT27, --KYNDRYL_ONLY_TEXT_27.USER_ID AS UUID_KoT27,\\n    KYNDRYL_ONLY_DATE_1.KYNDRYL_ONLY_DATE_1,\\n     KYNDRYL_ONLY_DATE_1.UPDATED_TIMESTAMP AS UTS_KoD1,\\n     KYNDRYL_ONLY_DATE_1.USER_ID AS UUID_KoD1,\\n    KYNDRYL_ONLY_DATE_2.KYNDRYL_ONLY_DATE_2,\\n     KYNDRYL_ONLY_DATE_2.UPDATED_TIMESTAMP AS UTS_KoD2,\\n     KYNDRYL_ONLY_DATE_2.USER_ID AS UUID_KoD2,\\n    KYNDRYL_ONLY_DATE_3.KYNDRYL_ONLY_DATE_3,\\n     KYNDRYL_ONLY_DATE_3.UPDATED_TIMESTAMP AS UTS_KoD3,\\n     KYNDRYL_ONLY_DATE_3.USER_ID AS UUID_KoD3,\\n    KYNDRYL_ONLY_DATE_4.KYNDRYL_ONLY_DATE_4,\\n     KYNDRYL_ONLY_DATE_4.UPDATED_TIMESTAMP AS UTS_KoD4,\\n     KYNDRYL_ONLY_DATE_4.USER_ID AS UUID_KoD4,\\n    KYNDRYL_ONLY_DATE_5.KYNDRYL_ONLY_DATE_5,\\n     KYNDRYL_ONLY_DATE_5.UPDATED_TIMESTAMP AS UTS_KoD5,\\n     KYNDRYL_ONLY_DATE_5.USER_ID AS UUID_KoD5,\\n    KYNDRYL_ONLY_DATE_6.KYNDRYL_ONLY_DATE_6,\\n     KYNDRYL_ONLY_DATE_6.UPDATED_TIMESTAMP AS UTS_KoD6,\\n     KYNDRYL_ONLY_DATE_6.USER_ID AS UUID_KoD6,\\n    KYNDRYL_ONLY_DATE_7.KYNDRYL_ONLY_DATE_7,\\n     KYNDRYL_ONLY_DATE_7.UPDATED_TIMESTAMP AS UTS_KoD7,\\n     KYNDRYL_ONLY_DATE_7.USER_ID AS UUID_KoD7,\\n    KYNDRYL_ONLY_DATE_8.KYNDRYL_ONLY_DATE_8,\\n     KYNDRYL_ONLY_DATE_8.UPDATED_TIMESTAMP AS UTS_KoD8,\\n     KYNDRYL_ONLY_DATE_8.USER_ID AS UUID_KoD8,\\n    KYNDRYL_ONLY_DATE_9.KYNDRYL_ONLY_DATE_9,\\n     KYNDRYL_ONLY_DATE_9.UPDATED_TIMESTAMP AS UTS_KoD9,\\n     KYNDRYL_ONLY_DATE_9.USER_ID AS UUID_KoD9,\\n    KYNDRYL_ONLY_DATE_10.KYNDRYL_ONLY_DATE_10,\\n     KYNDRYL_ONLY_DATE_10.UPDATED_TIMESTAMP AS UTS_KoD10,\\n     KYNDRYL_ONLY_DATE_10.USER_ID AS UUID_KoD10,\\n    KYNDRYL_ONLY_DATE_11.KYNDRYL_ONLY_DATE_11,\\n     KYNDRYL_ONLY_DATE_11.UPDATED_TIMESTAMP AS UTS_KoD11,\\n     KYNDRYL_ONLY_DATE_11.USER_ID AS UUID_KoD11,\\n    KYNDRYL_ONLY_DATE_12.KYNDRYL_ONLY_DATE_12,\\n     KYNDRYL_ONLY_DATE_12.UPDATED_TIMESTAMP AS UTS_KoD12,\\n     KYNDRYL_ONLY_DATE_12.USER_ID AS UUID_KoD12,\\n    KYNDRYL_ONLY_DATE_13.KYNDRYL_ONLY_DATE_13,\\n     KYNDRYL_ONLY_DATE_13.UPDATED_TIMESTAMP AS UTS_KoD13,\\n     KYNDRYL_ONLY_DATE_13.USER_ID AS UUID_KoD13,\\n    KYNDRYL_ONLY_DATE_14.KYNDRYL_ONLY_DATE_14,\\n     KYNDRYL_ONLY_DATE_14.UPDATED_TIMESTAMP AS UTS_KoD14,\\n     KYNDRYL_ONLY_DATE_14.USER_ID AS UUID_KoD14,\\n    KYNDRYL_ONLY_DATE_15.KYNDRYL_ONLY_DATE_15,\\n     KYNDRYL_ONLY_DATE_15.UPDATED_TIMESTAMP AS UTS_KoD15,\\n     KYNDRYL_ONLY_DATE_15.USER_ID AS UUID_KoD15\\nFrom\\n     PGMPDM.PROC_DIM PROCS\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID\\n     INNER JOIN (\\n          Select distinct\\n               PROC_DIM_UID\\n          From     \\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM\\n          Where     \\n               COALESCE(\\n                    IBM_ONLY_TEXT_1,\\n                    IBM_ONLY_TEXT_2,\\n                    IBM_ONLY_TEXT_3,\\n                    IBM_ONLY_TEXT_4,\\n                    IBM_ONLY_TEXT_5,\\n                    IBM_ONLY_TEXT_6,\\n                    IBM_ONLY_TEXT_7,\\n                    IBM_ONLY_TEXT_8,\\n                    IBM_ONLY_TEXT_9,\\n                    IBM_ONLY_TEXT_10,\\n                    IBM_ONLY_TEXT_11,\\n                    IBM_ONLY_TEXT_12,\\n                    IBM_ONLY_TEXT_13,\\n                    IBM_ONLY_TEXT_14,\\n                    IBM_ONLY_TEXT_15,\\n                    IBM_ONLY_TEXT_16,\\n                    IBM_ONLY_TEXT_17,\\n                    IBM_ONLY_TEXT_18,\\n                    IBM_ONLY_TEXT_19,\\n                    IBM_ONLY_TEXT_20,\\n                    IBM_ONLY_TEXT_21,\\n                    IBM_ONLY_TEXT_22,\\n                    IBM_ONLY_TEXT_23,\\n                    IBM_ONLY_TEXT_24,\\n                    IBM_ONLY_TEXT_25,\\n                    --IBM_ONLY_TEXT_26,\\n                    --IBM_ONLY_TEXT_27,\\n                    CAST(IBM_ONLY_DATE_1 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_2 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_3 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_4 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_5 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_6 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_7 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_8 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_9 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_10 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_11 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_12 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_13 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_14 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_15 AS VARCHAR)\\n               ) NOT IN (\\'\\', \\'NULL\\')\\n     ) KoPROC ON KoPROC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_1 AS KYNDRYL_ONLY_TEXT_1,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_1                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_1 = DKo.IBM_ONLY_TEXT_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_1 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_1 ON KYNDRYL_ONLY_TEXT_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_2 AS KYNDRYL_ONLY_TEXT_2,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_2                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_2 = DKo.IBM_ONLY_TEXT_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_2 not in (\\'\\') \\n     )KYNDRYL_ONLY_TEXT_2 ON KYNDRYL_ONLY_TEXT_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_3 AS KYNDRYL_ONLY_TEXT_3,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_3                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_3 = DKo.IBM_ONLY_TEXT_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_3 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_3 ON KYNDRYL_ONLY_TEXT_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_4 AS KYNDRYL_ONLY_TEXT_4,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP               \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_4                              \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_4 = DKo.IBM_ONLY_TEXT_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_4 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_4 ON KYNDRYL_ONLY_TEXT_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_5 AS KYNDRYL_ONLY_TEXT_5,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_5               \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_5 = DKo.IBM_ONLY_TEXT_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_5 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_5 ON KYNDRYL_ONLY_TEXT_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_6 AS KYNDRYL_ONLY_TEXT_6,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_6                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_6 = DKo.IBM_ONLY_TEXT_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_6 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_6 ON KYNDRYL_ONLY_TEXT_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_7 AS KYNDRYL_ONLY_TEXT_7,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_7                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_7 = DKo.IBM_ONLY_TEXT_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_7 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_7 ON KYNDRYL_ONLY_TEXT_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_8 AS KYNDRYL_ONLY_TEXT_8,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_8                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_8 = DKo.IBM_ONLY_TEXT_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_8 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_8 ON KYNDRYL_ONLY_TEXT_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_9 AS KYNDRYL_ONLY_TEXT_9,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_9                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_9 = DKo.IBM_ONLY_TEXT_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_9 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_9 ON KYNDRYL_ONLY_TEXT_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_10 AS KYNDRYL_ONLY_TEXT_10,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_10                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_10 = DKo.IBM_ONLY_TEXT_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_10 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_10 ON KYNDRYL_ONLY_TEXT_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_11 AS KYNDRYL_ONLY_TEXT_11,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_11                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_11 = DKo.IBM_ONLY_TEXT_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_11 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_11 ON KYNDRYL_ONLY_TEXT_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_12 AS KYNDRYL_ONLY_TEXT_12,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_12                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_12 = DKo.IBM_ONLY_TEXT_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_12 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_12 ON KYNDRYL_ONLY_TEXT_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_13 AS KYNDRYL_ONLY_TEXT_13,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_13                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_13 = DKo.IBM_ONLY_TEXT_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_13 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_13 ON KYNDRYL_ONLY_TEXT_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_14 AS KYNDRYL_ONLY_TEXT_14,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_14\\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_14 = DKo.IBM_ONLY_TEXT_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_14 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_14 ON KYNDRYL_ONLY_TEXT_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_15 AS KYNDRYL_ONLY_TEXT_15,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_15                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_15 = DKo.IBM_ONLY_TEXT_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_15 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_15 ON KYNDRYL_ONLY_TEXT_15.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_16 AS KYNDRYL_ONLY_TEXT_16,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_16,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_16                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_16 = DKo.IBM_ONLY_TEXT_16)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_16 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_16 ON KYNDRYL_ONLY_TEXT_16.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_17 AS KYNDRYL_ONLY_TEXT_17,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_17,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_17                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_17 = DKo.IBM_ONLY_TEXT_17)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_17 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_17 ON KYNDRYL_ONLY_TEXT_17.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_18 AS KYNDRYL_ONLY_TEXT_18,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_18,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_18                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_18 = DKo.IBM_ONLY_TEXT_18)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_18 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_18 ON KYNDRYL_ONLY_TEXT_18.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_19 AS KYNDRYL_ONLY_TEXT_19,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_19,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_19                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_19 = DKo.IBM_ONLY_TEXT_19)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_19 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_19 ON KYNDRYL_ONLY_TEXT_19.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_20 AS KYNDRYL_ONLY_TEXT_20,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_20,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_20                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_20 = DKo.IBM_ONLY_TEXT_20)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_20 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_20 ON KYNDRYL_ONLY_TEXT_20.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_21 AS KYNDRYL_ONLY_TEXT_21,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_21,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_21                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_21 = DKo.IBM_ONLY_TEXT_21)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_21 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_21 ON KYNDRYL_ONLY_TEXT_21.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_22 AS KYNDRYL_ONLY_TEXT_22,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_22,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_22                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_22 = DKo.IBM_ONLY_TEXT_22)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_22 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_22 ON KYNDRYL_ONLY_TEXT_22.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_23 AS KYNDRYL_ONLY_TEXT_23,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_23,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_23                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_23 = DKo.IBM_ONLY_TEXT_23)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_23 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_23 ON KYNDRYL_ONLY_TEXT_23.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_24 AS KYNDRYL_ONLY_TEXT_24,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_24,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_24                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_24 = DKo.IBM_ONLY_TEXT_24)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_24 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_24 ON KYNDRYL_ONLY_TEXT_24.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_25 AS KYNDRYL_ONLY_TEXT_25,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_25,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_25                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_25 = DKo.IBM_ONLY_TEXT_25)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_25 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_25 ON KYNDRYL_ONLY_TEXT_25.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    /*\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_26 AS KYNDRYL_ONLY_TEXT_26,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_26,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_26                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_26 = DKo.IBM_ONLY_TEXT_26)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_26 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_26 ON KYNDRYL_ONLY_TEXT_26.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_27 AS KYNDRYL_ONLY_TEXT_27,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_27,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_27               \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_27 = DKo.IBM_ONLY_TEXT_27)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_27 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_27 ON KYNDRYL_ONLY_TEXT_27.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     */\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_1 AS KYNDRYL_ONLY_DATE_1,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_1                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_1 = DKo.IBM_ONLY_DATE_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_1 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_1 ON KYNDRYL_ONLY_DATE_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_2 AS KYNDRYL_ONLY_DATE_2,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_2                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_2 = DKo.IBM_ONLY_DATE_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_2 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_2 ON KYNDRYL_ONLY_DATE_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_3 AS KYNDRYL_ONLY_DATE_3,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_3                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_3 = DKo.IBM_ONLY_DATE_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_3 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_3 ON KYNDRYL_ONLY_DATE_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_4 AS KYNDRYL_ONLY_DATE_4,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP     \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_4                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_4 = DKo.IBM_ONLY_DATE_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_4 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_4 ON KYNDRYL_ONLY_DATE_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_5 AS KYNDRYL_ONLY_DATE_5,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_5                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_5 = DKo.IBM_ONLY_DATE_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_5 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_5 ON KYNDRYL_ONLY_DATE_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_6 AS KYNDRYL_ONLY_DATE_6,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_6                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_6 = DKo.IBM_ONLY_DATE_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_6 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_6 ON KYNDRYL_ONLY_DATE_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_7 AS KYNDRYL_ONLY_DATE_7,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_7                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_7 = DKo.IBM_ONLY_DATE_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_7 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_7 ON KYNDRYL_ONLY_DATE_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_8 AS KYNDRYL_ONLY_DATE_8,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_8                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_8 = DKo.IBM_ONLY_DATE_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_8 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_8 ON KYNDRYL_ONLY_DATE_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_9 AS KYNDRYL_ONLY_DATE_9,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_9                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_9 = DKo.IBM_ONLY_DATE_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_9 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_9 ON KYNDRYL_ONLY_DATE_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_10 AS KYNDRYL_ONLY_DATE_10,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_10                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_10 = DKo.IBM_ONLY_DATE_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_10 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_10 ON KYNDRYL_ONLY_DATE_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_11 AS KYNDRYL_ONLY_DATE_11,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_11                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_11 = DKo.IBM_ONLY_DATE_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_11 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_11 ON KYNDRYL_ONLY_DATE_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_12 AS KYNDRYL_ONLY_DATE_12,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_12                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_12 = DKo.IBM_ONLY_DATE_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_12 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_12 ON KYNDRYL_ONLY_DATE_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_13 AS KYNDRYL_ONLY_DATE_13,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_13                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_13 = DKo.IBM_ONLY_DATE_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_13 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_13 ON KYNDRYL_ONLY_DATE_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_14 AS KYNDRYL_ONLY_DATE_14,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_14                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_14 = DKo.IBM_ONLY_DATE_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_14 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_14 ON KYNDRYL_ONLY_DATE_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_15 AS KYNDRYL_ONLY_DATE_15,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_15                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_15 = DKo.IBM_ONLY_DATE_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_15 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_15 ON KYNDRYL_ONLY_DATE_15.PROC_DIM_UID = PROCS.PROC_DIM_UID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData sort(asc(PROC_DIM_UID, false)) ~> sortedSourceData",
						"sortedSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTRPTS_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_CSTM_KOAF')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedSourceData"
						},
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          CNTRCT_DIM_UID as integer,",
						"          CNTRCT_NM as string,",
						"          KYNDRYL_ONLY_TEXT_1 as string,",
						"          UTS_KoT1 as timestamp,",
						"          UUID_KoT1 as binary,",
						"          KYNDRYL_ONLY_TEXT_2 as string,",
						"          UTS_KoT2 as timestamp,",
						"          UUID_KoT2 as binary,",
						"          KYNDRYL_ONLY_TEXT_3 as string,",
						"          UTS_KoT3 as timestamp,",
						"          UUID_KoT3 as binary,",
						"          KYNDRYL_ONLY_TEXT_4 as string,",
						"          UTS_KoT4 as timestamp,",
						"          UUID_KoT4 as binary,",
						"          KYNDRYL_ONLY_TEXT_5 as string,",
						"          UTS_KoT5 as timestamp,",
						"          UUID_KoT5 as binary,",
						"          KYNDRYL_ONLY_TEXT_6 as string,",
						"          UTS_KoT6 as timestamp,",
						"          UUID_KoT6 as binary,",
						"          KYNDRYL_ONLY_TEXT_7 as string,",
						"          UTS_KoT7 as timestamp,",
						"          UUID_KoT7 as binary,",
						"          KYNDRYL_ONLY_TEXT_8 as string,",
						"          UTS_KoT8 as timestamp,",
						"          UUID_KoT8 as binary,",
						"          KYNDRYL_ONLY_TEXT_9 as string,",
						"          UTS_KoT9 as timestamp,",
						"          UUID_KoT9 as binary,",
						"          KYNDRYL_ONLY_TEXT_10 as string,",
						"          UTS_KoT10 as timestamp,",
						"          UUID_KoT10 as binary,",
						"          KYNDRYL_ONLY_TEXT_11 as string,",
						"          UTS_KoT11 as timestamp,",
						"          UUID_KoT11 as binary,",
						"          KYNDRYL_ONLY_TEXT_12 as string,",
						"          UTS_KoT12 as timestamp,",
						"          UUID_KoT12 as binary,",
						"          KYNDRYL_ONLY_TEXT_13 as string,",
						"          UTS_KoT13 as timestamp,",
						"          UUID_KoT13 as binary,",
						"          KYNDRYL_ONLY_TEXT_14 as string,",
						"          UTS_KoT14 as timestamp,",
						"          UUID_KoT14 as binary,",
						"          KYNDRYL_ONLY_TEXT_15 as string,",
						"          UTS_KoT15 as timestamp,",
						"          UUID_KoT15 as binary,",
						"          KYNDRYL_ONLY_TEXT_16 as string,",
						"          UTS_KoT16 as timestamp,",
						"          UUID_KoT16 as binary,",
						"          KYNDRYL_ONLY_TEXT_17 as string,",
						"          UTS_KoT17 as timestamp,",
						"          UUID_KoT17 as binary,",
						"          KYNDRYL_ONLY_TEXT_18 as string,",
						"          UTS_KoT18 as timestamp,",
						"          UUID_KoT18 as binary,",
						"          KYNDRYL_ONLY_TEXT_19 as string,",
						"          UTS_KoT19 as timestamp,",
						"          UUID_KoT19 as binary,",
						"          KYNDRYL_ONLY_TEXT_20 as string,",
						"          UTS_KoT20 as timestamp,",
						"          UUID_KoT20 as binary,",
						"          KYNDRYL_ONLY_TEXT_21 as string,",
						"          UTS_KoT21 as timestamp,",
						"          UUID_KoT21 as binary,",
						"          KYNDRYL_ONLY_TEXT_22 as string,",
						"          UTS_KoT22 as timestamp,",
						"          UUID_KoT22 as binary,",
						"          KYNDRYL_ONLY_TEXT_23 as string,",
						"          UTS_KoT23 as timestamp,",
						"          UUID_KoT23 as binary,",
						"          KYNDRYL_ONLY_TEXT_24 as string,",
						"          UTS_KoT24 as timestamp,",
						"          UUID_KoT24 as binary,",
						"          KYNDRYL_ONLY_TEXT_25 as string,",
						"          UTS_KoT25 as timestamp,",
						"          UUID_KoT25 as binary,",
						"          KYNDRYL_ONLY_TEXT_26 as string,",
						"          UTS_KoT26 as timestamp,",
						"          UUID_KoT26 as string,",
						"          KYNDRYL_ONLY_TEXT_27 as string,",
						"          UTS_KoT27 as timestamp,",
						"          UUID_KoT27 as string,",
						"          KYNDRYL_ONLY_DATE_1 as date,",
						"          UTS_KoD1 as timestamp,",
						"          UUID_KoD1 as binary,",
						"          KYNDRYL_ONLY_DATE_2 as date,",
						"          UTS_KoD2 as timestamp,",
						"          UUID_KoD2 as binary,",
						"          KYNDRYL_ONLY_DATE_3 as date,",
						"          UTS_KoD3 as timestamp,",
						"          UUID_KoD3 as binary,",
						"          KYNDRYL_ONLY_DATE_4 as date,",
						"          UTS_KoD4 as timestamp,",
						"          UUID_KoD4 as binary,",
						"          KYNDRYL_ONLY_DATE_5 as date,",
						"          UTS_KoD5 as timestamp,",
						"          UUID_KoD5 as binary,",
						"          KYNDRYL_ONLY_DATE_6 as date,",
						"          UTS_KoD6 as timestamp,",
						"          UUID_KoD6 as binary,",
						"          KYNDRYL_ONLY_DATE_7 as date,",
						"          UTS_KoD7 as timestamp,",
						"          UUID_KoD7 as binary,",
						"          KYNDRYL_ONLY_DATE_8 as date,",
						"          UTS_KoD8 as timestamp,",
						"          UUID_KoD8 as binary,",
						"          KYNDRYL_ONLY_DATE_9 as date,",
						"          UTS_KoD9 as timestamp,",
						"          UUID_KoD9 as binary,",
						"          KYNDRYL_ONLY_DATE_10 as date,",
						"          UTS_KoD10 as timestamp,",
						"          UUID_KoD10 as binary,",
						"          KYNDRYL_ONLY_DATE_11 as date,",
						"          UTS_KoD11 as timestamp,",
						"          UUID_KoD11 as binary,",
						"          KYNDRYL_ONLY_DATE_12 as date,",
						"          UTS_KoD12 as timestamp,",
						"          UUID_KoD12 as binary,",
						"          KYNDRYL_ONLY_DATE_13 as date,",
						"          UTS_KoD13 as timestamp,",
						"          UUID_KoD13 as binary,",
						"          KYNDRYL_ONLY_DATE_14 as date,",
						"          UTS_KoD14 as timestamp,",
						"          UUID_KoD14 as binary,",
						"          KYNDRYL_ONLY_DATE_15 as date,",
						"          UTS_KoD15 as timestamp,",
						"          UUID_KoD15 as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select distinct\\n     PROCS.PROC_DIM_UID,\\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM,\\n    KYNDRYL_ONLY_TEXT_1.KYNDRYL_ONLY_TEXT_1,\\n     KYNDRYL_ONLY_TEXT_1.UPDATED_TIMESTAMP AS UTS_KoT1,\\n     KYNDRYL_ONLY_TEXT_1.USER_ID AS UUID_KoT1,\\n    KYNDRYL_ONLY_TEXT_2.KYNDRYL_ONLY_TEXT_2,\\n     KYNDRYL_ONLY_TEXT_2.UPDATED_TIMESTAMP AS UTS_KoT2,\\n     KYNDRYL_ONLY_TEXT_2.USER_ID AS UUID_KoT2,\\n    KYNDRYL_ONLY_TEXT_3.KYNDRYL_ONLY_TEXT_3,\\n     KYNDRYL_ONLY_TEXT_3.UPDATED_TIMESTAMP AS UTS_KoT3,\\n     KYNDRYL_ONLY_TEXT_3.USER_ID AS UUID_KoT3,\\n    KYNDRYL_ONLY_TEXT_4.KYNDRYL_ONLY_TEXT_4,\\n     KYNDRYL_ONLY_TEXT_4.UPDATED_TIMESTAMP AS UTS_KoT4,\\n     KYNDRYL_ONLY_TEXT_4.USER_ID AS UUID_KoT4,\\n    KYNDRYL_ONLY_TEXT_5.KYNDRYL_ONLY_TEXT_5,\\n     KYNDRYL_ONLY_TEXT_5.UPDATED_TIMESTAMP AS UTS_KoT5,\\n     KYNDRYL_ONLY_TEXT_5.USER_ID AS UUID_KoT5,\\n    KYNDRYL_ONLY_TEXT_6.KYNDRYL_ONLY_TEXT_6,\\n     KYNDRYL_ONLY_TEXT_6.UPDATED_TIMESTAMP AS UTS_KoT6,\\n     KYNDRYL_ONLY_TEXT_6.USER_ID AS UUID_KoT6,\\n    KYNDRYL_ONLY_TEXT_7.KYNDRYL_ONLY_TEXT_7,\\n     KYNDRYL_ONLY_TEXT_7.UPDATED_TIMESTAMP AS UTS_KoT7,\\n     KYNDRYL_ONLY_TEXT_7.USER_ID AS UUID_KoT7,\\n    KYNDRYL_ONLY_TEXT_8.KYNDRYL_ONLY_TEXT_8,\\n     KYNDRYL_ONLY_TEXT_8.UPDATED_TIMESTAMP AS UTS_KoT8,\\n     KYNDRYL_ONLY_TEXT_8.USER_ID AS UUID_KoT8,\\n    KYNDRYL_ONLY_TEXT_9.KYNDRYL_ONLY_TEXT_9,\\n     KYNDRYL_ONLY_TEXT_9.UPDATED_TIMESTAMP AS UTS_KoT9,\\n     KYNDRYL_ONLY_TEXT_9.USER_ID AS UUID_KoT9,\\n    KYNDRYL_ONLY_TEXT_10.KYNDRYL_ONLY_TEXT_10,\\n     KYNDRYL_ONLY_TEXT_10.UPDATED_TIMESTAMP AS UTS_KoT10,\\n     KYNDRYL_ONLY_TEXT_10.USER_ID AS UUID_KoT10,\\n    KYNDRYL_ONLY_TEXT_11.KYNDRYL_ONLY_TEXT_11,\\n     KYNDRYL_ONLY_TEXT_11.UPDATED_TIMESTAMP AS UTS_KoT11,\\n     KYNDRYL_ONLY_TEXT_11.USER_ID AS UUID_KoT11,\\n    KYNDRYL_ONLY_TEXT_12.KYNDRYL_ONLY_TEXT_12,\\n     KYNDRYL_ONLY_TEXT_12.UPDATED_TIMESTAMP AS UTS_KoT12,\\n     KYNDRYL_ONLY_TEXT_12.USER_ID AS UUID_KoT12,\\n    KYNDRYL_ONLY_TEXT_13.KYNDRYL_ONLY_TEXT_13,\\n     KYNDRYL_ONLY_TEXT_13.UPDATED_TIMESTAMP AS UTS_KoT13,\\n     KYNDRYL_ONLY_TEXT_13.USER_ID AS UUID_KoT13,\\n    KYNDRYL_ONLY_TEXT_14.KYNDRYL_ONLY_TEXT_14,\\n     KYNDRYL_ONLY_TEXT_14.UPDATED_TIMESTAMP AS UTS_KoT14,\\n     KYNDRYL_ONLY_TEXT_14.USER_ID AS UUID_KoT14,\\n    KYNDRYL_ONLY_TEXT_15.KYNDRYL_ONLY_TEXT_15,\\n     KYNDRYL_ONLY_TEXT_15.UPDATED_TIMESTAMP AS UTS_KoT15,\\n     KYNDRYL_ONLY_TEXT_15.USER_ID AS UUID_KoT15,\\n    KYNDRYL_ONLY_TEXT_16.KYNDRYL_ONLY_TEXT_16,\\n     KYNDRYL_ONLY_TEXT_16.UPDATED_TIMESTAMP AS UTS_KoT16,\\n     KYNDRYL_ONLY_TEXT_16.USER_ID AS UUID_KoT16,\\n    KYNDRYL_ONLY_TEXT_17.KYNDRYL_ONLY_TEXT_17,\\n     KYNDRYL_ONLY_TEXT_17.UPDATED_TIMESTAMP AS UTS_KoT17,\\n     KYNDRYL_ONLY_TEXT_17.USER_ID AS UUID_KoT17,\\n    KYNDRYL_ONLY_TEXT_18.KYNDRYL_ONLY_TEXT_18,\\n     KYNDRYL_ONLY_TEXT_18.UPDATED_TIMESTAMP AS UTS_KoT18,\\n     KYNDRYL_ONLY_TEXT_18.USER_ID AS UUID_KoT18,\\n    KYNDRYL_ONLY_TEXT_19.KYNDRYL_ONLY_TEXT_19,\\n     KYNDRYL_ONLY_TEXT_19.UPDATED_TIMESTAMP AS UTS_KoT19,\\n     KYNDRYL_ONLY_TEXT_19.USER_ID AS UUID_KoT19,\\n    KYNDRYL_ONLY_TEXT_20.KYNDRYL_ONLY_TEXT_20,\\n     KYNDRYL_ONLY_TEXT_20.UPDATED_TIMESTAMP AS UTS_KoT20,\\n     KYNDRYL_ONLY_TEXT_20.USER_ID AS UUID_KoT20,\\n    KYNDRYL_ONLY_TEXT_21.KYNDRYL_ONLY_TEXT_21,\\n     KYNDRYL_ONLY_TEXT_21.UPDATED_TIMESTAMP AS UTS_KoT21,\\n     KYNDRYL_ONLY_TEXT_21.USER_ID AS UUID_KoT21,\\n    KYNDRYL_ONLY_TEXT_22.KYNDRYL_ONLY_TEXT_22,\\n     KYNDRYL_ONLY_TEXT_22.UPDATED_TIMESTAMP AS UTS_KoT22,\\n     KYNDRYL_ONLY_TEXT_22.USER_ID AS UUID_KoT22,\\n    KYNDRYL_ONLY_TEXT_23.KYNDRYL_ONLY_TEXT_23,\\n     KYNDRYL_ONLY_TEXT_23.UPDATED_TIMESTAMP AS UTS_KoT23,\\n     KYNDRYL_ONLY_TEXT_23.USER_ID AS UUID_KoT23,\\n    KYNDRYL_ONLY_TEXT_24.KYNDRYL_ONLY_TEXT_24,\\n     KYNDRYL_ONLY_TEXT_24.UPDATED_TIMESTAMP AS UTS_KoT24,\\n     KYNDRYL_ONLY_TEXT_24.USER_ID AS UUID_KoT24,\\n    KYNDRYL_ONLY_TEXT_25.KYNDRYL_ONLY_TEXT_25,\\n     KYNDRYL_ONLY_TEXT_25.UPDATED_TIMESTAMP AS UTS_KoT25,\\n     KYNDRYL_ONLY_TEXT_25.USER_ID AS UUID_KoT25,\\n    \\'\\' AS KYNDRYL_ONLY_TEXT_26, --KYNDRYL_ONLY_TEXT_26.KYNDRYL_ONLY_TEXT_26,\\n     CURRENT_TIMESTAMP AS UTS_KoT26, --KYNDRYL_ONLY_TEXT_26.UPDATED_TIMESTAMP AS UTS_KoT26,\\n     \\'\\' AS UUID_KoT26, --KYNDRYL_ONLY_TEXT_26.USER_ID AS UUID_KoT26,\\n    \\'\\' AS KYNDRYL_ONLY_TEXT_27, --KYNDRYL_ONLY_TEXT_27.KYNDRYL_ONLY_TEXT_27,\\n     CURRENT_TIMESTAMP AS UTS_KoT27, --KYNDRYL_ONLY_TEXT_27.UPDATED_TIMESTAMP AS UTS_KoT27,\\n     \\'\\' AS UUID_KoT27, --KYNDRYL_ONLY_TEXT_27.USER_ID AS UUID_KoT27,\\n    KYNDRYL_ONLY_DATE_1.KYNDRYL_ONLY_DATE_1,\\n     KYNDRYL_ONLY_DATE_1.UPDATED_TIMESTAMP AS UTS_KoD1,\\n     KYNDRYL_ONLY_DATE_1.USER_ID AS UUID_KoD1,\\n    KYNDRYL_ONLY_DATE_2.KYNDRYL_ONLY_DATE_2,\\n     KYNDRYL_ONLY_DATE_2.UPDATED_TIMESTAMP AS UTS_KoD2,\\n     KYNDRYL_ONLY_DATE_2.USER_ID AS UUID_KoD2,\\n    KYNDRYL_ONLY_DATE_3.KYNDRYL_ONLY_DATE_3,\\n     KYNDRYL_ONLY_DATE_3.UPDATED_TIMESTAMP AS UTS_KoD3,\\n     KYNDRYL_ONLY_DATE_3.USER_ID AS UUID_KoD3,\\n    KYNDRYL_ONLY_DATE_4.KYNDRYL_ONLY_DATE_4,\\n     KYNDRYL_ONLY_DATE_4.UPDATED_TIMESTAMP AS UTS_KoD4,\\n     KYNDRYL_ONLY_DATE_4.USER_ID AS UUID_KoD4,\\n    KYNDRYL_ONLY_DATE_5.KYNDRYL_ONLY_DATE_5,\\n     KYNDRYL_ONLY_DATE_5.UPDATED_TIMESTAMP AS UTS_KoD5,\\n     KYNDRYL_ONLY_DATE_5.USER_ID AS UUID_KoD5,\\n    KYNDRYL_ONLY_DATE_6.KYNDRYL_ONLY_DATE_6,\\n     KYNDRYL_ONLY_DATE_6.UPDATED_TIMESTAMP AS UTS_KoD6,\\n     KYNDRYL_ONLY_DATE_6.USER_ID AS UUID_KoD6,\\n    KYNDRYL_ONLY_DATE_7.KYNDRYL_ONLY_DATE_7,\\n     KYNDRYL_ONLY_DATE_7.UPDATED_TIMESTAMP AS UTS_KoD7,\\n     KYNDRYL_ONLY_DATE_7.USER_ID AS UUID_KoD7,\\n    KYNDRYL_ONLY_DATE_8.KYNDRYL_ONLY_DATE_8,\\n     KYNDRYL_ONLY_DATE_8.UPDATED_TIMESTAMP AS UTS_KoD8,\\n     KYNDRYL_ONLY_DATE_8.USER_ID AS UUID_KoD8,\\n    KYNDRYL_ONLY_DATE_9.KYNDRYL_ONLY_DATE_9,\\n     KYNDRYL_ONLY_DATE_9.UPDATED_TIMESTAMP AS UTS_KoD9,\\n     KYNDRYL_ONLY_DATE_9.USER_ID AS UUID_KoD9,\\n    KYNDRYL_ONLY_DATE_10.KYNDRYL_ONLY_DATE_10,\\n     KYNDRYL_ONLY_DATE_10.UPDATED_TIMESTAMP AS UTS_KoD10,\\n     KYNDRYL_ONLY_DATE_10.USER_ID AS UUID_KoD10,\\n    KYNDRYL_ONLY_DATE_11.KYNDRYL_ONLY_DATE_11,\\n     KYNDRYL_ONLY_DATE_11.UPDATED_TIMESTAMP AS UTS_KoD11,\\n     KYNDRYL_ONLY_DATE_11.USER_ID AS UUID_KoD11,\\n    KYNDRYL_ONLY_DATE_12.KYNDRYL_ONLY_DATE_12,\\n     KYNDRYL_ONLY_DATE_12.UPDATED_TIMESTAMP AS UTS_KoD12,\\n     KYNDRYL_ONLY_DATE_12.USER_ID AS UUID_KoD12,\\n    KYNDRYL_ONLY_DATE_13.KYNDRYL_ONLY_DATE_13,\\n     KYNDRYL_ONLY_DATE_13.UPDATED_TIMESTAMP AS UTS_KoD13,\\n     KYNDRYL_ONLY_DATE_13.USER_ID AS UUID_KoD13,\\n    KYNDRYL_ONLY_DATE_14.KYNDRYL_ONLY_DATE_14,\\n     KYNDRYL_ONLY_DATE_14.UPDATED_TIMESTAMP AS UTS_KoD14,\\n     KYNDRYL_ONLY_DATE_14.USER_ID AS UUID_KoD14,\\n    KYNDRYL_ONLY_DATE_15.KYNDRYL_ONLY_DATE_15,\\n     KYNDRYL_ONLY_DATE_15.UPDATED_TIMESTAMP AS UTS_KoD15,\\n     KYNDRYL_ONLY_DATE_15.USER_ID AS UUID_KoD15\\nFrom\\n     PGMPDM.PROC_DIM PROCS\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID\\n     INNER JOIN (\\n          Select distinct\\n               PROC_DIM_UID\\n          From     \\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM\\n          Where     \\n               COALESCE(\\n                    IBM_ONLY_TEXT_1,\\n                    IBM_ONLY_TEXT_2,\\n                    IBM_ONLY_TEXT_3,\\n                    IBM_ONLY_TEXT_4,\\n                    IBM_ONLY_TEXT_5,\\n                    IBM_ONLY_TEXT_6,\\n                    IBM_ONLY_TEXT_7,\\n                    IBM_ONLY_TEXT_8,\\n                    IBM_ONLY_TEXT_9,\\n                    IBM_ONLY_TEXT_10,\\n                    IBM_ONLY_TEXT_11,\\n                    IBM_ONLY_TEXT_12,\\n                    IBM_ONLY_TEXT_13,\\n                    IBM_ONLY_TEXT_14,\\n                    IBM_ONLY_TEXT_15,\\n                    IBM_ONLY_TEXT_16,\\n                    IBM_ONLY_TEXT_17,\\n                    IBM_ONLY_TEXT_18,\\n                    IBM_ONLY_TEXT_19,\\n                    IBM_ONLY_TEXT_20,\\n                    IBM_ONLY_TEXT_21,\\n                    IBM_ONLY_TEXT_22,\\n                    IBM_ONLY_TEXT_23,\\n                    IBM_ONLY_TEXT_24,\\n                    IBM_ONLY_TEXT_25,\\n                    --IBM_ONLY_TEXT_26,\\n                    --IBM_ONLY_TEXT_27,\\n                    CAST(IBM_ONLY_DATE_1 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_2 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_3 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_4 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_5 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_6 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_7 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_8 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_9 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_10 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_11 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_12 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_13 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_14 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_15 AS VARCHAR)\\n               ) NOT IN (\\'\\', \\'NULL\\')\\n     ) KoPROC ON KoPROC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_1 AS KYNDRYL_ONLY_TEXT_1,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_1                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_1 = DKo.IBM_ONLY_TEXT_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_1 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_1 ON KYNDRYL_ONLY_TEXT_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_2 AS KYNDRYL_ONLY_TEXT_2,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_2                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_2 = DKo.IBM_ONLY_TEXT_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_2 not in (\\'\\') \\n     )KYNDRYL_ONLY_TEXT_2 ON KYNDRYL_ONLY_TEXT_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_3 AS KYNDRYL_ONLY_TEXT_3,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_3                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_3 = DKo.IBM_ONLY_TEXT_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_3 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_3 ON KYNDRYL_ONLY_TEXT_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_4 AS KYNDRYL_ONLY_TEXT_4,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP               \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_4                              \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_4 = DKo.IBM_ONLY_TEXT_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_4 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_4 ON KYNDRYL_ONLY_TEXT_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_5 AS KYNDRYL_ONLY_TEXT_5,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_5               \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_5 = DKo.IBM_ONLY_TEXT_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_5 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_5 ON KYNDRYL_ONLY_TEXT_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_6 AS KYNDRYL_ONLY_TEXT_6,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_6                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_6 = DKo.IBM_ONLY_TEXT_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_6 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_6 ON KYNDRYL_ONLY_TEXT_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_7 AS KYNDRYL_ONLY_TEXT_7,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_7                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_7 = DKo.IBM_ONLY_TEXT_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_7 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_7 ON KYNDRYL_ONLY_TEXT_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_8 AS KYNDRYL_ONLY_TEXT_8,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_8                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_8 = DKo.IBM_ONLY_TEXT_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_8 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_8 ON KYNDRYL_ONLY_TEXT_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_9 AS KYNDRYL_ONLY_TEXT_9,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_9                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_9 = DKo.IBM_ONLY_TEXT_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_9 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_9 ON KYNDRYL_ONLY_TEXT_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_10 AS KYNDRYL_ONLY_TEXT_10,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_10                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_10 = DKo.IBM_ONLY_TEXT_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_10 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_10 ON KYNDRYL_ONLY_TEXT_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_11 AS KYNDRYL_ONLY_TEXT_11,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_11                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_11 = DKo.IBM_ONLY_TEXT_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_11 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_11 ON KYNDRYL_ONLY_TEXT_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_12 AS KYNDRYL_ONLY_TEXT_12,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_12                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_12 = DKo.IBM_ONLY_TEXT_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_12 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_12 ON KYNDRYL_ONLY_TEXT_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_13 AS KYNDRYL_ONLY_TEXT_13,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_13                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_13 = DKo.IBM_ONLY_TEXT_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_13 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_13 ON KYNDRYL_ONLY_TEXT_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_14 AS KYNDRYL_ONLY_TEXT_14,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_14\\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_14 = DKo.IBM_ONLY_TEXT_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_14 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_14 ON KYNDRYL_ONLY_TEXT_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_15 AS KYNDRYL_ONLY_TEXT_15,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_15                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_15 = DKo.IBM_ONLY_TEXT_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_15 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_15 ON KYNDRYL_ONLY_TEXT_15.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_16 AS KYNDRYL_ONLY_TEXT_16,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_16,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_16                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_16 = DKo.IBM_ONLY_TEXT_16)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_16 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_16 ON KYNDRYL_ONLY_TEXT_16.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_17 AS KYNDRYL_ONLY_TEXT_17,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_17,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_17                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_17 = DKo.IBM_ONLY_TEXT_17)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_17 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_17 ON KYNDRYL_ONLY_TEXT_17.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_18 AS KYNDRYL_ONLY_TEXT_18,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_18,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_18                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_18 = DKo.IBM_ONLY_TEXT_18)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_18 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_18 ON KYNDRYL_ONLY_TEXT_18.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_19 AS KYNDRYL_ONLY_TEXT_19,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_19,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_19                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_19 = DKo.IBM_ONLY_TEXT_19)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_19 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_19 ON KYNDRYL_ONLY_TEXT_19.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_20 AS KYNDRYL_ONLY_TEXT_20,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_20,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_20                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_20 = DKo.IBM_ONLY_TEXT_20)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_20 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_20 ON KYNDRYL_ONLY_TEXT_20.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_21 AS KYNDRYL_ONLY_TEXT_21,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_21,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_21                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_21 = DKo.IBM_ONLY_TEXT_21)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_21 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_21 ON KYNDRYL_ONLY_TEXT_21.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_22 AS KYNDRYL_ONLY_TEXT_22,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_22,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_22                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_22 = DKo.IBM_ONLY_TEXT_22)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_22 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_22 ON KYNDRYL_ONLY_TEXT_22.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_23 AS KYNDRYL_ONLY_TEXT_23,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_23,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_23                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_23 = DKo.IBM_ONLY_TEXT_23)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_23 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_23 ON KYNDRYL_ONLY_TEXT_23.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_24 AS KYNDRYL_ONLY_TEXT_24,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_24,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_24                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_24 = DKo.IBM_ONLY_TEXT_24)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_24 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_24 ON KYNDRYL_ONLY_TEXT_24.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_25 AS KYNDRYL_ONLY_TEXT_25,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_25,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_25                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_25 = DKo.IBM_ONLY_TEXT_25)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_25 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_25 ON KYNDRYL_ONLY_TEXT_25.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    /*\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_26 AS KYNDRYL_ONLY_TEXT_26,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_26,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_26                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_26 = DKo.IBM_ONLY_TEXT_26)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_26 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_26 ON KYNDRYL_ONLY_TEXT_26.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_27 AS KYNDRYL_ONLY_TEXT_27,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_27,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_27               \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_27 = DKo.IBM_ONLY_TEXT_27)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_27 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_27 ON KYNDRYL_ONLY_TEXT_27.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     */\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_1 AS KYNDRYL_ONLY_DATE_1,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_1                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_1 = DKo.IBM_ONLY_DATE_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_1 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_1 ON KYNDRYL_ONLY_DATE_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_2 AS KYNDRYL_ONLY_DATE_2,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_2                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_2 = DKo.IBM_ONLY_DATE_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_2 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_2 ON KYNDRYL_ONLY_DATE_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_3 AS KYNDRYL_ONLY_DATE_3,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_3                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_3 = DKo.IBM_ONLY_DATE_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_3 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_3 ON KYNDRYL_ONLY_DATE_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_4 AS KYNDRYL_ONLY_DATE_4,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP     \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_4                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_4 = DKo.IBM_ONLY_DATE_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_4 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_4 ON KYNDRYL_ONLY_DATE_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_5 AS KYNDRYL_ONLY_DATE_5,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_5                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_5 = DKo.IBM_ONLY_DATE_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_5 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_5 ON KYNDRYL_ONLY_DATE_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_6 AS KYNDRYL_ONLY_DATE_6,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_6                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_6 = DKo.IBM_ONLY_DATE_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_6 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_6 ON KYNDRYL_ONLY_DATE_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_7 AS KYNDRYL_ONLY_DATE_7,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_7                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_7 = DKo.IBM_ONLY_DATE_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_7 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_7 ON KYNDRYL_ONLY_DATE_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_8 AS KYNDRYL_ONLY_DATE_8,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_8                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_8 = DKo.IBM_ONLY_DATE_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_8 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_8 ON KYNDRYL_ONLY_DATE_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_9 AS KYNDRYL_ONLY_DATE_9,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_9                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_9 = DKo.IBM_ONLY_DATE_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_9 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_9 ON KYNDRYL_ONLY_DATE_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_10 AS KYNDRYL_ONLY_DATE_10,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_10                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_10 = DKo.IBM_ONLY_DATE_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_10 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_10 ON KYNDRYL_ONLY_DATE_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_11 AS KYNDRYL_ONLY_DATE_11,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_11                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_11 = DKo.IBM_ONLY_DATE_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_11 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_11 ON KYNDRYL_ONLY_DATE_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_12 AS KYNDRYL_ONLY_DATE_12,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_12                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_12 = DKo.IBM_ONLY_DATE_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_12 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_12 ON KYNDRYL_ONLY_DATE_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_13 AS KYNDRYL_ONLY_DATE_13,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_13                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_13 = DKo.IBM_ONLY_DATE_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_13 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_13 ON KYNDRYL_ONLY_DATE_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_14 AS KYNDRYL_ONLY_DATE_14,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_14                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_14 = DKo.IBM_ONLY_DATE_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_14 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_14 ON KYNDRYL_ONLY_DATE_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_15 AS KYNDRYL_ONLY_DATE_15,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_15                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_15 = DKo.IBM_ONLY_DATE_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_15 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_15 ON KYNDRYL_ONLY_DATE_15.PROC_DIM_UID = PROCS.PROC_DIM_UID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData sort(asc(PROC_DIM_UID, false)) ~> sortedSourceData",
						"sortedSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTRPTS_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_CSTM_PAF')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedSourceData"
						},
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          CNTRCT_DIM_UID as integer,",
						"          CNTRCT_NM as string,",
						"          PUBLIC_TEXT_1 as string,",
						"          UTS_PT1 as timestamp,",
						"          UUID_PT1 as binary,",
						"          PUBLIC_TEXT_2 as string,",
						"          UTS_PT2 as timestamp,",
						"          UUID_PT2 as binary,",
						"          PUBLIC_TEXT_3 as string,",
						"          UTS_PT3 as timestamp,",
						"          UUID_PT3 as binary,",
						"          PUBLIC_TEXT_4 as string,",
						"          UTS_PT4 as timestamp,",
						"          UUID_PT4 as binary,",
						"          PUBLIC_TEXT_5 as string,",
						"          UTS_PT5 as timestamp,",
						"          UUID_PT5 as binary,",
						"          PUBLIC_TEXT_6 as string,",
						"          UTS_PT6 as timestamp,",
						"          UUID_PT6 as binary,",
						"          PUBLIC_TEXT_7 as string,",
						"          UTS_PT7 as timestamp,",
						"          UUID_PT7 as binary,",
						"          PUBLIC_TEXT_8 as string,",
						"          UTS_PT8 as timestamp,",
						"          UUID_PT8 as binary,",
						"          PUBLIC_TEXT_9 as string,",
						"          UTS_PT9 as timestamp,",
						"          UUID_PT9 as binary,",
						"          PUBLIC_TEXT_10 as string,",
						"          UTS_PT10 as timestamp,",
						"          UUID_PT10 as binary,",
						"          PUBLIC_TEXT_11 as string,",
						"          UTS_PT11 as timestamp,",
						"          UUID_PT11 as binary,",
						"          PUBLIC_TEXT_12 as string,",
						"          UTS_PT12 as timestamp,",
						"          UUID_PT12 as binary,",
						"          PUBLIC_TEXT_13 as string,",
						"          UTS_PT13 as timestamp,",
						"          UUID_PT13 as binary,",
						"          PUBLIC_TEXT_14 as string,",
						"          UTS_PT14 as timestamp,",
						"          UUID_PT14 as binary,",
						"          PUBLIC_TEXT_15 as string,",
						"          UTS_PT15 as timestamp,",
						"          UUID_PT15 as binary,",
						"          PUBLIC_TEXT_16 as string,",
						"          UTS_PT16 as timestamp,",
						"          UUID_PT16 as binary,",
						"          PUBLIC_TEXT_17 as string,",
						"          UTS_PT17 as timestamp,",
						"          UUID_PT17 as binary,",
						"          PUBLIC_TEXT_18 as string,",
						"          UTS_PT18 as timestamp,",
						"          UUID_PT18 as binary,",
						"          PUBLIC_TEXT_19 as string,",
						"          UTS_PT19 as timestamp,",
						"          UUID_PT19 as binary,",
						"          PUBLIC_TEXT_20 as string,",
						"          UTS_PT20 as timestamp,",
						"          UUID_PT20 as binary,",
						"          PUBLIC_TEXT_21 as string,",
						"          UTS_PT21 as timestamp,",
						"          UUID_PT21 as binary,",
						"          PUBLIC_TEXT_22 as string,",
						"          UTS_PT22 as timestamp,",
						"          UUID_PT22 as binary,",
						"          PUBLIC_TEXT_23 as string,",
						"          UTS_PT23 as timestamp,",
						"          UUID_PT23 as binary,",
						"          PUBLIC_TEXT_24 as string,",
						"          UTS_PT24 as timestamp,",
						"          UUID_PT24 as binary,",
						"          PUBLIC_TEXT_25 as string,",
						"          UTS_PT25 as timestamp,",
						"          UUID_PT25 as binary,",
						"          PUBLIC_TEXT_26 as string,",
						"          UTS_PT26 as timestamp,",
						"          UUID_PT26 as binary,",
						"          PUBLIC_TEXT_27 as string,",
						"          UTS_PT27 as timestamp,",
						"          UUID_PT27 as binary,",
						"          PUBLIC_TEXT_28 as string,",
						"          UTS_PT28 as timestamp,",
						"          UUID_PT28 as binary,",
						"          PUBLIC_TEXT_29 as string,",
						"          UTS_PT29 as timestamp,",
						"          UUID_PT29 as binary,",
						"          PUBLIC_TEXT_30 as string,",
						"          UTS_PT30 as timestamp,",
						"          UUID_PT30 as binary,",
						"          PUBLIC_TEXT_31 as string,",
						"          UTS_PT31 as timestamp,",
						"          UUID_PT31 as binary,",
						"          PUBLIC_TEXT_32 as string,",
						"          UTS_PT32 as timestamp,",
						"          UUID_PT32 as binary,",
						"          PUBLIC_TEXT_33 as string,",
						"          UTS_PT33 as timestamp,",
						"          UUID_PT33 as binary,",
						"          PUBLIC_TEXT_34 as string,",
						"          UTS_PT34 as timestamp,",
						"          UUID_PT34 as binary,",
						"          PUBLIC_TEXT_35 as string,",
						"          UTS_PT35 as timestamp,",
						"          UUID_PT35 as binary,",
						"          PUBLIC_TEXT_36 as string,",
						"          UTS_PT36 as timestamp,",
						"          UUID_PT36 as string,",
						"          PUBLIC_TEXT_37 as string,",
						"          UTS_PT37 as timestamp,",
						"          UUID_PT37 as string,",
						"          PUBLIC_DATE_1 as date,",
						"          UTS_PD1 as timestamp,",
						"          UUID_PD1 as binary,",
						"          PUBLIC_DATE_2 as date,",
						"          UTS_PD2 as timestamp,",
						"          UUID_PD2 as binary,",
						"          PUBLIC_DATE_3 as date,",
						"          UTS_PD3 as timestamp,",
						"          UUID_PD3 as binary,",
						"          PUBLIC_DATE_4 as date,",
						"          UTS_PD4 as timestamp,",
						"          UUID_PD4 as binary,",
						"          PUBLIC_DATE_5 as date,",
						"          UTS_PD5 as timestamp,",
						"          UUID_PD5 as binary,",
						"          PUBLIC_DATE_6 as date,",
						"          UTS_PD6 as timestamp,",
						"          UUID_PD6 as binary,",
						"          PUBLIC_DATE_7 as date,",
						"          UTS_PD7 as timestamp,",
						"          UUID_PD7 as binary,",
						"          PUBLIC_DATE_8 as date,",
						"          UTS_PD8 as timestamp,",
						"          UUID_PD8 as binary,",
						"          PUBLIC_DATE_9 as date,",
						"          UTS_PD9 as timestamp,",
						"          UUID_PD9 as binary,",
						"          PUBLIC_DATE_10 as date,",
						"          UTS_PD10 as timestamp,",
						"          UUID_PD10 as binary,",
						"          PUBLIC_DATE_11 as date,",
						"          UTS_PD11 as timestamp,",
						"          UUID_PD11 as binary,",
						"          PUBLIC_DATE_12 as date,",
						"          UTS_PD12 as timestamp,",
						"          UUID_PD12 as binary,",
						"          PUBLIC_DATE_13 as date,",
						"          UTS_PD13 as timestamp,",
						"          UUID_PD13 as binary,",
						"          PUBLIC_DATE_14 as date,",
						"          UTS_PD14 as timestamp,",
						"          UUID_PD14 as binary,",
						"          PUBLIC_DATE_15 as date,",
						"          UTS_PD15 as timestamp,",
						"          UUID_PD15 as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select distinct\\n     PROCS.PROC_DIM_UID,\\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM,\\n    PUBLIC_TEXT_1.PUBLIC_TEXT_1,\\n     PUBLIC_TEXT_1.UPDATED_TIMESTAMP AS UTS_PT1,\\n     PUBLIC_TEXT_1.USER_ID AS UUID_PT1,\\n    PUBLIC_TEXT_2.PUBLIC_TEXT_2,\\n     PUBLIC_TEXT_2.UPDATED_TIMESTAMP AS UTS_PT2,\\n     PUBLIC_TEXT_2.USER_ID AS UUID_PT2,\\n    PUBLIC_TEXT_3.PUBLIC_TEXT_3,\\n     PUBLIC_TEXT_3.UPDATED_TIMESTAMP AS UTS_PT3,\\n     PUBLIC_TEXT_3.USER_ID AS UUID_PT3,\\n    PUBLIC_TEXT_4.PUBLIC_TEXT_4,\\n     PUBLIC_TEXT_4.UPDATED_TIMESTAMP AS UTS_PT4,\\n     PUBLIC_TEXT_4.USER_ID AS UUID_PT4,\\n    PUBLIC_TEXT_5.PUBLIC_TEXT_5,\\n     PUBLIC_TEXT_5.UPDATED_TIMESTAMP AS UTS_PT5,\\n     PUBLIC_TEXT_5.USER_ID AS UUID_PT5,\\n    PUBLIC_TEXT_6.PUBLIC_TEXT_6,\\n     PUBLIC_TEXT_6.UPDATED_TIMESTAMP AS UTS_PT6,\\n     PUBLIC_TEXT_6.USER_ID AS UUID_PT6,\\n    PUBLIC_TEXT_7.PUBLIC_TEXT_7,\\n     PUBLIC_TEXT_7.UPDATED_TIMESTAMP AS UTS_PT7,\\n     PUBLIC_TEXT_7.USER_ID AS UUID_PT7,\\n    PUBLIC_TEXT_8.PUBLIC_TEXT_8,\\n     PUBLIC_TEXT_8.UPDATED_TIMESTAMP AS UTS_PT8,\\n     PUBLIC_TEXT_8.USER_ID AS UUID_PT8,\\n    PUBLIC_TEXT_9.PUBLIC_TEXT_9,\\n     PUBLIC_TEXT_9.UPDATED_TIMESTAMP AS UTS_PT9,\\n     PUBLIC_TEXT_9.USER_ID AS UUID_PT9,\\n    PUBLIC_TEXT_10.PUBLIC_TEXT_10,\\n     PUBLIC_TEXT_10.UPDATED_TIMESTAMP AS UTS_PT10,\\n     PUBLIC_TEXT_10.USER_ID AS UUID_PT10,\\n    PUBLIC_TEXT_11.PUBLIC_TEXT_11,\\n     PUBLIC_TEXT_11.UPDATED_TIMESTAMP AS UTS_PT11,\\n     PUBLIC_TEXT_11.USER_ID AS UUID_PT11,\\n    PUBLIC_TEXT_12.PUBLIC_TEXT_12,\\n     PUBLIC_TEXT_12.UPDATED_TIMESTAMP AS UTS_PT12,\\n     PUBLIC_TEXT_12.USER_ID AS UUID_PT12,\\n    PUBLIC_TEXT_13.PUBLIC_TEXT_13,\\n     PUBLIC_TEXT_13.UPDATED_TIMESTAMP AS UTS_PT13,\\n     PUBLIC_TEXT_13.USER_ID AS UUID_PT13,\\n    PUBLIC_TEXT_14.PUBLIC_TEXT_14,\\n     PUBLIC_TEXT_14.UPDATED_TIMESTAMP AS UTS_PT14,\\n     PUBLIC_TEXT_14.USER_ID AS UUID_PT14,\\n    PUBLIC_TEXT_15.PUBLIC_TEXT_15,\\n     PUBLIC_TEXT_15.UPDATED_TIMESTAMP AS UTS_PT15,\\n     PUBLIC_TEXT_15.USER_ID AS UUID_PT15,\\n    PUBLIC_TEXT_16.PUBLIC_TEXT_16,\\n     PUBLIC_TEXT_16.UPDATED_TIMESTAMP AS UTS_PT16,\\n     PUBLIC_TEXT_16.USER_ID AS UUID_PT16,\\n    PUBLIC_TEXT_17.PUBLIC_TEXT_17,\\n     PUBLIC_TEXT_17.UPDATED_TIMESTAMP AS UTS_PT17,\\n     PUBLIC_TEXT_17.USER_ID AS UUID_PT17,\\n    PUBLIC_TEXT_18.PUBLIC_TEXT_18,\\n     PUBLIC_TEXT_18.UPDATED_TIMESTAMP AS UTS_PT18,\\n     PUBLIC_TEXT_18.USER_ID AS UUID_PT18,\\n    PUBLIC_TEXT_19.PUBLIC_TEXT_19,\\n     PUBLIC_TEXT_19.UPDATED_TIMESTAMP AS UTS_PT19,\\n     PUBLIC_TEXT_19.USER_ID AS UUID_PT19,\\n    PUBLIC_TEXT_20.PUBLIC_TEXT_20,\\n     PUBLIC_TEXT_20.UPDATED_TIMESTAMP AS UTS_PT20,\\n     PUBLIC_TEXT_20.USER_ID AS UUID_PT20,\\n    PUBLIC_TEXT_21.PUBLIC_TEXT_21,\\n     PUBLIC_TEXT_21.UPDATED_TIMESTAMP AS UTS_PT21,\\n     PUBLIC_TEXT_21.USER_ID AS UUID_PT21,\\n    PUBLIC_TEXT_22.PUBLIC_TEXT_22,\\n     PUBLIC_TEXT_22.UPDATED_TIMESTAMP AS UTS_PT22,\\n     PUBLIC_TEXT_22.USER_ID AS UUID_PT22,\\n    PUBLIC_TEXT_23.PUBLIC_TEXT_23,\\n     PUBLIC_TEXT_23.UPDATED_TIMESTAMP AS UTS_PT23,\\n     PUBLIC_TEXT_23.USER_ID AS UUID_PT23,\\n    PUBLIC_TEXT_24.PUBLIC_TEXT_24,\\n     PUBLIC_TEXT_24.UPDATED_TIMESTAMP AS UTS_PT24,\\n     PUBLIC_TEXT_24.USER_ID AS UUID_PT24,\\n    PUBLIC_TEXT_25.PUBLIC_TEXT_25,\\n     PUBLIC_TEXT_25.UPDATED_TIMESTAMP AS UTS_PT25,\\n     PUBLIC_TEXT_25.USER_ID AS UUID_PT25,\\n    PUBLIC_TEXT_26.PUBLIC_TEXT_26,\\n     PUBLIC_TEXT_26.UPDATED_TIMESTAMP AS UTS_PT26,\\n     PUBLIC_TEXT_26.USER_ID AS UUID_PT26,\\n    PUBLIC_TEXT_27.PUBLIC_TEXT_27,\\n     PUBLIC_TEXT_27.UPDATED_TIMESTAMP AS UTS_PT27,\\n     PUBLIC_TEXT_27.USER_ID AS UUID_PT27,\\n     PUBLIC_TEXT_28.PUBLIC_TEXT_28,\\n     PUBLIC_TEXT_28.UPDATED_TIMESTAMP AS UTS_PT28,\\n     PUBLIC_TEXT_28.USER_ID AS UUID_PT28,\\n    PUBLIC_TEXT_29.PUBLIC_TEXT_29,\\n     PUBLIC_TEXT_29.UPDATED_TIMESTAMP AS UTS_PT29,\\n     PUBLIC_TEXT_29.USER_ID AS UUID_PT29,\\n    PUBLIC_TEXT_30.PUBLIC_TEXT_30,\\n     PUBLIC_TEXT_30.UPDATED_TIMESTAMP AS UTS_PT30,\\n     PUBLIC_TEXT_30.USER_ID AS UUID_PT30,\\n    PUBLIC_TEXT_31.PUBLIC_TEXT_31,\\n     PUBLIC_TEXT_31.UPDATED_TIMESTAMP AS UTS_PT31,\\n     PUBLIC_TEXT_31.USER_ID AS UUID_PT31,\\n    PUBLIC_TEXT_32.PUBLIC_TEXT_32,\\n     PUBLIC_TEXT_32.UPDATED_TIMESTAMP AS UTS_PT32,\\n     PUBLIC_TEXT_32.USER_ID AS UUID_PT32,\\n    PUBLIC_TEXT_33.PUBLIC_TEXT_33,\\n     PUBLIC_TEXT_33.UPDATED_TIMESTAMP AS UTS_PT33,\\n     PUBLIC_TEXT_33.USER_ID AS UUID_PT33,\\n    PUBLIC_TEXT_34.PUBLIC_TEXT_34,\\n     PUBLIC_TEXT_34.UPDATED_TIMESTAMP AS UTS_PT34,\\n     PUBLIC_TEXT_34.USER_ID AS UUID_PT34,\\n    PUBLIC_TEXT_35.PUBLIC_TEXT_35,\\n     PUBLIC_TEXT_35.UPDATED_TIMESTAMP AS UTS_PT35,\\n     PUBLIC_TEXT_35.USER_ID AS UUID_PT35,\\n    \\'\\' AS PUBLIC_TEXT_36, -- PUBLIC_TEXT_36.PUBLIC_TEXT_36,\\n     CURRENT_TIMESTAMP AS UTS_PT36, -- PUBLIC_TEXT_36.UPDATED_TIMESTAMP AS UTS_PT36,\\n     \\'\\' AS UUID_PT36, -- PUBLIC_TEXT_36.USER_ID AS UUID_PT36,\\n    \\'\\' AS PUBLIC_TEXT_37, -- PUBLIC_TEXT_37.PUBLIC_TEXT_37,\\n     CURRENT_TIMESTAMP AS UTS_PT37, -- PUBLIC_TEXT_37.UPDATED_TIMESTAMP AS UTS_PT37,\\n     \\'\\' AS UUID_PT37, -- PUBLIC_TEXT_37.USER_ID AS UUID_PT37,\\n    PUBLIC_DATE_1.PUBLIC_DATE_1,\\n     PUBLIC_DATE_1.UPDATED_TIMESTAMP AS UTS_PD1,\\n     PUBLIC_DATE_1.USER_ID AS UUID_PD1,\\n    PUBLIC_DATE_2.PUBLIC_DATE_2,\\n     PUBLIC_DATE_2.UPDATED_TIMESTAMP AS UTS_PD2,\\n     PUBLIC_DATE_2.USER_ID AS UUID_PD2,\\n    PUBLIC_DATE_3.PUBLIC_DATE_3,\\n     PUBLIC_DATE_3.UPDATED_TIMESTAMP AS UTS_PD3,\\n     PUBLIC_DATE_3.USER_ID AS UUID_PD3,\\n    PUBLIC_DATE_4.PUBLIC_DATE_4,\\n     PUBLIC_DATE_4.UPDATED_TIMESTAMP AS UTS_PD4,\\n     PUBLIC_DATE_4.USER_ID AS UUID_PD4,\\n    PUBLIC_DATE_5.PUBLIC_DATE_5,\\n     PUBLIC_DATE_5.UPDATED_TIMESTAMP AS UTS_PD5,\\n     PUBLIC_DATE_5.USER_ID AS UUID_PD5,\\n    PUBLIC_DATE_6.PUBLIC_DATE_6,\\n     PUBLIC_DATE_6.UPDATED_TIMESTAMP AS UTS_PD6,\\n     PUBLIC_DATE_6.USER_ID AS UUID_PD6,\\n    PUBLIC_DATE_7.PUBLIC_DATE_7,\\n     PUBLIC_DATE_7.UPDATED_TIMESTAMP AS UTS_PD7,\\n     PUBLIC_DATE_7.USER_ID AS UUID_PD7,\\n    PUBLIC_DATE_8.PUBLIC_DATE_8,\\n     PUBLIC_DATE_8.UPDATED_TIMESTAMP AS UTS_PD8,\\n     PUBLIC_DATE_8.USER_ID AS UUID_PD8,\\n    PUBLIC_DATE_9.PUBLIC_DATE_9,\\n     PUBLIC_DATE_9.UPDATED_TIMESTAMP AS UTS_PD9,\\n     PUBLIC_DATE_9.USER_ID AS UUID_PD9,\\n    PUBLIC_DATE_10.PUBLIC_DATE_10,\\n     PUBLIC_DATE_10.UPDATED_TIMESTAMP AS UTS_PD10,\\n     PUBLIC_DATE_10.USER_ID AS UUID_PD10,\\n    PUBLIC_DATE_11.PUBLIC_DATE_11,\\n     PUBLIC_DATE_11.UPDATED_TIMESTAMP AS UTS_PD11,\\n     PUBLIC_DATE_11.USER_ID AS UUID_PD11,\\n    PUBLIC_DATE_12.PUBLIC_DATE_12,\\n     PUBLIC_DATE_12.UPDATED_TIMESTAMP AS UTS_PD12,\\n     PUBLIC_DATE_12.USER_ID AS UUID_PD12,\\n    PUBLIC_DATE_13.PUBLIC_DATE_13,\\n     PUBLIC_DATE_13.UPDATED_TIMESTAMP AS UTS_PD13,\\n     PUBLIC_DATE_13.USER_ID AS UUID_PD13,\\n    PUBLIC_DATE_14.PUBLIC_DATE_14,\\n     PUBLIC_DATE_14.UPDATED_TIMESTAMP AS UTS_PD14,\\n     PUBLIC_DATE_14.USER_ID AS UUID_PD14,\\n    PUBLIC_DATE_15.PUBLIC_DATE_15,\\n     PUBLIC_DATE_15.UPDATED_TIMESTAMP AS UTS_PD15,\\n     PUBLIC_DATE_15.USER_ID AS UUID_PD15\\nFrom\\n     PGMPDM.PROC_DIM PROCS\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID\\n     INNER JOIN (\\n          Select distinct\\n               PROC_DIM_UID\\n          From     \\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM\\n          Where     \\n               COALESCE(\\n                    PUBLIC_TEXT_1,\\n                    PUBLIC_TEXT_2,\\n                    PUBLIC_TEXT_3,\\n                    PUBLIC_TEXT_4,\\n                    PUBLIC_TEXT_5,\\n                    PUBLIC_TEXT_6,\\n                    PUBLIC_TEXT_7,\\n                    PUBLIC_TEXT_8,\\n                    PUBLIC_TEXT_9,\\n                    PUBLIC_TEXT_10,\\n                    PUBLIC_TEXT_11,\\n                    PUBLIC_TEXT_12,\\n                    PUBLIC_TEXT_13,\\n                    PUBLIC_TEXT_14,\\n                    PUBLIC_TEXT_15,\\n                    PUBLIC_TEXT_16,\\n                    PUBLIC_TEXT_17,\\n                    PUBLIC_TEXT_18,\\n                    PUBLIC_TEXT_19,\\n                    PUBLIC_TEXT_20,\\n                    PUBLIC_TEXT_21,\\n                    PUBLIC_TEXT_22,\\n                    PUBLIC_TEXT_23,\\n                    PUBLIC_TEXT_24,\\n                    PUBLIC_TEXT_25,\\n                    PUBLIC_TEXT_26,\\n                    PUBLIC_TEXT_27,\\n                    PUBLIC_TEXT_28,\\n                    PUBLIC_TEXT_29,\\n                    PUBLIC_TEXT_30,\\n                    PUBLIC_TEXT_31,\\n                    PUBLIC_TEXT_32,\\n                    PUBLIC_TEXT_33,\\n                    PUBLIC_TEXT_34,\\n                    PUBLIC_TEXT_35,\\n                    --PUBLIC_TEXT_36,\\n                    --PUBLIC_TEXT_37,\\n                    CAST(PUBLIC_DATE_1 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_2 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_3 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_4 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_5 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_6 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_7 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_8 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_9 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_10 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_11 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_12 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_13 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_14 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_15 AS VARCHAR)\\n               ) NOT IN (\\'\\', \\'NULL\\')\\n     ) PPROC ON PPROC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_1 AS PUBLIC_TEXT_1,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_1                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_1 = DP.PUBLIC_TEXT_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_1 not in (\\'\\') \\n     ) PUBLIC_TEXT_1 ON PUBLIC_TEXT_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_2 AS PUBLIC_TEXT_2,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_2                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_2 = DP.PUBLIC_TEXT_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_2 not in (\\'\\') \\n     )PUBLIC_TEXT_2 ON PUBLIC_TEXT_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_3 AS PUBLIC_TEXT_3,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_3                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_3 = DP.PUBLIC_TEXT_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_3 not in (\\'\\') \\n     ) PUBLIC_TEXT_3 ON PUBLIC_TEXT_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_4 AS PUBLIC_TEXT_4,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP               \\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_4                              \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_4 = DP.PUBLIC_TEXT_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_4 not in (\\'\\') \\n     ) PUBLIC_TEXT_4 ON PUBLIC_TEXT_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_5 AS PUBLIC_TEXT_5,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_5               \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_5 = DP.PUBLIC_TEXT_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_5 not in (\\'\\') \\n     ) PUBLIC_TEXT_5 ON PUBLIC_TEXT_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_6 AS PUBLIC_TEXT_6,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_6                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_6 = DP.PUBLIC_TEXT_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_6 not in (\\'\\') \\n     ) PUBLIC_TEXT_6 ON PUBLIC_TEXT_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_7 AS PUBLIC_TEXT_7,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_7                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_7 = DP.PUBLIC_TEXT_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_7 not in (\\'\\') \\n     ) PUBLIC_TEXT_7 ON PUBLIC_TEXT_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_8 AS PUBLIC_TEXT_8,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_8                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_8 = DP.PUBLIC_TEXT_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_8 not in (\\'\\') \\n     ) PUBLIC_TEXT_8 ON PUBLIC_TEXT_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_9 AS PUBLIC_TEXT_9,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_9                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_9 = DP.PUBLIC_TEXT_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_9 not in (\\'\\') \\n     ) PUBLIC_TEXT_9 ON PUBLIC_TEXT_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_10 AS PUBLIC_TEXT_10,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_10                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_10 = DP.PUBLIC_TEXT_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_10 not in (\\'\\') \\n     ) PUBLIC_TEXT_10 ON PUBLIC_TEXT_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_11 AS PUBLIC_TEXT_11,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_11                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_11 = DP.PUBLIC_TEXT_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_11 not in (\\'\\') \\n     ) PUBLIC_TEXT_11 ON PUBLIC_TEXT_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_12 AS PUBLIC_TEXT_12,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_12                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_12 = DP.PUBLIC_TEXT_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_12 not in (\\'\\') \\n     ) PUBLIC_TEXT_12 ON PUBLIC_TEXT_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_13 AS PUBLIC_TEXT_13,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_13                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_13 = DP.PUBLIC_TEXT_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_13 not in (\\'\\') \\n     ) PUBLIC_TEXT_13 ON PUBLIC_TEXT_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_14 AS PUBLIC_TEXT_14,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_14\\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_14 = DP.PUBLIC_TEXT_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_14 not in (\\'\\') \\n     ) PUBLIC_TEXT_14 ON PUBLIC_TEXT_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_15 AS PUBLIC_TEXT_15,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_15                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_15 = DP.PUBLIC_TEXT_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_15 not in (\\'\\') \\n     ) PUBLIC_TEXT_15 ON PUBLIC_TEXT_15.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_16 AS PUBLIC_TEXT_16,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_16,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_16                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_16 = DP.PUBLIC_TEXT_16)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_16 not in (\\'\\') \\n     ) PUBLIC_TEXT_16 ON PUBLIC_TEXT_16.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_17 AS PUBLIC_TEXT_17,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_17,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_17                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_17 = DP.PUBLIC_TEXT_17)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_17 not in (\\'\\') \\n     ) PUBLIC_TEXT_17 ON PUBLIC_TEXT_17.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_18 AS PUBLIC_TEXT_18,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_18,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_18                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_18 = DP.PUBLIC_TEXT_18)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_18 not in (\\'\\') \\n     ) PUBLIC_TEXT_18 ON PUBLIC_TEXT_18.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_19 AS PUBLIC_TEXT_19,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_19,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_19                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_19 = DP.PUBLIC_TEXT_19)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_19 not in (\\'\\') \\n     ) PUBLIC_TEXT_19 ON PUBLIC_TEXT_19.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_20 AS PUBLIC_TEXT_20,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_20,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_20                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_20 = DP.PUBLIC_TEXT_20)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_20 not in (\\'\\') \\n     ) PUBLIC_TEXT_20 ON PUBLIC_TEXT_20.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_21 AS PUBLIC_TEXT_21,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_21,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_21                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_21 = DP.PUBLIC_TEXT_21)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_21 not in (\\'\\') \\n     ) PUBLIC_TEXT_21 ON PUBLIC_TEXT_21.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_22 AS PUBLIC_TEXT_22,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_22,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_22                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_22 = DP.PUBLIC_TEXT_22)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_22 not in (\\'\\') \\n     ) PUBLIC_TEXT_22 ON PUBLIC_TEXT_22.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_23 AS PUBLIC_TEXT_23,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_23,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_23                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_23 = DP.PUBLIC_TEXT_23)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_23 not in (\\'\\') \\n     ) PUBLIC_TEXT_23 ON PUBLIC_TEXT_23.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_24 AS PUBLIC_TEXT_24,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_24,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_24                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_24 = DP.PUBLIC_TEXT_24)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_24 not in (\\'\\') \\n     ) PUBLIC_TEXT_24 ON PUBLIC_TEXT_24.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_25 AS PUBLIC_TEXT_25,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_25,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_25                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_25 = DP.PUBLIC_TEXT_25)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_25 not in (\\'\\') \\n     ) PUBLIC_TEXT_25 ON PUBLIC_TEXT_25.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_26 AS PUBLIC_TEXT_26,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_26,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_26                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_26 = DP.PUBLIC_TEXT_26)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_26 not in (\\'\\') \\n     ) PUBLIC_TEXT_26 ON PUBLIC_TEXT_26.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_27 AS PUBLIC_TEXT_27,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_27,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_27               \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_27 = DP.PUBLIC_TEXT_27)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_27 not in (\\'\\') \\n     ) PUBLIC_TEXT_27 ON PUBLIC_TEXT_27.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_28 AS PUBLIC_TEXT_28,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_28,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_28                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_28 = DP.PUBLIC_TEXT_28)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_28 not in (\\'\\') \\n     ) PUBLIC_TEXT_28 ON PUBLIC_TEXT_28.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_29 AS PUBLIC_TEXT_29,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_29,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_29                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_29 = DP.PUBLIC_TEXT_29)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_29 not in (\\'\\') \\n     ) PUBLIC_TEXT_29 ON PUBLIC_TEXT_29.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_30 AS PUBLIC_TEXT_30,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_30,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_30                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_30 = DP.PUBLIC_TEXT_30)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_30 not in (\\'\\') \\n     ) PUBLIC_TEXT_30 ON PUBLIC_TEXT_30.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_31 AS PUBLIC_TEXT_31,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_31,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_31                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_31 = DP.PUBLIC_TEXT_31)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_31 not in (\\'\\') \\n     ) PUBLIC_TEXT_31 ON PUBLIC_TEXT_31.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_32 AS PUBLIC_TEXT_32,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_32,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_32                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_32 = DP.PUBLIC_TEXT_32)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_32 not in (\\'\\') \\n     ) PUBLIC_TEXT_32 ON PUBLIC_TEXT_32.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_33 AS PUBLIC_TEXT_33,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_33,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_33                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_33 = DP.PUBLIC_TEXT_33)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_33 not in (\\'\\') \\n     ) PUBLIC_TEXT_33 ON PUBLIC_TEXT_33.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_34 AS PUBLIC_TEXT_34,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_34,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_34                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_34 = DP.PUBLIC_TEXT_34)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_34 not in (\\'\\') \\n     ) PUBLIC_TEXT_34 ON PUBLIC_TEXT_34.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_35 AS PUBLIC_TEXT_35,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_35,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_35                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_35 = DP.PUBLIC_TEXT_35)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_35 not in (\\'\\') \\n     ) PUBLIC_TEXT_35 ON PUBLIC_TEXT_35.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    /*\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_36 AS PUBLIC_TEXT_36,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_36,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_36                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_36 = DP.PUBLIC_TEXT_36)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_36 not in (\\'\\') \\n     ) PUBLIC_TEXT_36 ON PUBLIC_TEXT_36.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_37 AS PUBLIC_TEXT_37,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_37,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_37               \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_37 = DP.PUBLIC_TEXT_37)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_37 not in (\\'\\') \\n     ) PUBLIC_TEXT_37 ON PUBLIC_TEXT_37.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    */\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_1 AS PUBLIC_DATE_1,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_1                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_1 = DP.PUBLIC_DATE_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_1 not like (\\'\\') \\n     ) PUBLIC_DATE_1 ON PUBLIC_DATE_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_2 AS PUBLIC_DATE_2,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_2                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_2 = DP.PUBLIC_DATE_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_2 not like (\\'\\') \\n     ) PUBLIC_DATE_2 ON PUBLIC_DATE_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_3 AS PUBLIC_DATE_3,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_3                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_3 = DP.PUBLIC_DATE_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_3 not like (\\'\\') \\n     ) PUBLIC_DATE_3 ON PUBLIC_DATE_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_4 AS PUBLIC_DATE_4,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP     \\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_4                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_4 = DP.PUBLIC_DATE_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_4 not like (\\'\\') \\n     ) PUBLIC_DATE_4 ON PUBLIC_DATE_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_5 AS PUBLIC_DATE_5,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_5                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_5 = DP.PUBLIC_DATE_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_5 not like (\\'\\') \\n     ) PUBLIC_DATE_5 ON PUBLIC_DATE_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_6 AS PUBLIC_DATE_6,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_6                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_6 = DP.PUBLIC_DATE_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_6 not like (\\'\\') \\n     ) PUBLIC_DATE_6 ON PUBLIC_DATE_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_7 AS PUBLIC_DATE_7,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_7                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_7 = DP.PUBLIC_DATE_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_7 not like (\\'\\') \\n     ) PUBLIC_DATE_7 ON PUBLIC_DATE_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_8 AS PUBLIC_DATE_8,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_8                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_8 = DP.PUBLIC_DATE_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_8 not like (\\'\\') \\n     ) PUBLIC_DATE_8 ON PUBLIC_DATE_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_9 AS PUBLIC_DATE_9,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_9                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_9 = DP.PUBLIC_DATE_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_9 not like (\\'\\') \\n     ) PUBLIC_DATE_9 ON PUBLIC_DATE_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_10 AS PUBLIC_DATE_10,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_10                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_10 = DP.PUBLIC_DATE_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_10 not like (\\'\\') \\n     ) PUBLIC_DATE_10 ON PUBLIC_DATE_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_11 AS PUBLIC_DATE_11,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_11                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_11 = DP.PUBLIC_DATE_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_11 not like (\\'\\') \\n     ) PUBLIC_DATE_11 ON PUBLIC_DATE_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_12 AS PUBLIC_DATE_12,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_12                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_12 = DP.PUBLIC_DATE_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_12 not like (\\'\\') \\n     ) PUBLIC_DATE_12 ON PUBLIC_DATE_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_13 AS PUBLIC_DATE_13,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_13                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_13 = DP.PUBLIC_DATE_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_13 not like (\\'\\') \\n     ) PUBLIC_DATE_13 ON PUBLIC_DATE_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_14 AS PUBLIC_DATE_14,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_14                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_14 = DP.PUBLIC_DATE_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_14 not like (\\'\\') \\n     ) PUBLIC_DATE_14 ON PUBLIC_DATE_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_15 AS PUBLIC_DATE_15,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_15                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_15 = DP.PUBLIC_DATE_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_15 not like (\\'\\') \\n     ) PUBLIC_DATE_15 ON PUBLIC_DATE_15.PROC_DIM_UID = PROCS.PROC_DIM_UID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData sort(asc(PROC_DIM_UID, false)) ~> sortedSourceData",
						"sortedSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTRPTS_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_TMF_GEO')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ln_rfs_pgmp",
								"type": "LinkedServiceReference"
							},
							"name": "TMFGEO"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dl_tgt_geo",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: true,",
						"     limit: 100,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'with\\n     -- This sub query gets the limits of the data that will be processed\\n     FIL as (\\n          select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.[ZAUX_ETL_EXCTN]\\n          where\\n               IS_CURR_IND = \\'Y\\'\\n     ),\\n     \\n     -- This subquery gets the JOB ID. Returns -1 is the value is unknown\\n     JOB as (\\n          select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\'          \\n     ),\\n     \\n     -- This subquery gets the SRC_SYS_ID. Returns -1 is the value is unknown\\n     SYS as (\\n          select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'TMF\\'               \\n     )\\n     \\n     \\n\\n\\nselect\\n     G.GEO_DIM_UID,\\n     G.IOT_RGN_CD/*,\\n     G.IOT_RGN_NM,\\n     G.IMT_RGN_CD,\\n     G.IMT_SRGN_NM,\\n     G.SRGN_CD,\\n     G.SRGNCTRY_CD,\\n     G.SRGNCTRY_NM,\\n     G.IOTSORT_NUM,\\n     G.GEO_CD,\\n     G.GEO_TYP_CD,\\n     G.GMR_RGN_CD,\\n     G.GMR_RGN_NM,\\n     G.IOT_SHORT_NM,\\n     G.IMT_SHORT_NM,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID*/\\nfrom\\n     PGMPDM.GEO_DIM_TMF G\\n     left join\\n     JOB\\n     on 1 = 1\\n     left join\\n     FIL\\n     on 1 = 1\\n     left join\\n     SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> TMFGEO",
						"TMFGEO sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ln_rfs_pgmp')]",
				"[concat(variables('workspaceId'), '/datasets/dl_tgt_geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText2",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Dataset')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DBXDH_DHT_PROJECT_SIV",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText2",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROJECT_KEY as integer,",
						"          PROJECT_VERSION as integer,",
						"          PROJECT_ID as string,",
						"          FINANCIAL_COUNTRY_CD as string,",
						"          LEDGER_CD as string,",
						"          OFFERING_COMPONENT_CD as string,",
						"          OPPORTUNITY_NUM as string,",
						"          PROJECT_DESC as string,",
						"          SIGNINGS_CD as string,",
						"          SIGNINGS_DESC as string,",
						"          BUSINESS_TYPE_CD as string,",
						"          BUSINESS_TYPE_DESC as string,",
						"          PROJECT_STATUS_CD as string,",
						"          PROJECT_STATUS_DESC as string,",
						"          PROJECT_CUSTOMER_NO as string,",
						"          PROJECT_CREATION_DATE as date,",
						"          ACCOUTNING_DIVISION as string,",
						"          RESPONSIBLE_SERV_OFFICE as string,",
						"          CURRENT_IND as string,",
						"          EXTRACT_DT as timestamp,",
						"          REC_START_DT as timestamp,",
						"          REC_END_DT as timestamp,",
						"          SOURCE_SYSTEM as string,",
						"          REC_CHECKSUM as string,",
						"          REC_STATUS as string,",
						"          IMG_LST_UPD_DT as timestamp,",
						"          IMG_CREATED_DT as timestamp,",
						"          DATA_IND as string,",
						"          ACTIVE_IN_SOURCE_IND as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_COMMITTED',",
						"     query: 'select * from DBXDH.DHT_PROJECT_SIV where PROJECT_ID = \\'EEI0001\\'',",
						"     format: 'query') ~> source1",
						"source1 select(mapColumn(",
						"          PROJECT_KEY,",
						"          PROJECT_VERSION,",
						"          PROJECT_ID,",
						"          FINANCIAL_COUNTRY_CD,",
						"          LEDGER_CD,",
						"          OFFERING_COMPONENT_CD,",
						"          OPPORTUNITY_NUM,",
						"          PROJECT_DESC,",
						"          SIGNINGS_CD,",
						"          SIGNINGS_DESC,",
						"          BUSINESS_TYPE_CD,",
						"          BUSINESS_TYPE_DESC,",
						"          PROJECT_STATUS_CD,",
						"          PROJECT_STATUS_DESC,",
						"          PROJECT_CUSTOMER_NO,",
						"          PROJECT_CREATION_DATE,",
						"          ACCOUTNING_DIVISION,",
						"          RESPONSIBLE_SERV_OFFICE,",
						"          CURRENT_IND,",
						"          EXTRACT_DT,",
						"          REC_START_DT,",
						"          REC_END_DT,",
						"          SOURCE_SYSTEM,",
						"          REC_CHECKSUM,",
						"          REC_STATUS,",
						"          IMG_LST_UPD_DT,",
						"          IMG_CREATED_DT,",
						"          DATA_IND,",
						"          ACTIVE_IN_SOURCE_IND",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     partitionFileNames:['proj_tgt_file_with_filter.csv'],",
						"     umask: 0777,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DBXDH_DHT_PROJECT_SIV')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow_copy1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText2",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     partitionFileNames:['sell_cycle_tgt_file.csv'],",
						"     umask: 0777,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Dataset')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Df_BAL0001_GEO_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:05-06-2022\nJob name: Df_BAL0001_GEO_DIM\nCreatedBy: Varaprasad\n\nThis job will pull all GEO_TMF and loading into PGMPDM.GEO_DIM table",
				"folder": {
					"name": "PGMP"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcgeo"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srtdtgtgeolkp"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdgeodimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtGeoDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtgeodata"
						},
						{
							"name": "split"
						},
						{
							"name": "rowstat"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "rowstatIns"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     G.GEO_DIM_UID,\\n     G.IOT_RGN_CD,\\n     G.IOT_RGN_NM,\\n     G.IMT_RGN_CD,\\n     G.IMT_SRGN_NM,\\n     G.SRGN_CD,\\n     G.SRGNCTRY_CD,\\n     G.SRGNCTRY_NM,\\n     G.IOTSORT_NUM,\\n     G.GEO_CD,\\n     G.GEO_TYP_CD,\\n     G.GMR_RGN_CD,\\n     G.GMR_RGN_NM,\\n     G.IOT_SHORT_NM,\\n     G.IMT_SHORT_NM,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID\\nfrom\\n     PGMPDM.GEO_DIM_TMF G\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\'     )JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.[ZAUX_ETL_EXCTN]\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'TMF\\')SYS\\n     on 1 = 1\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcgeo",
						"source(output(",
						"          LKP_GEO_DIM_UID as integer,",
						"          LKP_IOT_RGN_CD as string,",
						"          LKP_IOT_RGN_NM as string,",
						"          LKP_IMT_RGN_CD as string,",
						"          LKP_IMT_SRGN_NM as string,",
						"          LKP_SRGN_CD as string,",
						"          LKP_SRGNCTRY_CD as string,",
						"          LKP_SRGNCTRY_NM as string,",
						"          LKP_IOTSORT_NUM as integer,",
						"          LKP_GEO_CD as string,",
						"          LKP_GEO_TYP_CD as string,",
						"          LKP_GMR_RGN_CD as string,",
						"          LKP_GMR_RGN_NM as string,",
						"          LKP_IOT_SHORT_NM as string,",
						"          LKP_IMT_SHORT_NM as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n      GEO_DIM_UID  LKP_GEO_DIM_UID ,\\n     IOT_RGN_CD AS LKP_IOT_RGN_CD  ,\\n    IOT_RGN_NM as  LKP_IOT_RGN_NM  ,\\n     IMT_RGN_CD as LKP_IMT_RGN_CD  ,\\n     IMT_SRGN_NM as LKP_IMT_SRGN_NM  ,\\n     SRGN_CD as LKP_SRGN_CD  ,\\n     SRGNCTRY_CD as LKP_SRGNCTRY_CD  ,\\n     SRGNCTRY_NM as LKP_SRGNCTRY_NM  ,\\n     IOTSORT_NUM as LKP_IOTSORT_NUM   ,\\n     GEO_CD as LKP_GEO_CD  ,\\n     GEO_TYP_CD as LKP_GEO_TYP_CD  ,\\n     GMR_RGN_CD as LKP_GMR_RGN_CD  ,\\n     GMR_RGN_NM as LKP_GMR_RGN_NM  ,\\n     IOT_SHORT_NM as LKP_IOT_SHORT_NM  ,\\n     IMT_SHORT_NM as LKP_IMT_SHORT_NM  ,\\n     ETL_JOB_ID as LKP_ETL_JOB_ID  ,\\n     SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID  \\nfrom\\n     PGMPDM.GEO_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srtdtgtgeolkp",
						"srcgeo, srtdtgtgeodata join(GEO_DIM_UID == LKP_GEO_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"srtdtgtgeolkp sort(asc(LKP_GEO_DIM_UID, true)) ~> srtdtgtgeodata",
						"mergeddata split(GEO_DIM_UID==LKP_GEO_DIM_UID,",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"split@datatobeupdted derive(DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\") ~> rowstat",
						"rowstat select(mapColumn(",
						"          GEO_DIM_UID,",
						"          IOT_RGN_CD,",
						"          IOT_RGN_NM,",
						"          IMT_RGN_CD,",
						"          IMT_SRGN_NM,",
						"          SRGN_CD,",
						"          SRGNCTRY_CD,",
						"          SRGNCTRY_NM,",
						"          IOTSORT_NUM,",
						"          GEO_CD,",
						"          GEO_TYP_CD,",
						"          GMR_RGN_CD,",
						"          GMR_RGN_NM,",
						"          IOT_SHORT_NM,",
						"          IMT_SHORT_NM,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"split@datatobeinserted derive(DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\") ~> rowstatIns",
						"rowstatIns select(mapColumn(",
						"          GEO_DIM_UID,",
						"          IOT_RGN_CD,",
						"          IOT_RGN_NM,",
						"          IMT_RGN_CD,",
						"          IMT_SRGN_NM,",
						"          SRGN_CD,",
						"          SRGNCTRY_CD,",
						"          SRGNCTRY_NM,",
						"          IOTSORT_NUM,",
						"          GEO_CD,",
						"          GEO_TYP_CD,",
						"          GMR_RGN_CD,",
						"          GMR_RGN_NM,",
						"          IOT_SHORT_NM,",
						"          IMT_SHORT_NM,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'GEO_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['GEO_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          GEO_DIM_UID,",
						"          IOT_RGN_CD,",
						"          IOT_RGN_NM,",
						"          IMT_RGN_CD,",
						"          IMT_SRGN_NM,",
						"          SRGN_CD,",
						"          SRGNCTRY_CD,",
						"          SRGNCTRY_NM,",
						"          IOTSORT_NUM,",
						"          GEO_CD,",
						"          GEO_TYP_CD,",
						"          GMR_RGN_CD,",
						"          GMR_RGN_NM,",
						"          IOT_SHORT_NM,",
						"          IMT_SHORT_NM,",
						"          SRC_SYS_DIM_UID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID",
						"     )) ~> Tgtupdgeodimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'GEO_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          GEO_DIM_UID,",
						"          IOT_RGN_CD,",
						"          IOT_RGN_NM,",
						"          IMT_RGN_CD,",
						"          IMT_SRGN_NM,",
						"          SRGN_CD,",
						"          SRGNCTRY_CD,",
						"          SRGNCTRY_NM,",
						"          IOTSORT_NUM,",
						"          GEO_CD,",
						"          GEO_TYP_CD,",
						"          GMR_RGN_CD,",
						"          GMR_RGN_NM,",
						"          IOT_SHORT_NM,",
						"          IMT_SHORT_NM,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID",
						"     )) ~> InsTgtGeoDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Src_tgt_Geo_CDC')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "GetSrGeoTmf"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtGeoDim"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtUpsertGeoData"
						}
					],
					"transformations": [
						{
							"name": "changecapture"
						},
						{
							"name": "sortedlkpdata"
						},
						{
							"name": "srclkpgeodata"
						},
						{
							"name": "UpsertGeo"
						}
					],
					"scriptLines": [
						"source(output(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     G.GEO_DIM_UID,\\n     G.IOT_RGN_CD,\\n     G.IOT_RGN_NM,\\n     G.IMT_RGN_CD,\\n     G.IMT_SRGN_NM,\\n     G.SRGN_CD,\\n     G.SRGNCTRY_CD,\\n     G.SRGNCTRY_NM,\\n     G.IOTSORT_NUM,\\n     G.GEO_CD,\\n     G.GEO_TYP_CD,\\n     G.GMR_RGN_CD,\\n     G.GMR_RGN_NM,\\n     G.IOT_SHORT_NM,\\n     G.IMT_SHORT_NM,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID\\nfrom\\n     PGMPDM.GEO_DIM_TMF G\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\'     )JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.[ZAUX_ETL_EXCTN]\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'TMF\\')SYS\\n     on 1 = 1\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> GetSrGeoTmf",
						"source(output(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ETL_JOB_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     GEO_DIM_UID,\\n     IOT_RGN_CD,\\n     IOT_RGN_NM,\\n     IMT_RGN_CD,\\n     IMT_SRGN_NM,\\n     SRGN_CD,\\n     SRGNCTRY_CD,\\n     SRGNCTRY_NM,\\n     IOTSORT_NUM,\\n     GEO_CD,\\n     GEO_TYP_CD,\\n     GMR_RGN_CD,\\n     GMR_RGN_NM,\\n     IOT_SHORT_NM,\\n     IMT_SHORT_NM,\\n     ETL_JOB_ID,\\n     SRC_SYS_DIM_UID\\nfrom\\n     PGMPDM.GEO_DIM\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> TgtGeoDim",
						"sortedlkpdata, TgtGeoDim join(GetSrGeoTmf@GEO_DIM_UID == TgtGeoDim@GEO_DIM_UID,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> changecapture",
						"GetSrGeoTmf sort(asc(GEO_DIM_UID, true)) ~> sortedlkpdata",
						"TgtGeoDim sort(asc(GEO_DIM_UID, true)) ~> srclkpgeodata",
						"changecapture alterRow(upsertIf(GetSrGeoTmf@GEO_DIM_UID!=toInteger(null()))) ~> UpsertGeo",
						"UpsertGeo sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'GEO_DIM',",
						"     insertable: false,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: true,",
						"     keys:['GEO_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> TgtUpsertGeoData"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_job')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "output_ar",
								"type": "DatasetReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "CDC"
						},
						{
							"name": "sortedLookupData"
						},
						{
							"name": "selectNewData"
						},
						{
							"name": "upsertData"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ACCTRPTS_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ACCTRPTS_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     ACC.PROC_ID as ACCTRPTS_DIM_UID,\\n     ACC.PROJECT_ID as PRJCT_ID,\\n     cast(ACC.REMARKS as varchar(1024)) as ACCTRPTS_REM_TXT,\\n     ACC.CREATED_TS as SRC_CRETD_TMS,\\n     ACC.CREATED_USERID as SRC_CRETD_USER_ID,\\n     ACC.UPDATED_TS as SRC_UPDTD_TMS,\\n     ACC.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nfrom\\n     APPFUN.ACCTRPTS ACC\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on ACC.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on ACC.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'BALD0010_ACCTRPTS_DIM\\'          \\n     ) as JOB\\n     on \\'1\\' = \\'1\\'\\n     left join\\n     (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\'\\n     ) as FIL\\n     on \\'1\\' = \\'1\\'\\n     left join\\n     (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\'               \\n     ) as SYS\\n     on \\'1\\' = \\'1\\'\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          ACCTRPTS_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ACCTRPTS_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.ACCTRPTS_DIM_UID,\\n     S.PRJCT_ID,\\n     S.ACCTRPTS_REM_TXT,\\n     S.SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID\\nfrom\\n     PGMPDM.ACCTRPTS_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.ACCTRPTS_DIM_UID = ZDT.PROC_ID     \\n     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getSourceData, sortedLookupData join(getSourceData@ACCTRPTS_DIM_UID == getLookupData@ACCTRPTS_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> CDC",
						"getLookupData sort(asc(ACCTRPTS_DIM_UID, false)) ~> sortedLookupData",
						"CDC select(mapColumn(",
						"          ACCTRPTS_DIM_UID = getSourceData@ACCTRPTS_DIM_UID,",
						"          PRJCT_ID = getSourceData@PRJCT_ID,",
						"          ACCTRPTS_REM_TXT = getSourceData@ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS = getSourceData@SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID = getSourceData@SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS = getSourceData@SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID = getSourceData@SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID = getSourceData@ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID = getSourceData@SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ACCTRPTS_DIM_UID = getLookupData@ACCTRPTS_DIM_UID,",
						"          PRJCT_ID = getLookupData@PRJCT_ID,",
						"          ACCTRPTS_REM_TXT = getLookupData@ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS = getLookupData@SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID = getLookupData@SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS = getLookupData@SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID = getLookupData@SRC_UPDTD_USER_ID,",
						"          SRC_SYS_DIM_UID = getLookupData@SRC_SYS_DIM_UID,",
						"          ETL_JOB_ID = getLookupData@ETL_JOB_ID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectNewData",
						"selectNewData alterRow(upsertIf(ACCTRPTS_DIM_UID!=toInteger(null()))) ~> upsertData",
						"upsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     partitionBy('hash', 1)) ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]",
				"[concat(variables('workspaceId'), '/datasets/output_ar')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_lookup_df')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "pgmpdm_acctrpts_dim",
								"type": "DatasetReference"
							},
							"name": "pgmpdmAcctrptsDim"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_date_triggers",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxDateTriggers"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "account_reports_lookup",
								"type": "DatasetReference"
							},
							"name": "lookupAcctrpts"
						}
					],
					"transformations": [
						{
							"name": "accIJzdt"
						},
						{
							"name": "selectLookupCols"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ACCTRPTS_DIM_UID as string,",
						"          PRJCT_ID as string,",
						"          ACCTRPTS_REM_TXT as string,",
						"          SRC_CRETD_TMS as string,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as string,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as string,",
						"          ETL_JOB_ID as string,",
						"          ETL_EXCTN_ID as string,",
						"          DM_CRETD_TMS as string,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as string,",
						"          DM_UPDTD_USER_ID as string,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmAcctrptsDim",
						"source(output(",
						"          PROC_ID as string,",
						"          CRETD_DT as string,",
						"          RCVD_DT as string,",
						"          PRPSL_SENT_TO_CLNT_DT as string,",
						"          PRPSL_DSPSN_DT as string,",
						"          PRPSL_ACCPTD_DT as string,",
						"          IMPLMTN_READY_DT as string,",
						"          IMPLMTN_CLOSE_OUT_DT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxDateTriggers",
						"pgmpdmAcctrptsDim, pgmpdmZauxDateTriggers join(ACCTRPTS_DIM_UID == PROC_ID,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> accIJzdt",
						"accIJzdt select(mapColumn(",
						"          ACCTRPTS_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ROW_STAT_CD,",
						"          SRC_SYS_DIM_UID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          DM_CRETD_TMS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          ORIG_ORG",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectLookupCols",
						"selectLookupCols sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['lookupData.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> lookupAcctrpts"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_acctrpts_dim')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_date_triggers')]",
				"[concat(variables('workspaceId'), '/datasets/account_reports_lookup')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_source_df')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "appfun_acctrpts",
								"type": "DatasetReference"
							},
							"name": "appfunAcctrpts"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_date_triggers_src",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxDateTriggers"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_deld_proc",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxDeldProc"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_etl_exctn",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxEtlExctn"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_etl_jobs",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxEtlJobs"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_src_sys_dim",
								"type": "DatasetReference"
							},
							"name": "pgmpdmSrcSysDim"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "account_reports_source",
								"type": "DatasetReference"
							},
							"name": "sourceAcctrpts"
						}
					],
					"transformations": [
						{
							"name": "accIJzdt"
						},
						{
							"name": "accLJpdel"
						},
						{
							"name": "accLJjob"
						},
						{
							"name": "accLJfil"
						},
						{
							"name": "accLJsys"
						},
						{
							"name": "selectSourceCols"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_ID as string,",
						"          CNTRY as string,",
						"          PROJECT_ID as string,",
						"          DUE_DATE as string,",
						"          REVISD_DUE_DATE as string,",
						"          REMARKS as string,",
						"          ORIG_ORG as string,",
						"          CREATED_TS as string,",
						"          CREATED_USERID as string,",
						"          UPDATED_TS as string,",
						"          UPDATED_USERID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> appfunAcctrpts",
						"source(output(",
						"          PROC_ID as string,",
						"          CRETD_DT as string,",
						"          RCVD_DT as string,",
						"          PRPSL_SENT_TO_CLNT_DT as string,",
						"          PRPSL_DSPSN_DT as string,",
						"          PRPSL_ACCPTD_DT as string,",
						"          IMPLMTN_READY_DT as string,",
						"          IMPLMTN_CLOSE_OUT_DT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxDateTriggers",
						"source(output(",
						"          PROC_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxDeldProc",
						"source(output(",
						"          ETL_EXCTN_ID as string,",
						"          ETL_PARAM_START_TMS as string,",
						"          ETL_PARAM_END_TMS as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxEtlExctn",
						"source(output(",
						"          ETL_JOB_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxEtlJobs",
						"source(output(",
						"          SRC_SYS_DIM_UID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmSrcSysDim",
						"appfunAcctrpts, pgmpdmZauxDateTriggers join(appfunAcctrpts@PROC_ID == pgmpdmZauxDateTriggers@PROC_ID,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> accIJzdt",
						"accIJzdt, pgmpdmZauxDeldProc join(appfunAcctrpts@PROC_ID == pgmpdmZauxDeldProc@PROC_ID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> accLJpdel",
						"accLJpdel, pgmpdmZauxEtlJobs join(abs(1) == abs(1),",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'right')~> accLJjob",
						"accLJjob, pgmpdmZauxEtlExctn join(abs(1) == abs(1),",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'right')~> accLJfil",
						"accLJfil, pgmpdmSrcSysDim join(abs(1) == abs(1),",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'right')~> accLJsys",
						"accLJsys select(mapColumn(",
						"          PROC_ID = appfunAcctrpts@PROC_ID,",
						"          PROJECT_ID,",
						"          REMARKS,",
						"          CREATED_TS,",
						"          CREATED_USERID,",
						"          UPDATED_TS,",
						"          UPDATED_USERID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectSourceCols",
						"selectSourceCols sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sourceAcctrpts"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/appfun_acctrpts')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_date_triggers_src')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_deld_proc')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_etl_exctn')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_etl_jobs')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_src_sys_dim')]",
				"[concat(variables('workspaceId'), '/datasets/account_reports_source')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_scdType2_Project_Dimension')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "backup"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "DimensionTable"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "select1"
						},
						{
							"name": "select2"
						},
						{
							"name": "select3"
						},
						{
							"name": "select4"
						},
						{
							"name": "select5"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash_existing = md5(byName($NaturalKey)),",
						"          columns_hash_existing = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashExisting",
						"derivedAddHashInput, derivedAddHashExisting join(nk_hash == nk_hash_existing,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(nk_hash_existing,nk_hash) && equals(columns_hash_existing,columns_hash),",
						"     equals(nk_hash_existing,nk_hash) && iifNull((columns_hash_existing),'NULL',(columns_hash_existing)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(nk_hash_existing),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          nk_hash = nk_hash_existing,",
						"          columns_hash = columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"NoChangeRecords@ChangedRecordsForUpdate select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          nk_hash_existing,",
						"          columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"NoChangeRecords@NewrecodsForInsert select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          nk_hash_existing,",
						"          columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select3",
						"NoChangeRecords@NotActiveInSource select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          nk_hash_existing,",
						"          columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select4",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          nk_hash_existing,",
						"          columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@NoChangeRecords sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> DimensionTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_scdType2_Project_Dimension_28thApril')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "backup"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DBXDH_DHT_PROJECT_SIV",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "DBXDH_DHT_PROJECT_SIV",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "DBXDH_DHT_PROJECT_SIV",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "select7"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     UpdateKey4Changes as string ('PROJECT_KEY,REC_START_DT'),",
						"     SurrogateKey as string ('PROJECT_KEY'),",
						"     Updt_Key as string ('REC_START_DT')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: 'SELECT A.*, MAX(A.PROJECT_KEY) OVER () AS MAX_KEY\\n FROM DBXDH.DHT_PROJECT_SIV A',",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U') ~> derivedChangedRows4Update",
						"select7 alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(CURRENT_IND = 'Y') ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = currentTimestamp(),",
						"          REC_START_DT = currentDate(),",
						"          REC_END_DT = '9999-12-31',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(Key1,NewKey)) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = currentTimestamp(),",
						"          REC_START_DT = currentDate(),",
						"          REC_END_DT = '9999-12-31',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'U',",
						"          ACTIVE_IN_SOURCE_IND = 'Y') ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          nk_hash,",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          existing_nk_hash,",
						"          existing_columns_hash,",
						"          CURRENT_IND,",
						"          EXTRACT_DT,",
						"          REC_START_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_REC_START_DT'),",
						"               'REC_START_DT' = $$),",
						"          each(match(true()))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          PROJECT_KEY as integer,",
						"          PROJECT_VERSION as integer,",
						"          PROJECT_ID as string,",
						"          FINANCIAL_COUNTRY_CD as string,",
						"          LEDGER_CD as string,",
						"          OFFERING_COMPONENT_CD as string,",
						"          OPPORTUNITY_NUM as string,",
						"          PROJECT_DESC as string,",
						"          SIGNINGS_CD as string,",
						"          SIGNINGS_DESC as string,",
						"          BUSINESS_TYPE_CD as string,",
						"          BUSINESS_TYPE_DESC as string,",
						"          PROJECT_STATUS_CD as string,",
						"          PROJECT_STATUS_DESC as string,",
						"          PROJECT_CUSTOMER_NO as string,",
						"          PROJECT_CREATION_DATE as date,",
						"          ACCOUTNING_DIVISION as string,",
						"          RESPONSIBLE_SERV_OFFICE as string,",
						"          CURRENT_IND as string,",
						"          EXTRACT_DT as timestamp,",
						"          REC_START_DT as timestamp,",
						"          REC_END_DT as timestamp,",
						"          SOURCE_SYSTEM as string,",
						"          REC_CHECKSUM as string,",
						"          REC_STATUS as string,",
						"          IMG_LST_UPD_DT as timestamp,",
						"          IMG_CREATED_DT as timestamp,",
						"          DATA_IND as string,",
						"          ACTIVE_IN_SOURCE_IND as string",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:[($SurrogateKey),('REC_START_DT')],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          PROJECT_KEY as integer,",
						"          PROJECT_VERSION as integer,",
						"          PROJECT_ID as string,",
						"          FINANCIAL_COUNTRY_CD as string,",
						"          LEDGER_CD as string,",
						"          OFFERING_COMPONENT_CD as string,",
						"          OPPORTUNITY_NUM as string,",
						"          PROJECT_DESC as string,",
						"          SIGNINGS_CD as string,",
						"          SIGNINGS_DESC as string,",
						"          BUSINESS_TYPE_CD as string,",
						"          BUSINESS_TYPE_DESC as string,",
						"          PROJECT_STATUS_CD as string,",
						"          PROJECT_STATUS_DESC as string,",
						"          PROJECT_CUSTOMER_NO as string,",
						"          PROJECT_CREATION_DATE as date,",
						"          ACCOUTNING_DIVISION as string,",
						"          RESPONSIBLE_SERV_OFFICE as string,",
						"          CURRENT_IND as string,",
						"          EXTRACT_DT as timestamp,",
						"          REC_START_DT as timestamp,",
						"          REC_END_DT as timestamp,",
						"          SOURCE_SYSTEM as string,",
						"          REC_CHECKSUM as string,",
						"          REC_STATUS as string,",
						"          IMG_LST_UPD_DT as timestamp,",
						"          IMG_CREATED_DT as timestamp,",
						"          DATA_IND as string,",
						"          ACTIVE_IN_SOURCE_IND as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          PROJECT_KEY as integer,",
						"          PROJECT_VERSION as integer,",
						"          PROJECT_ID as string,",
						"          FINANCIAL_COUNTRY_CD as string,",
						"          LEDGER_CD as string,",
						"          OFFERING_COMPONENT_CD as string,",
						"          OPPORTUNITY_NUM as string,",
						"          PROJECT_DESC as string,",
						"          SIGNINGS_CD as string,",
						"          SIGNINGS_DESC as string,",
						"          BUSINESS_TYPE_CD as string,",
						"          BUSINESS_TYPE_DESC as string,",
						"          PROJECT_STATUS_CD as string,",
						"          PROJECT_STATUS_DESC as string,",
						"          PROJECT_CUSTOMER_NO as string,",
						"          PROJECT_CREATION_DATE as date,",
						"          ACCOUTNING_DIVISION as string,",
						"          RESPONSIBLE_SERV_OFFICE as string,",
						"          CURRENT_IND as string,",
						"          EXTRACT_DT as timestamp,",
						"          REC_START_DT as timestamp,",
						"          REC_END_DT as timestamp,",
						"          SOURCE_SYSTEM as string,",
						"          REC_CHECKSUM as string,",
						"          REC_STATUS as string,",
						"          IMG_LST_UPD_DT as timestamp,",
						"          IMG_CREATED_DT as timestamp,",
						"          DATA_IND as string,",
						"          ACTIVE_IN_SOURCE_IND as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]",
				"[concat(variables('workspaceId'), '/datasets/DBXDH_DHT_PROJECT_SIV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_scdType2_Project_Dimension_Latest')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "backup"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "selectChangedRecords4Update"
						},
						{
							"name": "select7"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     SurrogateKey as string ('PROJECT_KEY'),",
						"     DimTableName as string ('DHT_PROJECT_SIV')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: ('SELECT * FROM DBXDH.' + $DimTableName + ' WHERE CURRENT_IND=\\'' + 'Y' + '\\''),",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = toString(byName('REC_CHECKSUM'))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY')) +'VERSION')) +1) ~> derivedChangedRows4Update",
						"selectChangedRecords4Update alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(ACTIVE_IN_SOURCE_IND = 'Y',",
						"          IMG_LST_UPD_DT = currentTimestamp()) ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(iifNull(Key1,0),NewKey),",
						"          VERSION = 1) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'I',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION'))+1) ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          CURRENT_IND,",
						"          REC_END_DT,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectChangedRecords4Update",
						"derivedColumn1 select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          ACTIVE_IN_SOURCE_IND,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"select7 alterRow(updateIf(true())) ~> alterRow1",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 3,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 4,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_scdType2_Project_Dimension_Latest_2ndMay')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "backup"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "selectChangedRecords4Update"
						},
						{
							"name": "select7"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     SurrogateKey as string ('PROJECT_KEY'),",
						"     DimTableName as string ('DHT_PROJECT_SIV')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: ('SELECT * FROM DBXDH.' + $DimTableName + ' WHERE CURRENT_IND=\\'' + 'Y' + '\\''),",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = toString(byName('REC_CHECKSUM'))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY')) +'VERSION')) +1) ~> derivedChangedRows4Update",
						"selectChangedRecords4Update alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(ACTIVE_IN_SOURCE_IND = 'Y',",
						"          IMG_LST_UPD_DT = currentTimestamp()) ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(iifNull(Key1,0),NewKey),",
						"          VERSION = 1) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'I',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION'))+1) ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          CURRENT_IND,",
						"          REC_END_DT,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectChangedRecords4Update",
						"derivedColumn1 select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          ACTIVE_IN_SOURCE_IND,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"select7 alterRow(updateIf(true())) ~> alterRow1",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 3,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     preSQLs: ([$SurrogateKey,'REC_CHECKSUM']),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 4,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/df_SELL_CYCLE')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DBXDH_DHTS_SELL_CYCLE",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> source1",
						"source1 select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(insertIf(true())) ~> alterRow1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Dataset')]",
				"[concat(variables('workspaceId'), '/datasets/DBXDH_DHTS_SELL_CYCLE')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/risk_df')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT \\n    C.CNTRCT_DIM_UID,\\n    C.CNTRCT_NM AS CONTRACT,\\n    HFCT.PROC_DIM_UID AS UNIQUE_ID,\\n    PROC.CLNT_REF_NUM AS CLIENT_REFERENCE,\\n    PROC.PROC_DSPLY_ID AS DISPLAY_ID,\\n    ZACNTRCT.GLBL_BUY_GRP_ID,\\n    PARENT_PROC.PROC_DIM_UID AS PARENT_UNIQUE_ID,\\n    PARENT_PROC.PROC_DSPLY_ID AS PARENT_DISPLAY_ID,\\n    PARENT_PROC.TITLE_TXT AS PARENT_TITLE,\\n    PROC.TITLE_TXT AS PROC_TITLE,\\n    INITWDD.WKFLW_DEF_ID,\\n    INITWDD.DESC AS INIT_WKFLW_DEF_ID_DESCR,\\n    PROC.PROC_DESC AS PROC_DESCRIPTION,\\n    WKFLW.WKFLW_DEF_ID AS CURRENT_WORKFLOW,\\n    WKFLW_STEP.WKFLW_STEP_DEF_ID AS CURRENT_WKFLW_STEP,\\n    WKFLW_STEP.WKFLW_STEP_SEQ_NUM AS CURRENT_STEP_SEQ,\\n    WKFLW_STEP.WKFLW_STEP_DESC AS STEP_DESCRIPTION,\\n    ST.STATE_TITLE_TXT STATE_DESC,\\n    STATUS.STAT_IBM_DESC AS STATUS_DESC,\\n    decrypt_char(ASSGNTO.CONCAT_NM_LOGIN,MIS_REP_REF.MIS_REP_REF_CD) AS ASSIGNED_TO,\\n    AUDIENCE.ADNC_TXT AS AUDIENCE,\\n    P.PRIORITY_TXT,\\n    COND.COND_TXT AS CONDITION,\\n    COALESCE(RGN.REGION,GEO.KYNDRYL_RGN) AS REGION,\\n    decrypt_char(PROC.RQSTR_TXT,MIS_REP_REF.MIS_REP_REF_CD) AS REQUESTOR,\\n    decrypt_char(CREATOR.CONCAT_NM_LOGIN,MIS_REP_REF.MIS_REP_REF_CD) AS CREATED_BY,\\n    HFCT.SRC_CRETD_TMS AS CREATED_DATE,\\n    HFCT.SRC_UPDTD_TMS AS LAST_UPDATED,\\n    FCT.RISK_RSPNS_DUE_DT AS RISK_RESPONSE_DUE_DATE,\\n    FCT.RVSD_RISK_RSPNS_DUE_DT AS REVISED_RISK_RESPONSE_DUE_DATE,\\n    DAYS(NVL(FCT.RVSD_RISK_RSPNS_DUE_DT,FCT.RISK_RSPNS_DUE_DT)) - DAYS((CURRENT_TIMESTAMP - CURRENT_TIMEZONE) + AF.TZ HOURS) AS DUE_IN_DAY_CNT,\\n    CASE WHEN DAYS(NVL(FCT.RVSD_RISK_RSPNS_DUE_DT,FCT.RISK_RSPNS_DUE_DT)) - DAYS((CURRENT_TIMESTAMP - CURRENT_TIMEZONE) + AF.TZ HOURS) < 0 THEN \\'Y\\' ELSE \\'N\\' END AS OVERDUE,\\n    HFCT.CMPLTD_DT AS COMPLETION_DATE,\\n    PROC.CMPLTD_RSN_TXT AS COMPLETION_REASON,\\n    GEO.SRGNCTRY_NM AS COUNTRY,\\n    RISKSRC.RISK_SRC_TXT AS RISK_SOURCE,\\n    RISK.RISK_REM_TXT AS REMARKS,\\n    RISK.ORGNTG_ORGNZN_TXT AS ORIGINATING_ORG,\\n    decrypt_char(RISK.RISK_OWNR_TXT,MIS_REP_REF.MIS_REP_REF_CD) AS RISK_OWNER,\\n    FCT.PRBBLTY_NUM AS PROBABILITY,\\n    CASE \\n        WHEN FCT.PROBABILITY_SMPL =  \\'Z\\' THEN \\'10\\' \\n        WHEN FCT.PROBABILITY_SMPL =  \\'D\\' THEN \\'25\\' \\n        WHEN FCT.PROBABILITY_SMPL =  \\'C\\' THEN \\'50\\' \\n        WHEN FCT.PROBABILITY_SMPL =  \\'S\\' THEN \\'75\\' \\n    END AS PROBABILITY_SMPL ,\\n    FCT.IMPCT_CD AS IMPACT,\\n    RISK.MTGTN_RSPNS_PLAN_TXT AS RESPONSE_PLAN,\\n    CRNCY.CRNCY_NM AS LOCAL_CURRENCY,\\n    FCT.IMPCT_AMT AS IMPACT_AMOUNT_LOCAL_CURRENCY,\\n    RISK.GS_RISK_ID ,\\n    CASE \\n        WHEN RISK.BUS_CNTRL_RISK_IND=\\'Y\\' THEN \\'YES\\' \\n        WHEN RISK.BUS_CNTRL_RISK_IND=\\'N\\' THEN \\'NO\\' \\n    END AS BUSINESS_CONTROLS_RISK,\\n    RISK.WWRADB_REF_TXT AS WWBCIT_REFERENCE,\\n    FCT.RISK_ANLYSS_DUE_DT,\\n    FCT.RVSD_RISK_ANLYSS_DUE_DT,\\n    CASE \\n        WHEN RISK.RISK_RSPNS_TYPE_CD = \\'ACC\\' THEN \\'Accept/Retain\\' \\n        WHEN RISK.RISK_RSPNS_TYPE_CD = \\'AVO\\' THEN \\'Avoid\\' \\n        WHEN RISK.RISK_RSPNS_TYPE_CD = \\'CON\\' THEN \\'Contain/Reduce\\' \\n        WHEN RISK.RISK_RSPNS_TYPE_CD = \\'INS\\' THEN \\'Use Insurance\\'  \\n        WHEN RISK.RISK_RSPNS_TYPE_CD = \\'XFR\\' THEN \\'Transfer\\' \\n        WHEN RISK.RISK_RSPNS_TYPE_CD = \\'RES\\' THEN \\'Use Risk Reserve\\' \\n        ELSE RISK.RISK_RSPNS_TYPE_CD \\n    END AS RISK_RSPNS_TYPE_CD,\\n    RISK.RISK_OCCURRED_CD,\\n    RISK.RISK_CLOSE_RSN_TXT,\\n    SRC.SRC_SYS_CD,\\n    \\'\\' AS LEGACY_UNIQUE_ID,\\n    \\'\\' AS LEGACY_DISPLAYED_ID,\\n    TRIM(INOTES.NOTES_DESC) AS LATEST_IBM_ONLY_NOTE,\\n    TRIM(PNOTES.NOTES_DESC) AS LATEST_IBM_AND_CLIENT_NOTE,\\n    CTYPE.CNTRCT_TYPE_DESC,\\n    PTYPE.PROC_TYPE_ID,\\n    PTYPE.PROC_TYPE_DESC,\\n    ONHOLD.ON_HOLD_CD,\\n    HFCT.WITHDRWN_DIM_UID,\\n    HFCT.WITHDRWN_DT,\\n    HFCT.ORGNZN_DIM_UID,\\n    DATAIBM.IBM_ONLY_TEXT_1,\\n    DATAIBM.IBM_ONLY_TEXT_2,\\n    DATAIBM.IBM_ONLY_TEXT_3,\\n    DATAIBM.IBM_ONLY_TEXT_4,\\n    DATAIBM.IBM_ONLY_TEXT_5,\\n    DATAIBM.IBM_ONLY_TEXT_6,\\n    DATAIBM.IBM_ONLY_DATE_1,\\n    DATAIBM.IBM_ONLY_DATE_2,\\n    DATAIBM.IBM_ONLY_DATE_3,\\n    DATAIBM.IBM_ONLY_DATE_4,\\n    DATAIBM.IBM_ONLY_DATE_5,\\n    DATAIBM.IBM_ONLY_DATE_6,\\n    DATAPBLIC.PUBLIC_TEXT_1,\\n    DATAPBLIC.PUBLIC_TEXT_2,\\n    DATAPBLIC.PUBLIC_TEXT_3,\\n    DATAPBLIC.PUBLIC_TEXT_4,\\n    DATAPBLIC.PUBLIC_TEXT_5,\\n    DATAPBLIC.PUBLIC_TEXT_6,\\n    DATAPBLIC.PUBLIC_DATE_1,\\n    DATAPBLIC.PUBLIC_DATE_2,\\n    DATAPBLIC.PUBLIC_DATE_3,\\n    DATAPBLIC.PUBLIC_DATE_4,\\n    DATAPBLIC.PUBLIC_DATE_5,\\n    DATAPBLIC.PUBLIC_DATE_6,\\n    SOO.STAT_OF_ORGNZN_DESC AS STATE_OF_ORGANIZATION,\\n    SCTR.SCTR_NM,\\n    ZACNTRCT.CLIENT_UNIT_NM AS CLIENT_UNIT\\n\\nFROM \\n\\n    PGMPDM.RISK_DIM RISK\\n    INNER JOIN PGMPDM.PROC_HEADR_FCT HFCT ON HFCT.PROC_DIM_UID = RISK.RISK_DIM_UID AND HFCT.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.PROC_TYPE_DIM PTYPE ON PTYPE.PROC_TYPE_DIM_UID = HFCT.PROC_TYPE_DIM_UID AND PTYPE.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.PROC_DIM PROC ON PROC.PROC_DIM_UID = HFCT.PROC_DIM_UID AND PROC.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROC.CNTRCT_DIM_UID AND C.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.CNTRCT_TYPE_DIM CTYPE ON CTYPE.CNTRCT_TYPE_DIM_UID = C.CNTRCT_TYPE_DIM_UID AND CTYPE.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.WKFLW_DEF_DIM INITWDD ON INITWDD.WKFLW_DEF_DIM_UID = HFCT.INIT_WKFLW_DEF_DIM_UID AND INITWDD.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.WKFLW_DEF_DIM WKFLW ON WKFLW.WKFLW_DEF_DIM_UID = HFCT.WKFLW_DEF_DIM_UID AND WKFLW.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.WKFLW_STEP_DEF_DIM WKFLW_STEP ON WKFLW_STEP.WKFLW_STEP_DEF_DIM_UID = HFCT.WKFLW_STEP_DEF_DIM_UID AND WKFLW_STEP.ROW_STAT_CD <> \\'D\\' \\n    INNER JOIN PGMPDM.STATE_DIM ST ON ST.STATE_DIM_UID = HFCT.STATE_DIM_UID AND ST.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.PROC_STEP_DAT_DIM PSD ON PROC.PROC_DIM_UID = PSD.PROC_DIM_UID AND PROC.CURR_PROC_STEP_DAT_DIM_UID = PSD.PROC_STEP_DAT_DIM_UID AND PSD.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.GEO_DIM GEO ON GEO.GEO_DIM_UID = HFCT.GEO_DIM_UID AND GEO.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.MISC_FCT FCT ON FCT.PROC_HEADR_FCT_UID = HFCT.PROC_HEADR_FCT_UID AND FCT.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.SRC_SYS_DIM SRC ON SRC.SRC_SYS_DIM_UID = HFCT.SRC_SYS_DIM_UID AND SRC.ROW_STAT_CD <> \\'D\\'\\n    INNER JOIN PGMPDM.SCTR_DIM SCTR ON SCTR.SCTR_DIM_UID = C.SCTR_DIM_UID AND SCTR.ROW_STAT_CD <> \\'D\\'\\n    LEFT OUTER JOIN PGMPDM.STAT_DIM STATUS ON STATUS.STAT_DIM_UID = HFCT.STAT_DIM_UID AND STATUS.ROW_STAT_CD <> \\'D\\'\\n    LEFT OUTER JOIN PGMPDM.MISC_REP_REF MIS_REP_REF ON MIS_REP_REF_UID = 3\\n    LEFT OUTER JOIN PGMPDM.USER_DIM ASSGNTO ON ASSGNTO.USER_ID = PSD.ASSGN_TO_NUM AND ASSGNTO.ROW_STAT_CD <> \\'D\\'\\n    LEFT OUTER JOIN PGMPDM.ADNC_DIM AUDIENCE ON AUDIENCE.ADNC_CD = PROC.ADNC_CD\\n    LEFT OUTER JOIN PGMPDM.PRIORITY_DIM P ON P.PRIORITY_NUM = PROC.PRIORTY_NUM\\n    LEFT OUTER JOIN PGMPDM.COND_DIM COND ON COND.COND_CD = PROC.CNDTN_CD\\n    LEFT OUTER JOIN APPFUN.PROC_REGION_V RGN ON RGN.PROC_ID = PROC.PROC_DIM_UID AND RGN.INTERNAL_VAL <> \\'NONE\\'\\n    LEFT OUTER JOIN PGMPDM.USER_DIM CREATOR ON CREATOR.USER_DIM_UID = HFCT.CRETD_BY_USER_DIM_UID AND CREATOR.ROW_STAT_CD <> \\'D\\'\\n    LEFT OUTER JOIN PGMPDM.NOTES_DIM INOTES ON INOTES.PROC_DIM_UID = PROC.PROC_DIM_UID AND INOTES.ROW_STAT_CD <> \\'D\\' AND INOTES.NOTE_TYPE_CD = \\'I\\' AND INOTES.LATEST_NOTE_IND = \\'Y\\'\\n    LEFT OUTER JOIN PGMPDM.NOTES_DIM PNOTES ON PNOTES.PROC_DIM_UID = PROC.PROC_DIM_UID AND PNOTES.ROW_STAT_CD <> \\'D\\' AND PNOTES.NOTE_TYPE_CD = \\'P\\' AND PNOTES.LATEST_NOTE_IND = \\'Y\\'\\n    LEFT OUTER JOIN PGMPDM.ON_HOLD_DIM ONHOLD ON ONHOLD.ON_HOLD_DIM_UID = HFCT.ON_HOLD_DIM_UID AND ONHOLD.ROW_STAT_CD <> \\'D\\'\\n    LEFT OUTER JOIN PGMPDM.ON_HOLD_RSN_DIM HOLDRSN ON HOLDRSN.ON_HOLD_RSN_ID = PROC.ON_HOLD_RSN_ID AND HOLDRSN.ROW_STAT_CD <> \\'D\\'\\n    LEFT OUTER JOIN PGMPDM.CRNCY_DIM CRNCY ON CRNCY.CRNCY_DIM_UID = FCT.LOCAL_CRNCY_DIM_UID AND CRNCY.ROW_STAT_CD <> \\'D\\'\\n    LEFT OUTER JOIN PGMPDM.PROC_DIM PARENT_PROC ON PARENT_PROC.PROC_DIM_UID = PROC.PARNT_PROC_ID AND PARENT_PROC.ROW_STAT_CD <> \\'D\\'\\n    LEFT OUTER JOIN PGMPDM.RISK_SRC_DIM RISKSRC ON RISKSRC.RISK_SRC_DIM_CD = RISK.RISK_SRC_IND AND RISKSRC.ROW_STAT_CD <> \\'D\\'\\n    LEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DATAPBLIC ON DATAPBLIC.PROC_DIM_UID = PROC.PROC_DIM_UID\\n    LEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DATAIBM ON DATAIBM.PROC_DIM_UID = PROC.PROC_DIM_UID\\n    LEFT OUTER JOIN PGMPDM.ORG_CNTRCT_MAP_DIM ORG_MAP ON ORG_MAP.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID\\n    LEFT OUTER JOIN PGMPDM.ORGNZN_DIM ORG ON ORG.ORGNZN_DIM_UID = ORG_MAP.ORGNZN_DIM_UID\\n    LEFT OUTER JOIN PGMPDM.STAT_OF_ORGNZN_DIM SOO ON SOO.STAT_OF_ORGNZN_DIM_UID = ORG.STAT_OF_ORGNZN_DIM_UID\\n    LEFT OUTER JOIN PGMPDM.CNTRCT_TZ_V AF ON C.CNTRCT_DIM_UID = AF.CNTRCT_DIM_UID\\n    LEFT OUTER JOIN PGMPDM.ZAUX_CNTRCT_GLBL_BUY_GRP_MAP ZACNTRCT ON ZACNTRCT.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID \\n\\nWHERE \\n    RISK.ROW_STAT_CD <> \\'D\\' \\n    AND PTYPE.PROC_TYPE_ID IN (\\'RISK\\', \\'RISKNEW\\') \\n    AND PROC.ADNC_ACCSS_CD <> \\'C\\' \\n    AND HFCT.DELD_DT IS NULL \\n    AND SRC.SRC_SYS_CD = \\'PGMP\\'\\n\\n\\n\\n\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'RISK_RPT',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     recreate: true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scdType2_Generict_Dimension_Load')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4SoftDeletedRows"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "selectChangedRecords4Update"
						},
						{
							"name": "select7"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID1'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     SurrogateKey as string ('CUSTOMER'),",
						"     DimTableName as string ('TableName1')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: ('SELECT * FROM DBXDH.' + $DimTableName + ' WHERE CURRENT_IND=\\'' + 'Y' + '\\''),",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = toString(byName('REC_CHECKSUM'))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY')) +'VERSION')) +1) ~> derivedChangedRows4Update",
						"selectChangedRecords4Update alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(ACTIVE_IN_SOURCE_IND = 'N',",
						"          IMG_LST_UPD_DT = currentTimestamp()) ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(iifNull(Key1,0),NewKey),",
						"          VERSION = 1) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'I',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION'))+1) ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          {'U'} = REC_STATUS,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          CURRENT_IND,",
						"          REC_END_DT,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_'+$NaturalKey),",
						"               $NaturalKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectChangedRecords4Update",
						"derivedColumn1 select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          ACTIVE_IN_SOURCE_IND,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"select7 alterRow(updateIf(true())) ~> alterRow1",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 3,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 4,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4SoftDeletedRows"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scdType2_Generict_Dimension_Load_WOVersion')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4SoftDeletedRows"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "selectChangedRecords4Update"
						},
						{
							"name": "select7"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID1'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     DimTableName as string ('TableName1'),",
						"     UpdateKey as string ('Key'),",
						"     SurrogateKey as string ('CUSTOMER_KEY')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: ('SELECT * FROM DBXDH.' + $DimTableName + ' WHERE CURRENT_IND=\\'' + 'Y' + '\\''),",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = toString(byName('REC_CHECKSUM'))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U') ~> derivedChangedRows4Update",
						"selectChangedRecords4Update alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(ACTIVE_IN_SOURCE_IND = 'N',",
						"          IMG_LST_UPD_DT = currentTimestamp()) ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(iifNull(Key1,0),NewKey),",
						"          VERSION = 1) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'I',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          VERSION = toInteger(byName('existing_VERSION'))+1) ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          VERSION,",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          {'U'} = REC_STATUS,",
						"          VERSION,",
						"          each(match(and(instr(name,'existing')!=0,instr(name,'KEY')!=0)),",
						"               substring($$,10) = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          CURRENT_IND,",
						"          REC_END_DT,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_'+$NaturalKey),",
						"               $NaturalKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectChangedRecords4Update",
						"derivedColumn1 select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          ACTIVE_IN_SOURCE_IND,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"select7 alterRow(updateIf(true())) ~> alterRow1",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:[($UpdateKey),'REC_CHECKSUM'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 3,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$UpdateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 4,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4SoftDeletedRows"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateExternalDeltaTable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseDeltaFormatCustomer') \n\tCREATE EXTERNAL FILE FORMAT [SynapseDeltaFormatCustomer] \n\tWITH ( FORMAT_TYPE = DELTA)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE customer_V3_dimension (\n\t[CUSTOMER_KEY] nvarchar(4000),\n\t[VERSION] nvarchar(4000),\n\t[CUSTOMER_NO] nvarchar(4000),\n\t[FINANCIAL_COUNTRY_CD] nvarchar(4000),\n\t[GBG_ID] nvarchar(4000),\n\t[CUSTOMER_NAME] nvarchar(4000),\n\t[CURRENT_IND] nvarchar(4000),\n\t[EXTRACT_DT] nvarchar(4000),\n\t[REC_START_DT] nvarchar(4000),\n\t[REC_END_DT] nvarchar(4000),\n\t[SOURCE_SYSTEM] nvarchar(4000),\n\t[REC_CHECKSUM] nvarchar(4000),\n\t[REC_STATUS] nvarchar(4000),\n\t[IMG_LST_UPD_DT] nvarchar(4000),\n\t[IMG_CREATED_DT] nvarchar(4000),\n\t[DATA_IND] nvarchar(4000),\n\t[ACTIVE_IN_SOURCE_IND] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'data/customer/customer_dimension2/',\n\tDATA_SOURCE = [deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseDeltaFormatCustomer]\n\t)\nGO\n\n\nSELECT count(*) FROM dbo.customer_V3_dimension\nGO\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "kyn",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Query data with SQL')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\nSELECT * FROM DBXDH.DHT_CUSTOMER\n\nINSERT INTO DBXDH.DHT_CUSTOMER\n(CUSTOMER_KEY, VERSION, CUSTOMER_NO, FINANCIAL_COUNTRY_CD, GBG_ID, CUSTOMER_NAME, CURRENT_IND, EXTRACT_DT, REC_START_DT, REC_END_DT, SOURCE_SYSTEM, REC_CHECKSUM, REC_STATUS, IMG_LST_UPD_DT, IMG_CREATED_DT, DATA_IND, ACTIVE_IN_SOURCE_IND)\nVALUES(2, 1, '4514136', '897', 'GB302S62', 'SETERUS INC', 'Y', '2022-05-16 10:03:26.927' , '2022-05-16 10:03:26.927', '9999-12-31 00:00:00.000', 'BMSIW', '55cccdEDS643aaac65deb1388ada8643f5', 'I', '2022-05-16 10:03:26.927', '2022-05-16 10:03:26.927', 'LG', 'Y');\n\n\n--select CURRENT_TIMESTAMP\n--update\nupdate DBXDH.DHT_CUSTOMER\nset FINANCIAL_COUNTRY_CD='902'\nWHERE CUSTOMER_NO='9549530'\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "dsqlpoolKyn001494DevEtlHubEUS001",
						"poolName": "dsqlpoolKyn001494DevEtlHubEUS001"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/year=2018/month=*/*.parquet',\n        FORMAT='PARQUET'\n    ) AS [weather];\n\nhttps://adls4fsoetlhubdevuseast.blob.core.windows.net/project/project_siv_data.csv\n\nselect top 10 *\nfrom openrowset(\n    bulk 'https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/project_data.csv',\n    format = 'csv',\n    parser_version = '2.0',\n    firstrow = 2 ) as rows\n;\n\nSELECT * FROM DBXDH.DHT_PROJECT_SIV\n\n\n\n\nSELECT\n\nTOP 100 *\n\nFROM\n\nOPENROWSET(\n\nBULK 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/proj_tgt_file.csv',\n\nFORMAT = 'CSV',\n\nPARSER_VERSION = '2.0'\n\n) AS [result]\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 10')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "update etlhubConfirmed.dbo.sell_cycle set SIEBEL_SALES_STAGE_NAME = \"1200 newly added\"\nwhere SIEBEL_SALES_STAGE_CODE = \"1000\"",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 11')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": " update [etlhubconfirmed].[dbo].[dht_sell_cycle]\n set [SIEBEL_SALES_STAGE_NAME] = \"\"\n where [SIEBEL_SALES_STAGE_CODE] = \"1000\"",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 12')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE customer_dim_temp (\n\t[CUSTOMER_KEY] nvarchar(4000),\n\t[VERSION] nvarchar(4000),\n\t[CUSTOMER_NO] nvarchar(4000),\n\t[FINANCIAL_COUNTRY_CD] nvarchar(4000),\n\t[GBG_ID] nvarchar(4000),\n\t[CUSTOMER_NAME] nvarchar(4000),\n\t[CURRENT_IND] nvarchar(4000),\n\t[EXTRACT_DT] nvarchar(4000),\n\t[REC_START_DT] nvarchar(4000),\n\t[REC_END_DT] nvarchar(4000),\n\t[SOURCE_SYSTEM] nvarchar(4000),\n\t[REC_CHECKSUM] nvarchar(4000),\n\t[REC_STATUS] nvarchar(4000),\n\t[IMG_LST_UPD_DT] nvarchar(4000),\n\t[IMG_CREATED_DT] nvarchar(4000),\n\t[DATA_IND] nvarchar(4000),\n\t[ACTIVE_IN_SOURCE_IND] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'data/customer/customer_dimension2/**',\n\tDATA_SOURCE = [deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.customer_dim_temp\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "kyn",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 13')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 14')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select top 10 * from MASTER.DBXDH_DHT_PROJECT_SIV\n\n\nselect top 10 *\nfrom openrowset(\n    bulk 'https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/project_data.csv',\n    format = 'csv',\n    parser_version = '2.0',\n    firstrow = 2 ) as rows",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://adlskyn001494deveus001.dfs.core.windows.net/kyn/DH_SF_SELLCYCE_DATA.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_tgt.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 6')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://contosolake.dfs.core.windows.net/users/NYCTripSmall.parquet',\n        FORMAT='PARQUET'\n    ) AS [result] ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "dsqlpoolKyn001494DevEtlHubEUS001",
						"poolName": "dsqlpoolKyn001494DevEtlHubEUS001"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 7')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id WHERE O.NAME = 'NYCTaxiTripSmall' AND O.TYPE = 'U' AND S.NAME = 'dbo')\nCREATE TABLE dbo.NYCTaxiTripSmall\n    (\n     [DateID] int,\n     [MedallionID] int,\n     [HackneyLicenseID] int,\n     [PickupTimeID] int,\n     [DropoffTimeID] int,\n     [PickupGeographyID] int,\n     [DropoffGeographyID] int,\n     [PickupLatitude] float,\n     [PickupLongitude] float,\n     [PickupLatLong] nvarchar(4000),\n     [DropoffLatitude] float,\n     [DropoffLongitude] float,\n     [DropoffLatLong] nvarchar(4000),\n     [PassengerCount] int,\n     [TripDurationSeconds] int,\n     [TripDistanceMiles] float,\n     [PaymentType] nvarchar(4000),\n     [FareAmount] numeric(19,4),\n     [SurchargeAmount] numeric(19,4),\n     [TaxAmount] numeric(19,4),\n     [TipAmount] numeric(19,4),\n     [TollsAmount] numeric(19,4),\n     [TotalAmount] numeric(19,4)\n    )\nWITH\n    (\n    DISTRIBUTION = ROUND_ROBIN,\n     CLUSTERED COLUMNSTORE INDEX\n     -- HEAP\n    )\nGO\n\nCOPY INTO dbo.NYCTaxiTripSmall\n(DateID 1, MedallionID 2, HackneyLicenseID 3, PickupTimeID 4, DropoffTimeID 5,\nPickupGeographyID 6, DropoffGeographyID 7, PickupLatitude 8, PickupLongitude 9, \nPickupLatLong 10, DropoffLatitude 11, DropoffLongitude 12, DropoffLatLong 13, \nPassengerCount 14, TripDurationSeconds 15, TripDistanceMiles 16, PaymentType 17, \nFareAmount 18, SurchargeAmount 19, TaxAmount 20, TipAmount 21, TollsAmount 22, \nTotalAmount 23)\nFROM 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/data/NYCTripSmall.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET'\n    ,MAXERRORS = 0\n    ,IDENTITY_INSERT = 'OFF'\n)",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "dsqlpoolKyn001494DevEtlHubEUS001",
						"poolName": "dsqlpoolKyn001494DevEtlHubEUS001"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 8')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/customer_dimension1/part-00000-75122f32-1c34-4457-a5e8-c099f452d769-c000.snappy.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 9')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [SIEBEL_SALES_STAGE_CODE]\n,[SIEBEL_SALES_STAGE_NAME]\n,[SSM_STEP_NO]\n,[SSM_STEP_NAME]\n FROM [etlhubconfirmed].[dbo].[dht_sell_cycle]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmp_sample_sql')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE TABLE ACCTRPTS_DIM (\n    ACCTRPTS_DIM_UID as BIGINT,\n    REMARKS as CHAR(255)\n);\n\nSELECT name, is_cdc_enabled FROM sys.databases;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/30thMay_deltaLake_Customer_SCD_Type2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "82d36037-50df-42bf-a42d-a3a840d768cf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'customer_data.csv' \n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) existing_MAX_KEY from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"#df3=spark.sql(qry1).count()\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3 || ' rows affected ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"/*\n",
							"\n",
							"select * from incrementalData_DF2;\n",
							"\n",
							"select * from existingDataDF1;\n",
							"\n",
							"select * from fullJoin;\n",
							"*/\n",
							"\n",
							"--select * from incrementalData_DF2;\n",
							"\n",
							"--select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"SELECT 'All Rows' as Title, a.* FROM fullJoin a;\n",
							"\n",
							"\n",
							"--No change records, ignore --7ROWS\n",
							"select 'No Change Rows' as Title, a.*  from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert2' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert2' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--select * from etlhubConfirmed.customer_dimension where rec_checksum='c475b27f1b384e1d2289948edad59d84'\n",
							"/*\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;\n",
							"\n",
							"UPDATE etlhubConfirmed.customer_dimension\n",
							"SET GBG_ID='GB302S66'\n",
							"    ,REC_CHECKSUM='c475b27f1b384e1d2289948edad59d86'\n",
							"    WHERE CUSTOMER_KEY=5\n",
							"    ;\n",
							"\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;    \n",
							"*/\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"\n",
							"--select * from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"/*\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='N'\n",
							"    ,REC_END_DT=current_timestamp \n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"WHERE (CUSTOMER_NO ) =  (select CUSTOMER_NO from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum) )\n",
							"AND CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select existing_CUSTOMER_KEY,1+existing_VERSION as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert after insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert after insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.CUSTOMER_NO\n",
							"AND LOWER(B.CUSTOMER_NO) = LOWER(B.existing_CUSTOMER_NO) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"select 'Final rows in SCD Type2' as Title,a.* from etlhubconfirmed.customer_dimension a;\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension ;\n",
							"--CUSTOMER_NO='C0000216';\n",
							"--group by VERSION having count(*)>1;\n",
							"/*\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='Y'\n",
							"    WHERE CURRENT_IND='N' and VERSION=2.0;\n",
							"--DELETE FROM etlhubconfirmed.customer_dimension WHERE customer_key in (3.0,4.0);\n",
							"\n",
							"\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set FINANCIAL_COUNTRY_CD='896'\n",
							"    ,REC_CHECKSUM='3145dfee7cc94e4483b4b0c7244a9949'\n",
							"    WHERE customer_key=7.0;\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set GBG_ID='GB302S60'\n",
							"    ,customer_name='WALPOLE CO-OPERATIVE BANK1'\n",
							"    ,REC_CHECKSUM='0520612ce8718d5df3b8bb4b165a6548'\n",
							"    WHERE customer_key=8.0;\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET CURRENT_IND='N'\n",
							"    WHERE CUSTOMER_KEY=11.0;\n",
							"*/\n",
							"--select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"\n",
							"\n",
							"select * from fullJoin;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select customer_key,count(*) from etlhubconfirmed.customer_dimension where current_ind='Y' group by customer_key having count(*)>1;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_table_scripts')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ad8ff950-e363-42ca-8d9a-22667831768b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.opportunity_business_partner_daily_ft (\r\n",
							"    BUSINESS_PARTNER_FACT_KEY INT NOT NULL,\t\r\n",
							"    BUSINESS_PARTNER_FACT_VERSION INT ,\t\r\n",
							"    OPPORTUNITY_KEY INT,\r\n",
							"    BUSINESS_PARTNER_KEY INT,\r\n",
							"\tINFLUENCER_ROLE_CODE varchar(10) ,\r\n",
							"\tCURRENT_IND varchar(1)  ,\r\n",
							"\tEXTRACT_DT TIMESTAMP ,\r\n",
							"\tREC_START_DT TIMESTAMP ,\r\n",
							"\tREC_END_DT TIMESTAMP ,\r\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\r\n",
							"\tREC_CHECKSUM varchar(32)  ,\r\n",
							"\tREC_STATUS varchar(1)  ,\r\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\r\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\r\n",
							"\tDATA_IND varchar(10)  ,\r\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \r\n",
							")\r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/BUSINESS_PARTNER_DAILY_FT'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.BUSINESS_PARTNER  (\r\n",
							"BUSINESS_PARTNER_KEY INT NOT NULL , \r\n",
							"BUSINESS_PARTNER_VERSION INT NOT NULL , \r\n",
							"BUSINESS_PARTNER_ID VARCHAR(30), \r\n",
							"BUS_PARTNER_NM VARCHAR(50), \r\n",
							"AGR_BUS_PARTNER_ID VARCHAR(30), \r\n",
							"AGR_BUS_PARTNER_NM VARCHAR(50), \r\n",
							"CURRENT_IND VARCHAR(1) NOT NULL , \r\n",
							"EXTRACT_DT TIMESTAMP, \r\n",
							"REC_START_DT TIMESTAMP, \r\n",
							"REC_END_DT TIMESTAMP, \r\n",
							"SOURCE_SYSTEM VARCHAR(50), \r\n",
							"REC_CHECKSUM VARCHAR(32), \r\n",
							"REC_STATUS VARCHAR(1), \r\n",
							"IMG_LST_UPD_DT TIMESTAMP , \r\n",
							"IMG_CREATED_DT TIMESTAMP )  \r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/Business_Partner'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DELETE etlhubConfirmed.BUSINESS_PARTNER\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.opportunity(\r\n",
							"OPPORTUNITY_KEY INTEGER NOT NULL , \r\n",
							"OPPORTUNITY_VERSION INT NOT NULL, \r\n",
							"OPPORTUNITY_NUM varchar(50), \r\n",
							"CURRENT_IND varchar(1), \r\n",
							"EXTRACT_DT TIMESTAMP, \r\n",
							"REC_START_DT TIMESTAMP, \r\n",
							"REC_END_DT TIMESTAMP, \r\n",
							"SOURCE_SYSTEM varchar(50), \r\n",
							"REC_CHECKSUM varchar(32), \r\n",
							"REC_STATUS varchar(1), \r\n",
							"IMG_LST_UPD_DT TIMESTAMP, \r\n",
							"IMG_CREATED_DT TIMESTAMP, \r\n",
							"ISA_CODE varchar(10), \r\n",
							"ARCH_REASON_CD varchar(20), \r\n",
							"BUSINESS_TRANSACTION_TYPE varchar(1), \r\n",
							"CLIENT_REPRESENTATIVE_NAME varchar(100), \r\n",
							"COMPETITOR_LIST varchar(100), \r\n",
							"IDENTIFIER_USER_NAME varchar(100), \r\n",
							"OPPORTUNITY_NAME varchar(100), \r\n",
							"OPPORTUNITY_SOURCE_CD varchar(20), \r\n",
							"SBS_SOL_VALID_IND varchar(1) , \r\n",
							"OPPORTUNITY_IDENTIFIER varchar(100) , \r\n",
							"ROGUE_IND varchar(1), \r\n",
							"REASON_TO_ACT varchar(255) , \r\n",
							"SOLUTION_CATG varchar(1024) , \r\n",
							"BRAND_SPONSOR_LIST varchar(500) , \r\n",
							"GBS_GEOGRAPHY_NAME varchar(60), \r\n",
							"GBS_BUSINESS_UNIT_GROUP_NAME varchar(60) , \r\n",
							"GBS_BUSINESS_UNIT_NAME varchar(60) , \r\n",
							"TAG_LIST varchar(1000) , \r\n",
							"OPPORTUNITY_LEGACY_NO varchar(100) )\r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/OPPORTUNITY'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"ALTER TABLE etlhubConfirmed.opportunity ADD COLUMN ACTIVE_IN_SOURCE_IND char(1)\r\n",
							"\t--ACTIVE_IN_SOURCE_IND char(1)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta lake table create statement')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c6e77609-c7a1-4757-9f05-a0341d00f594"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_PROJECT definition\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"/*\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"--rename INTEGER with INT\n",
							"--replace VARCHAR, CHAR as STRING\n",
							"--Default is not supported\n",
							"CREATE TABLE IF NOT EXISTS ETLHUBCONFIRMED.DHT_PROJECT  (\n",
							"\t\t  PROJECT_KEY INT NOT NULL , \n",
							"\t\t  PROJECT_VERSION INT NOT NULL , \n",
							"\t\t  PROJECT_ID STRING  , \n",
							"\t\t  FINANCIAL_COUNTRY_CD STRING , \n",
							"\t\t  LEDGER_CD STRING  , \n",
							"\t\t  OFFERING_COMPONENT_CD STRING  , \n",
							"\t\t  OPPORTUNITY_NUM STRING  , \n",
							"\t\t  PROJECT_DESC STRING  , \n",
							"\t\t  SIGNINGS_CD STRING  , \n",
							"\t\t  SIGNINGS_DESC STRING  , \n",
							"\t\t  BUSINESS_TYPE_CD STRING , \n",
							"\t\t  BUSINESS_TYPE_DESC STRING , \n",
							"\t\t  PROJECT_STATUS_CD STRING  , \n",
							"\t\t  PROJECT_STATUS_DESC STRING  , \n",
							"\t\t  PROJECT_CUSTOMER_NO STRING   , \n",
							"\t\t  PROJECT_CREATION_DATE DATE  , \n",
							"\t\t  ACCOUTNING_DIVISION STRING  , \n",
							"\t\t  RESPONSIBLE_SERV_OFFICE STRING , \n",
							"\t\t  PROJECT_LOCAL_CURRENCY STRING , \n",
							"\t\t  LOCAL_ATTRIBUTE07 STRING , \n",
							"\t\t  CURRENT_IND STRING  NOT NULL , \n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \n",
							"\t\t  REC_START_DT TIMESTAMP  , \n",
							"\t\t  REC_END_DT TIMESTAMP  , \n",
							"\t\t  SOURCE_SYSTEM STRING  , \n",
							"\t\t  REC_CHECKSUM STRING , \n",
							"\t\t  REC_STATUS STRING  , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP, -- NOT NULL GENERATED ALWAYS AS CURRENT TIMESTAMP , \n",
							"\t\t  IMG_CREATED_DT TIMESTAMP, -- NOT NULL GENERATED ALWAYS AS CURRENT TIMESTAMP , \n",
							"\t\t  SERVICE_TYPE_CD STRING  , \n",
							"\t\t  SERVICE_TYPE_DESC STRING , \n",
							"\t\t  RATE_TYPE_CD STRING , \n",
							"\t\t  RATE_TYPE_DESC STRING , \n",
							"\t\t  CHARGE_CD STRING  , \n",
							"\t\t  CHARGE_TO_SERVICE_OFFICE_CD STRING   , \n",
							"\t\t  CHARGE_TO_SERVICE_OFFICE_DESC STRING   , \n",
							"\t\t  CUSTOMER_PROJECT_DESC STRING  , \n",
							"\t\t  DESCRIPTION_CD STRING  , \n",
							"\t\t  PROJECT_TITLE STRING , \n",
							"\t\t  CUSTOMER_TYPE_CD STRING , \n",
							"\t\t  CUSTOMER_TYPE_DESC STRING , \n",
							"\t\t  SAP_STATUS_CD STRING , \n",
							"\t\t  DATA_IND STRING   , \n",
							"\t\t  CONTACT_NM STRING   , \n",
							"\t\t  LABOR_CLAIM_IND STRING   , \n",
							"\t\t  COMMENT_TEXT STRING   , \n",
							"\t\t  BID_AND_PROPOSAL_PROJECT_IND STRING   , \n",
							"\t\t  SIGNINGS_EXCEPTION_REASON_CODE STRING , \n",
							"\t\t  CHANNEL_INDICATOR STRING,\n",
							"          ACTIVE_IN_SOURCE_IND STRING  )   \n",
							"\t\t  USING DELTA\n",
							"\t\t  PARTITIONED BY (PROJECT_KEY)\n",
							"\t\t  LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/project_dimension'\n",
							" ;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b641cdfd-41dd-4bc0-8071-55a0c4528a89"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"import pyodbc\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"conn = connect(':memory:')\r\n",
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle.csv?sp=r&st=2022-05-11T09:48:50Z&se=2022-05-11T17:48:50Z&spr=https&sv=2020-08-04&sr=c&sig=ScsDhYGM4HWdCD8kMSMmLfY7Pex8jvmc02LvGatfwPI%3D\"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\r\n",
							"#print(df)\r\n",
							"sell_cycleDF=spark.createDataFrame(df)\r\n",
							"col_list=[]\r\n",
							"for i in sell_cycleDF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    #print (col_list)\r\n",
							"sell_cyclenkDF = sell_cycleDF.withColumn(\"nk_hash\",md5(\"SIEBEL_SALES_STAGE_CODE\"))\r\n",
							"sell_cyclecolhashDF = sell_cyclenkDF.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"sell_cyclecolhashDF.createOrReplaceTempView(\"sellcyclesrc\")\r\n",
							"server = 'sqlserver-kyn-001494-dev-eus-001.database.windows.net' \r\n",
							"database = 'sqldb-etlhub-confirmed' \r\n",
							"username = 'sqladminuser' \r\n",
							"password = 'try2find$5' \r\n",
							"cnxn = pyodbc.connect('DRIVER={com.microsoft.sqlserver.jdbc.SQLServerDriver};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\r\n",
							"#sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							" #   .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"  #  .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as existing_nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as existing_column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"   # .option(\"user\", \"sqladminuser\") \\\r\n",
							"    #.option(\"password\", \"try2find$5\") \\\r\n",
							"    #.option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    #.load()\r\n",
							"sellcycletgtDF.show()\r\n",
							"sellcycletgtDF.createOrReplaceTempView(\"sellcycletgt\")\r\n",
							"#insertsellcycle=pd.read_sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME,a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a left join  sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\",conn)\r\n",
							"#insertsellcycledf=spark.createDataFrame(insertsellcycle)\r\n",
							"#insertsellcycledf.show()\r\n",
							"#%%sql\r\n",
							"#select * from sellcyclesrc"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "405445b7-1887-422a-991e-e29084333dc4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import required modules\r\n",
							"from pyspark import SparkConf, SparkContext , SQLContext\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import SQLContext\r\n",
							" \r\n",
							"#Create spark configuration object\r\n",
							"conf = SparkConf()\r\n",
							"conf.setMaster(\"local\").setAppName(\"My app\")\r\n",
							" \r\n",
							"#Create spark context and sparksession\r\n",
							"sc = SparkContext.getOrCreate(conf=conf)\r\n",
							"spark = SparkSession(sc)\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"#read table data into a spark dataframe\r\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={database};\") \\\r\n",
							"    .option(\"dbtable\", table) \\\r\n",
							"    .option(\"user\", user) \\\r\n",
							"    .option(\"password\", password) \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"#show the data loaded into dataframe\r\n",
							"#jdbcDF.show()\r\n",
							"\r\n",
							"%%sql\r\n",
							"select * from DBXDH.DHTS_SELL_CYCLE\r\n",
							"\r\n",
							"#sqlContext.sql(select * from DBXDH.DHTS_SELL_CYCLE)\r\n",
							" #print(\"update successful\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from pyspark.sql.functions import when\r\n",
							"from pyspark.sql.context import SQLContext\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'project' # fill in your container name \r\n",
							"relative_path = '/' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \r\n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'sell_cycle_2022.csv' \r\n",
							"#csv_path='https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"df_csv = spark.read.csv(csv_path, header = 'true')\r\n",
							"#df_csv.show(20,False)\r\n",
							"\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"sqlContext.sql(\"select * from DBXDH.DHTS_SELL_CYCLE\")\r\n",
							"#sqlContext.sql(\"select * from DBXDH.DHTS_SELL_CYCLE\")\r\n",
							" #print(\"update successful\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 5')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "479488f5-cd51-442a-8e69-ac01e706e0c4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"conn = connect(':memory:')\r\n",
							"natural_key=\"CUSTOMER_NO\"\r\n",
							"columns = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&st=2022-05-15T14%3A59%3A08Z&se=2023-05-16T14%3A59%3A00Z&sr=b&sp=r&sig=RYE103C28iVzI4%2BTmiDyMqJhGGNqBooxZgUc4LITF4U%3D\"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"incrementalData = pd.read_csv(StringIO(blob_data.content_as_text()))\r\n",
							"#print(df)\r\n",
							"incrementalData_DF=spark.createDataFrame(incrementalData)\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    #print (col_list)\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\r\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\r\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \r\n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\r\n",
							"\r\n",
							"#Insert for New rows\r\n",
							"\r\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\r\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\r\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\r\n",
							"\r\n",
							"#insert new rows into database\r\n",
							"\r\n",
							"fullJoin2.write \\\r\n",
							"        .format(\"jdbc\") \\\r\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\r\n",
							"        .option(\"user\", \"sqladminuser\") \\\r\n",
							"        .option(\"password\", \"try2find$5\") \\\r\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\r\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\r\n",
							"        .mode(\"append\") \\\r\n",
							"        .save()\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 6')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d675b475-c43e-4a62-9394-92c8c1ab17a0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"#set the data lake file location:\r\n",
							"file_location = \"abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/sell_cycle_2022.csv\"\r\n",
							" \r\n",
							"#read in the data to dataframe df\r\n",
							"df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"delimiter\",\",\").load(file_location)\r\n",
							" \r\n",
							"#display the dataframe\r\n",
							"display(df)\r\n",
							"\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"#set the data lake file location:\r\n",
							"file_location = \"abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/sell_cycle_2022.csv\"\r\n",
							" \r\n",
							"#read in the data to dataframe df\r\n",
							"df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"delimiter\",\",\").load(file_location)\r\n",
							" \r\n",
							"#display the dataframe\r\n",
							"display(df)\r\n",
							"\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"\r\n",
							"\r\n",
							"CREATE TABLE etlhubConfirmed.SELL_CYCLE_2022\r\n",
							"     (SIEBEL_SALES_STAGE_CODE string,  \r\n",
							"      SIEBEL_SALES_STAGE_NAME string,  \r\n",
							"      SSM_STEP_NO string,  \r\n",
							"      SSM_STEP_NAME string)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%sql\r\n",
							"CREATE TABLE IF NOT EXISTS DBXDH.DHTS_SELL_CYCLE_2022\r\n",
							"USING CSV\r\n",
							"LOCATION 'abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/sell_cycle_2022.csv'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 8')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fa08d8a3-7e65-4778-8730-7f43ab7add39"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "Synapse SQL"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"spark.sql(\"CREATE TABLE etlhubconfirmed.sell_cycle (SIEBEL_SALES_STAGE_CODE string, SIEBEL_SALES_STAGE_NAME string, SSM_STEP_NO string, SSM_STEP_NAME string)\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 9')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0ea1c941-6c3f-4c4c-a59c-1b769f49bd8d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"\n",
							"spark.sql(\"SELECT CURRENT_IND,COUNT(*) FROM etlhubconfirmed.customer_dimension GROUP BY CURRENT_IND\")\n",
							"\n",
							"spark.stop()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_R')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "19142ba3-cf3f-4f0a-a0a6-cc18031d89f8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"d39a2370-4c1d-46ed-9a35-67499bf08ce8": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"0": "A",
												"1": "22",
												"2": "45000"
											},
											{
												"0": "B",
												"1": "35",
												"2": "65000"
											},
											{
												"0": "C",
												"1": "50",
												"2": "85000"
											}
										],
										"schema": [
											{
												"key": "0",
												"name": "state",
												"type": "string"
											},
											{
												"key": "1",
												"name": "age",
												"type": "bigint"
											},
											{
												"key": "2",
												"name": "salary",
												"type": "bigint"
											}
										],
										"truncated": false
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [
												"0"
											],
											"seriesFieldKeys": [
												"1"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"new_rows = [('A',22, 45000),(\"B\",35,65000) ,(\"C\",50,85000)]\r\n",
							"demo_df = spark.createDataFrame(new_rows, ['state', 'age', 'salary'])\r\n",
							"demo_df.show() \r\n",
							"demo_df.createOrReplaceTempView('demo_df')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# demo_df.createOrReplaceTempView('demo_df')\r\n",
							" # demo_df.write.csv('demo_df', mode='overwrite')\r\n",
							" # demo_df.write.parquet('abfss://<<TheNameOfAStorageAccountFileSystem>>@<<TheNameOfAStorageAccount>>.dfs.core.windows.net/demodata/demo_df', mode='overwrite')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Language Change**"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(spark.sql('SELECT * FROM demo_df'))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from demo_df;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_rows = [('D',42, 55000),(\"E\",65,75000) ,(\"F\",70,95000)]\r\n",
							"demo_dfs = spark.createDataFrame(new_rows, ['state', 'age', 'salary'])\r\n",
							"demo_dfs.show() \r\n",
							"demo_dfs.createOrReplaceTempView('demo_dfs')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from demo_df full join demo_dfs using(state,age,salary) order by state asc;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_Siva2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bc9a73e8-16ca-4335-821d-6b5e19c920d4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Final Code SCD Type2"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"conn = connect(':memory:')\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"columns = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&st=2022-05-15T14%3A59%3A08Z&se=2023-05-16T14%3A59%3A00Z&sr=b&sp=r&sig=RYE103C28iVzI4%2BTmiDyMqJhGGNqBooxZgUc4LITF4U%3D\"\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\n",
							"blob_data = blob_client.download_blob()\n",
							"incrementalData = pd.read_csv(StringIO(blob_data.content_as_text()))\n",
							"#print(df)\n",
							"incrementalData_DF=spark.createDataFrame(incrementalData)\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_Siva3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "390349d1-5b8a-47c2-90a4-36249c09414a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Try Delta Lake"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\n",
							"mssparkutils.fs.mount(\n",
							"    \"abfss://customer@adls4fsoetlhubdevuseast.dfs.core.windows.net\",\n",
							"    \"/etlhubadls6\",\n",
							"    {\"linkedService\":\"ls_adls_project_dimension\"}\n",
							")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId=mssparkutils.env.getJobId()\n",
							"print(jobId)\n",
							"df=spark.read.load('synfs:/' + jobId + '/etlhubadls6/customer_data.csv',format='csv',header=True)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define column headers\n",
							"columns = [\"Employee\",\"Salary\"]\n",
							"\n",
							"# Define data for original dataframe\n",
							"empOriginal = [(\"Employee_1\",50000),(\"Employee_2\",55000)]\n",
							"\n",
							"# Define data for updates dataframe\n",
							"empUpdates = [(\"Employee_1\",50000),(\"Employee_2\",60000),(\"Employee_3\",55000)]\n",
							"\n",
							"# Create dataframe with orignial employee data\n",
							"dfOriginal = spark.createDataFrame(data = empOriginal,schema = columns)\n",
							"\n",
							"# Create dataframe with updated employee data\n",
							"dfUpdates = spark.createDataFrame(data = empUpdates,schema = columns)\n",
							"\n",
							"# Display dfOriginal\n",
							"dfOriginal.show()\n",
							"\n",
							"# Display dfUpdates\n",
							"dfUpdates.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create temp views of both the dfOriginal and dfUpdates dataframes \n",
							"# so that we can easily query them with Spark SQL later\n",
							"\n",
							"#dfOriginal\n",
							"dfOriginal.createOrReplaceTempView('Employee_Original')\n",
							"\n",
							"#dfUpdates\n",
							"dfUpdates.createOrReplaceTempView('Employee_Updates')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE DELTA_Employees (\n",
							"  Employee STRING NOT NULL,\n",
							"  Salary INT NOT NULL,\n",
							"  BeginDate DATE NOT NULL,\n",
							"  EndDate DATE NOT NULL,\n",
							"  CurrentRecord INT NOT NULL \n",
							")\n",
							"USING DELTA\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/dl1'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"df = spark.sql(\"SELECT * FROM nyctaxi.passengercountstats\")\n",
							"df = df.repartition(1) # This ensures we'll get a single file during write()\n",
							"df.write.mode(\"overwrite\").csv(\"/NYCTaxi/PassengerCountStats_csvformat\")\n",
							"df.write.mode(\"overwrite\").parquet(\"/NYCTaxi/PassengerCountStats_parquetformat\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE TABLE IF NOT EXISTS default.people10m (\n",
							"  id INT,\n",
							"  firstName STRING,\n",
							"  middleName STRING,\n",
							"  lastName STRING,\n",
							"  gender STRING,\n",
							"  birthDate TIMESTAMP,\n",
							"  ssn STRING,\n",
							"  salary INT\n",
							") USING DELTA"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_insert')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "166de406-4008-4abd-b552-61725fa7fba1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"conn = connect(':memory:')\r\n",
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv?sp=r&st=2022-05-16T11:04:46Z&se=2022-05-16T19:04:46Z&spr=https&sv=2020-08-04&sr=b&sig=udm7PnviOcxI9VLp6Kuvyn9AjghiU1sAobpG7EdlNvg%3D\"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\r\n",
							"df.show(10,False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.session import SparkSession\r\n",
							"\r\n",
							"spark = SparkSession \\\r\n",
							".builder \\\r\n",
							".appName(\"SparkUpsert\") \\\r\n",
							".master(\"Local[*]\") \\\r\n",
							".getOrCreate\r\n",
							"\r\n",
							"df_csv = spark \\\r\n",
							"    .read \\\r\n",
							"    .option(\"header\",\"true\") \\\r\n",
							"    .csv(\"project/sell_cycle_2022.csv\")\r\n",
							"\r\n",
							"df_csv.show(10,False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import required modules\r\n",
							"from pyspark import SparkConf, SparkContext , SQLContext\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import SQLContext\r\n",
							" \r\n",
							"#Create spark configuration object\r\n",
							"conf = SparkConf()\r\n",
							"conf.setMaster(\"local\").setAppName(\"My app\")\r\n",
							" \r\n",
							"#Create spark context and sparksession\r\n",
							"sc = SparkContext.getOrCreate(conf=conf)\r\n",
							"spark = SparkSession(sc)\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"#read table data into a spark dataframe\r\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={database};\") \\\r\n",
							"    .option(\"dbtable\", table) \\\r\n",
							"    .option(\"user\", user) \\\r\n",
							"    .option(\"password\", password) \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"#show the data loaded into dataframe\r\n",
							"#jdbcDF.show()\r\n",
							"\r\n",
							"%%sql\r\n",
							"select * from DBXDH.DHTS_SELL_CYCLE\r\n",
							"\r\n",
							"#sqlContext.sql(select * from DBXDH.DHTS_SELL_CYCLE)\r\n",
							" #print(\"update successful\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from pyspark.sql.functions import when\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'project' # fill in your container name \r\n",
							"relative_path = '/' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \r\n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'sell_cycle_2022.csv' \r\n",
							"#csv_path='https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"df_csv = spark.read.csv(csv_path, header = 'true')\r\n",
							"df_csv.show(20,False)\r\n",
							"\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"#write the dataframe into a sql table\r\n",
							"df_csv.write.mode(\"append\") \\\r\n",
							"    .format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={database};\") \\\r\n",
							"    .option(\"dbtable\", table) \\\r\n",
							"    .option(\"user\", user) \\\r\n",
							"    .option(\"password\", password) \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .save()\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 51
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_siva')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a6ed62c0-bd62-49e6-9df2-663df53c4d97"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"3009228f-3872-4c4a-8d8a-fa5f8f9fd19f": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"0": "EEI0001",
												"1": "602",
												"2": "NG",
												"3": "602EE00119",
												"4": "6950-94G",
												"5": "PX-WR1SFFU",
												"6": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B024",
												"9": "A",
												"10": "004324",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006C",
												"14": "EUR",
												"16": "2020-01-30",
												"17": "2020-02-15",
												"18": "2023-03-31",
												"26": "EENIS00001",
												"27": "46",
												"28": "2021-09-29 18:32:46.558",
												"29": "IC",
												"30": "147500.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "Z042",
												"35": "006C",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "147500.00000",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "A"
											},
											{
												"0": "EEI0002",
												"1": "602",
												"2": "NG",
												"3": "IBM",
												"4": "6940-92A",
												"6": "Ţ�����@م������@Ö������@�����\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"7": "XS",
												"8": "B046",
												"9": "C",
												"10": "003550",
												"11": "2021-09-28",
												"12": "DD",
												"13": "006C",
												"14": "EUR",
												"16": "2021-09-01",
												"17": "2021-09-01",
												"18": "2021-12-31",
												"26": "EENIS00002",
												"27": "30",
												"28": "2022-03-04 20:58:18.790",
												"29": "IC",
												"30": "1517.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"35": "006C",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "1517.00000",
												"39": "IBM",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "A"
											},
											{
												"0": "LVI0001",
												"1": "608",
												"2": "NG",
												"3": "608CEMEX",
												"4": "6940-98A",
												"6": "�@�����@⥃�ǣǖ�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "E011",
												"9": "A",
												"10": "004300",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006O",
												"14": "EUR",
												"16": "2013-02-28",
												"17": "2014-11-24",
												"18": "2022-08-31",
												"26": "LVNIS00001",
												"27": "30",
												"28": "2021-09-16 11:42:34.627",
												"29": "IC",
												"30": "0.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"35": "006O",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Z"
											},
											{
												"0": "LVI0002",
												"1": "608",
												"2": "NG",
												"3": "608CEMEX",
												"4": "6941-97X",
												"6": "���@���@����@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@��������a�������\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B048",
												"9": "A",
												"10": "004300",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006O",
												"14": "EUR",
												"16": "2013-02-28",
												"17": "2014-08-12",
												"18": "2022-08-22",
												"26": "LVNIS00001",
												"27": "30",
												"28": "2021-09-16 11:42:34.627",
												"29": "IC",
												"30": "0.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "S011",
												"35": "006O",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Z"
											},
											{
												"0": "LVI0003",
												"1": "608",
												"2": "NG",
												"3": "608CEMEX",
												"4": "6940-97K",
												"6": "���@���@¤��\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"7": "SR",
												"8": "B048",
												"9": "A",
												"10": "004300",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006O",
												"14": "EUR",
												"16": "2013-02-28",
												"17": "2017-10-20",
												"18": "2022-08-31",
												"26": "LVNIS00001",
												"27": "30",
												"28": "2021-09-20 11:42:33.707",
												"29": "BE",
												"30": "22499.91000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "S011",
												"35": "006O",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"39": "608CEMEX",
												"40": "22499.91000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Z"
											},
											{
												"0": "LVI0004",
												"1": "608",
												"2": "NG",
												"3": "608CEMEX",
												"4": "6940-97K",
												"6": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"7": "XS",
												"8": "E011",
												"9": "A",
												"10": "004300",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006O",
												"14": "EUR",
												"16": "2013-02-28",
												"17": "2019-01-01",
												"18": "2022-08-31",
												"26": "LVNIS00001",
												"27": "30",
												"28": "2021-09-16 11:42:34.627",
												"29": "IC",
												"30": "0.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "S011",
												"35": "006O",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Z"
											},
											{
												"0": "LVI0005",
												"1": "608",
												"2": "NG",
												"3": "NEWCOLV00300124",
												"4": "6950-94G",
												"6": "�����������a�������a������a������aÅ��@⁣������@ę������@ĉ�������a����������@���\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B008",
												"9": "A",
												"10": "003560",
												"11": "2021-08-31",
												"12": "DD",
												"13": "0076",
												"14": "EUR",
												"16": "2017-06-06",
												"17": "2017-06-08",
												"18": "2022-08-31",
												"22": "608",
												"23": "NEWIS",
												"24": "000090",
												"25": "96DH9",
												"26": "LVNIS00002",
												"27": "46",
												"28": "2021-10-28 17:25:54.303",
												"29": "IC",
												"30": "0.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "Z028",
												"35": "0076",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"39": "608LV000217/L100460",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Y"
											},
											{
												"0": "LVI0006",
												"1": "608",
												"2": "NG",
												"3": "NEWCOLV00300124",
												"4": "6950-94G",
												"6": "�����������a�������a������a������aÅ��@⁣������@ę������@ĉ�������a����������@���@@@@@@@@@@@@@@@@@@��@@����������\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B008",
												"9": "A",
												"10": "003560",
												"11": "2021-08-31",
												"12": "DD",
												"13": "0076",
												"14": "EUR",
												"16": "2017-06-06",
												"17": "2017-06-08",
												"18": "2022-08-31",
												"22": "608",
												"23": "NEWIS",
												"24": "000090",
												"25": "8M2Q6",
												"26": "LVNIS00002",
												"27": "46",
												"28": "2021-10-28 17:25:54.303",
												"29": "IC",
												"30": "520000.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "Z028",
												"35": "0076",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "520000.00000",
												"39": "608LV000217/L100461",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Y"
											},
											{
												"0": "LVI0007",
												"1": "608",
												"2": "NG",
												"3": "NEWCOLV00300124",
												"4": "6950-94G",
												"6": "�����������@�����������@Ȗ�����@▓�����\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B008",
												"9": "A",
												"10": "003560",
												"11": "2021-08-31",
												"12": "DD",
												"13": "0076",
												"14": "EUR",
												"16": "2017-06-06",
												"17": "2019-01-01",
												"18": "2022-08-31",
												"22": "608",
												"23": "NEWIS",
												"24": "000090",
												"25": "96DH9",
												"26": "LVNIS00002",
												"27": "46",
												"28": "2021-10-28 17:25:54.303",
												"29": "IC",
												"30": "95680.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "Z028",
												"35": "0076",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "95680.00000",
												"39": "608LV000217",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "K"
											}
										],
										"schema": [
											{
												"key": "0",
												"name": "PROJECT_ID",
												"type": "string"
											},
											{
												"key": "1",
												"name": "FINANCIAL_COUNTRY_CD",
												"type": "string"
											},
											{
												"key": "2",
												"name": "LEDGER_CD",
												"type": "string"
											},
											{
												"key": "3",
												"name": "LEGAL_CONTRACT_ID",
												"type": "string"
											},
											{
												"key": "4",
												"name": "OFFERING_COMPONENT_CD",
												"type": "string"
											},
											{
												"key": "5",
												"name": "OPPORTUNITY_NUM",
												"type": "string"
											},
											{
												"key": "6",
												"name": "PROJECT_DESC",
												"type": "string"
											},
											{
												"key": "7",
												"name": "SIGNINGS_CD",
												"type": "string"
											},
											{
												"key": "8",
												"name": "BUSINESS_TYPE_CD",
												"type": "string"
											},
											{
												"key": "9",
												"name": "PROJECT_STATUS_CD",
												"type": "string"
											},
											{
												"key": "10",
												"name": "PROJECT_CUSTOMER_NO",
												"type": "string"
											},
											{
												"key": "11",
												"name": "PROJECT_CREATION_DATE",
												"type": "string"
											},
											{
												"key": "12",
												"name": "ACCOUTNING_DIVISION",
												"type": "string"
											},
											{
												"key": "13",
												"name": "RESPONSIBLE_SERV_OFFICE",
												"type": "string"
											},
											{
												"key": "14",
												"name": "PROJECT_LOCAL_CURRENCY",
												"type": "string"
											},
											{
												"key": "15",
												"name": "LOCAL_ATTRIBUTE07",
												"type": "string"
											},
											{
												"key": "16",
												"name": "CONTRACT_SIGNED_DATE",
												"type": "string"
											},
											{
												"key": "17",
												"name": "PROJECT_START_DATE",
												"type": "string"
											},
											{
												"key": "18",
												"name": "PROJECT_END_DATE",
												"type": "string"
											},
											{
												"key": "19",
												"name": "LOBPARTNERCOMPANY",
												"type": "string"
											},
											{
												"key": "20",
												"name": "LOBPARTNERCOUNTRY",
												"type": "string"
											},
											{
												"key": "21",
												"name": "LOBPARTNERSERNUM",
												"type": "string"
											},
											{
												"key": "22",
												"name": "PMCOUNTRY",
												"type": "string"
											},
											{
												"key": "23",
												"name": "PMCOMPANY",
												"type": "string"
											},
											{
												"key": "24",
												"name": "PMSERNUM",
												"type": "string"
											},
											{
												"key": "25",
												"name": "BUSINESS_PARTNER_ID",
												"type": "string"
											},
											{
												"key": "26",
												"name": "IBM_CONTRACT_NUM",
												"type": "string"
											},
											{
												"key": "27",
												"name": "BUS_MEASMNT_DIV_CD",
												"type": "string"
											},
											{
												"key": "28",
												"name": "LAST_UPDATE_DT",
												"type": "string"
											},
											{
												"key": "29",
												"name": "SERVICE_TYPE",
												"type": "string"
											},
											{
												"key": "30",
												"name": "TOTAL_TCV",
												"type": "string"
											},
											{
												"key": "31",
												"name": "TOTAL_UNITS",
												"type": "string"
											},
											{
												"key": "32",
												"name": "HOURS_PER_DAY",
												"type": "string"
											},
											{
												"key": "33",
												"name": "RATE_TYPE",
												"type": "string"
											},
											{
												"key": "34",
												"name": "CHARGE_CODE",
												"type": "string"
											},
											{
												"key": "35",
												"name": "CHARGE_TO_SERVOFFICE",
												"type": "string"
											},
											{
												"key": "36",
												"name": "CUSTOMER_PROJECT",
												"type": "string"
											},
											{
												"key": "37",
												"name": "DESCRIPTION_CODE",
												"type": "string"
											},
											{
												"key": "38",
												"name": "LABOR_AMOUNT",
												"type": "string"
											},
											{
												"key": "39",
												"name": "PROJECT_TITLE",
												"type": "string"
											},
											{
												"key": "40",
												"name": "SCHEDULED_CHARGES",
												"type": "string"
											},
											{
												"key": "41",
												"name": "NON_SCHEDULED_CHARGES",
												"type": "string"
											},
											{
												"key": "42",
												"name": "CUSTOMER_TYPE",
												"type": "string"
											},
											{
												"key": "43",
												"name": "TRAVEL_CHARGES",
												"type": "string"
											},
											{
												"key": "44",
												"name": "SAP_STAT_CD",
												"type": "string"
											},
											{
												"key": "45",
												"name": "DATA_IND",
												"type": "string"
											},
											{
												"key": "46",
												"name": "CHANNEL_INDICATOR",
												"type": "string"
											},
											{
												"key": "47",
												"name": "SIGNINGS_EXCEPTION_CODE",
												"type": "string"
											},
											{
												"key": "48",
												"name": "LEAD_PARNTER_SER_NUM",
												"type": "string"
											},
											{
												"key": "49",
												"name": "LEAD_PARNTER_COMPANYCODE",
												"type": "string"
											},
											{
												"key": "50",
												"name": "LEAD_PARNTER_COUNTRY",
												"type": "string"
											}
										],
										"truncated": false
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "count",
											"categoryFieldKeys": [
												"0"
											],
											"seriesFieldKeys": [
												"0"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_rows=[('Andhra',35,30000),('Telangana',30,50000),('Maharashtra',45,65000)]\n",
							"demo_df=spark.createDataFrame(new_rows,['State', 'Age', 'Salary'])\n",
							"demo_df.createOrReplaceTempView('demo_df')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from demo_df;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read data from ADLS "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"\n",
							"# Primary storage info \n",
							"account_name = 'Your primary storage account name' # fill in your primary account name \n",
							"container_name = 'Your container name' # fill in your container name \n",
							"relative_path = 'Your relative path' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'Your file name ' \n",
							"df_csv = spark.read.csv(csv_path, header = 'true') \n",
							"\n",
							"# Read a parquet file \n",
							"parquet_path = adls_path + ' Your file name ' \n",
							"df_parquet = spark.read.parquet(parquet_path) \n",
							"\n",
							"# Read a json file \n",
							"json_path = adls_path + 'Your file name ' \n",
							"df_json = spark.read.json(json_path) "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read data from dedicated SQL pool"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark \n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._ \n",
							"import com.microsoft.spark.sqlanalytics.utils.Constants \n",
							"\n",
							"val sql_pool_name = \"Your sql pool name\" //fill in your sql pool name \n",
							"val schema_name = \"Your sql schema name\" //fill in your sql schema name \n",
							"val table_name = \"Your sql table name\" //fill in your sql table name \n",
							"\n",
							"// Read the sql table as a Spark dataframe \n",
							"val spark_read = spark.read. \n",
							"    sqlanalytics(s\"$sql_pool_name.$schema_name.$table_name\") \n",
							"spark_read.show(5, truncate = false) "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sql db connection"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"database = \"sqldb1\"\n",
							"table = \"DBXDH.DHT_PROJECT_SIV\"\n",
							"user = \"sqladminuser\"\n",
							"password  = \"try2find$5\"\n",
							"\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", \"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={sqldb-etlhub-confirmed}\") \\\n",
							"    .option(\"dbtable\", \"DBXDH.DHT_PROJECT_SIV\") \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"\n",
							"jdbcDF.createOrReplaceTempView('jdbcDF')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select count(*) as cnt from jdbcDF "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read CSV file"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"\n",
							"# Primary storage info \n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '/' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/customer_data.csv'\n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"#csv_path = adls_path + 'customer_data.csv' \n",
							"csv_path='abfss://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/customer_data.csv'\n",
							"df_csv = spark.read.csv(csv_path, header = 'true')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.read.csv(\"abfss://customer@adls4fsoetlhubdevuseast.dfs.core.windows.net/customer_data.csv\").count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name='ls_adls_project_dimension'\n",
							"spark.conf.set(\"spark.storage.synapse.linkedServiceName\",linked_service_name)\n",
							"spark.conf.set(\"fs.azure.account.oauth.provider.type\",\"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\n",
							"\n",
							"spark.read.csv('abfss://https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv')\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&si=customer-18095228EC2&sr=b&sig=iClJ7jEmBjlM0da5og1b93hnK4rNk1twl3phbFf1clY%3D\"\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\n",
							"blob_data = blob_client.download_blob()\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\n",
							"#print(df)\n",
							"sparkDF=spark.createDataFrame(df) \n",
							"\n",
							"sparkDF.createOrReplaceTempView('customer_df')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from customer_df"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"\n",
							"# Azure storage access info \n",
							"blob_account_name = 'adls4fsoetlhubdevuseast' # replace with your blob name \n",
							"blob_container_name = 'customer' # replace with your container name \n",
							"blob_relative_path = '' # replace with your relative folder path \n",
							"linked_service_name = 'ls_adls_project_dimension' # replace with your linked service name \n",
							"\n",
							"blob_sas_token = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&si=customer-18095228EC2&sr=b&sig=iClJ7jEmBjlM0da5og1b93hnK4rNk1twl3phbFf1clY%3D\"\n",
							"#mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \n",
							"\n",
							"# Allow SPARK to access from Blob remotely \n",
							"wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \n",
							"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \n",
							"print('Remote blob path: ' + wasbs_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = wasbs_path + 'customer_data.csv ' \n",
							"df_csv = spark.read.csv(csv_path, header = 'true') "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adfsas"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df3=spark.read.load('abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/project_data.csv',format='csv',header=True)\n",
							"display(df3)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_siva1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3c87bc4d-9332-4c94-bfb4-7eb60596a9a3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Incremental source data in CSV file on Azure data lake storage"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/project_siv_data.csv?sv=2020-10-02&st=2022-05-11T09%3A48%3A28Z&se=2022-05-12T09%3A48%3A28Z&sr=b&sp=r&sig=JhdSw1F%2BODRA4%2B5nko0YwY8RGyjUjbuyA4ms2GblpeE%3D\"\n",
							"\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\n",
							"blob_data = blob_client.download_blob()\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\n",
							"#print(df)\n",
							"sparkDF=spark.createDataFrame(df) \n",
							"\n",
							"sparkDF.createOrReplaceTempView('customer_df')\n",
							"#sparkDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Existing Data in SQL DB table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"user = \"sqladminuser\"\n",
							"password  = \"try2find$5\"\n",
							"\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", \"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={sqldb-etlhub-confirmed}\") \\\n",
							"    .option(\"query\", \"SELECT tgt.* FROM DBXDH.DHT_PROJECT_PYSPARK as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#jdbcDF.show()\n",
							"jdbcDF.createOrReplaceTempView('jdbcDF')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Full Outer Join to identify New, Changed, Deleted rows and the ones no longer active in source"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin=sparkDF.join(jdbcDF,sparkDF.PROJECT_ID == jdbcDF.PROJECT_ID, \"fullouter\") \n",
							"fullJoin.createOrReplaceTempView('fullJoin')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from fullJoin;\n",
							"\n",
							"select * from customer_df I \n",
							"     full outer join \n",
							"     jdbcDF E on I.PROJECT_ID=E.PROJECT_ID;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Conditional split: New, Changed, Deleted records and no longer active in source"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin.spark"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
							"    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
							"    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
							"    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
							"    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
							"      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
							"  ]\n",
							"empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
							"       \"emp_dept_id\",\"gender\",\"salary\"]\n",
							"\n",
							"empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
							"empDF.printSchema()\n",
							"empDF.show(truncate=False)\n",
							"\n",
							"dept = [(\"Finance\",10), \\\n",
							"    (\"Marketing\",20), \\\n",
							"    (\"Sales\",30), \\\n",
							"    (\"IT\",40) \\\n",
							"  ]\n",
							"deptColumns = [\"dept_name\",\"dept_id\"]\n",
							"deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
							"deptDF.printSchema()\n",
							"deptDF.show(truncate=False)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
							"     .show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
							"    .show(truncate=False)\n",
							"empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
							"    .show(truncate=False)\n",
							"empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
							"    .show(truncate=False)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\n",
							"for i in sell_cycleDF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"sell_cyclenkDF = sell_cycleDF.withColumn(\"nk_hash\",md5(\"SIEBEL_SALES_STAGE_CODE\"))\n",
							"sell_cyclecolhashDF = sell_cyclenkDF.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"sell_cyclecolhashDF.createOrReplaceTempView(\"sellcyclesrc\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Final Code SCD Type2"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"conn = connect(':memory:')\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"columns = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&st=2022-05-15T14%3A59%3A08Z&se=2023-05-16T14%3A59%3A00Z&sr=b&sp=r&sig=RYE103C28iVzI4%2BTmiDyMqJhGGNqBooxZgUc4LITF4U%3D\"\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\n",
							"blob_data = blob_client.download_blob()\n",
							"incrementalData = pd.read_csv(StringIO(blob_data.content_as_text()))\n",
							"#print(df)\n",
							"incrementalData_DF=spark.createDataFrame(incrementalData)\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"\n",
							"#insertsellcycle=pd.read_sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME,a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a left join  sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\",conn)\n",
							"#insertsellcycledf=spark.createDataFrame(insertsellcycle)\n",
							"#insertsellcycledf.show()\n",
							"#%%sql\n",
							"#select * from sellcyclesrc\n",
							"\n",
							"\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER1 as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from existingDataDF"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_nk_hash) and LOWER(column_hash) = LOWER(rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_nk_hash) and LOWER(column_hash) <> LOWER(rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_upsert')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7cb888a4-d3ec-4c48-acd6-945ba728ffd9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from pyspark.sql.functions import when\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'project' # fill in your container name \r\n",
							"relative_path = '/' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \r\n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'sell_cycle_2022.csv' \r\n",
							"#csv_path='https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"df_csv = spark.read.csv(csv_path, header = 'true')\r\n",
							"df_csv.show(20,False)\r\n",
							"\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from pyspark.sql.functions import when\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'project' # fill in your container name \r\n",
							"relative_path = '/' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \r\n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'sell_cycle_2022.csv' \r\n",
							"#csv_path='https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"df = spark.read.csv(csv_path, header = 'true')\r\n",
							"df.show(20,False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#package com.flblue.scalatrain\r\n",
							"import java\r\n",
							"import java.sql\r\n",
							"import java.util.properties\r\n",
							"import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\r\n",
							"import org.apache.spark.sql.SparkSession\r\n",
							"\r\n",
							"#jdbcUrl = \"jdbc:mysql://sqlserver-kyn-001494-dev-eus-001.database.windows.net:3306/?serverTimezone=ESTSEDT\"\r\n",
							"#jdbcDriver = \"com.mysql.cj.jdbc.Driver\"\r\n",
							"#val connectionProperties = new Properties\r\n",
							"connectionProperties.put(\"user\",\"sqladminuser\")\r\n",
							"connectionProperties.put(\"password\",\"try2find$5\")\r\n",
							"connectionProperties.put(\"jdbcUrl\",\"jdbc:mysql://sqlserver-kyn-001494-dev-eus-001.database.windows.net:3306/?serverTimezone=ESTSEDT\")\r\n",
							"connectionProperties.put(\"jdbcDriver\",\"com.mysql.cj.jdbc.Driver\")\r\n",
							"connectionProperties.put(\"dbname\",\"sqldb-etlhub-confirmed\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import required modules\r\n",
							"from pyspark import SparkConf, SparkContext\r\n",
							"from pyspark.sql import SparkSession\r\n",
							" \r\n",
							"#Create spark configuration object\r\n",
							"conf = SparkConf()\r\n",
							"conf.setMaster(\"local\").setAppName(\"My app\")\r\n",
							" \r\n",
							"#Create spark context and sparksession\r\n",
							"sc = SparkContext.getOrCreate(conf=conf)\r\n",
							"spark = SparkSession(sc)\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"#read table data into a spark dataframe\r\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={database};\") \\\r\n",
							"    .option(\"dbtable\", table) \\\r\n",
							"    .option(\"user\", user) \\\r\n",
							"    .option(\"password\", password) \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							" \r\n",
							"#show the data loaded into dataframe\r\n",
							"jdbcDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_scala')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "10334c61-cb17-44b6-8520-332ad9299590"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import org.apache.log4j.Logger\r\n",
							"import org.apache.spark.SparkConf\r\n",
							"import org.apache.spark.sql.{DataFrame, SparkSession}\r\n",
							"\r\n",
							"import scala.io.Source\r\n",
							"//import org.ini4j.Ini\r\n",
							"\r\n",
							"import java.io.File\r\n",
							"import java.util.Properties\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"object AccountReports extends Serializable {\r\n",
							"\r\n",
							"  @transient lazy val logger : Logger = Logger.getLogger(getClass.getName)\r\n",
							"\r\n",
							"  def main(args: Array[String] ) = {\r\n",
							"    logger.info(\"Started\")\r\n",
							"    val spark = createSparkSession()\r\n",
							"    val (jdbcURL, jdbcProps) = connectJDBC(spark)\r\n",
							"    val sourceDF = sourceData(spark, jdbcURL, jdbcProps)\r\n",
							"    sourceDF.show(5, false)\r\n",
							"    val lookupDF = lookupData(spark, jdbcURL, jdbcProps)\r\n",
							"    lookupDF.show(5, false)\r\n",
							"    //val (insertDF, updateDF) = changeCapture(sourceDF, lookupDF)\r\n",
							"    //loadTarget(insertDF, updateDF)\r\n",
							"    spark.stop()\r\n",
							"    logger.info(\"Stopped\")\r\n",
							"  }\r\n",
							"\r\n",
							"  def createSparkSession() = {\r\n",
							"    SparkSession.builder()\r\n",
							"      //.master(\"local[5]\")\r\n",
							"      //.config(getSparkAppConf())\r\n",
							"      .getOrCreate()\r\n",
							"  }\r\n",
							"\r\n",
							"  def connectJDBC(spark: SparkSession) = {\r\n",
							"    val props = new Properties()\r\n",
							"    //Ini data = new Ini(new File('properties.ini'))\r\n",
							"    //val driverInfo = spark.read      .option(\"multiline\",true)      .json(\"properties.json\")\r\n",
							"    val url = \"sql-server-pgmp-dev.database.windows.net\" //driverInfo.select(\"url\").collect()(0).mkString(\"\")\r\n",
							"    props.put(\"url\",url)\r\n",
							"    props.put(\"user\",\"pgmpetl\") //driverInfo.select(\"username\").collect()(0).mkString(\"\"))\r\n",
							"    props.put(\"password\",\"Zaq1@wsxZaq1@wsx\") //driverInfo.select(\"password\").collect()(0).mkString(\"\"))\r\n",
							"    //props.setProperty(\"driver\",driverInfo.select(\"driver\").collect()(0).mkString(\"\"))\r\n",
							"    logger.info(\"Props - \" + props)\r\n",
							"    (url, props)\r\n",
							"  }\r\n",
							"\r\n",
							"  def sourceData(spark: SparkSession, jdbcURL: String, jdbcProps: Properties) = {\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_ETL_EXCTN\", jdbcProps)\r\n",
							"      .select(\"ETL_EXCTN_ID\", \"ETL_PARAM_START_TMS\", \"ETL_PARAM_END_TMS\")\r\n",
							"      .where(\"IS_CURR_IND = 'Y'\")\r\n",
							"      .createOrReplaceTempView(\"FIL\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_ETL_JOBS\", jdbcProps)\r\n",
							"      .where(\"ETL_JOB_NM = 'BALD0010_ACCTRPTS_DIM'\")\r\n",
							"      .selectExpr(\"coalesce(max(ETL_JOB_ID), -1) as `ETL_JOB_ID`\")\r\n",
							"      .createOrReplaceTempView(\"JOB\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.SRC_SYS_DIM\", jdbcProps)\r\n",
							"      .where(\"SRC_SYS_CD = 'PGMP'\")\r\n",
							"      .selectExpr(\"coalesce(max(SRC_SYS_DIM_UID), -1) as `SRC_SYS_DIM_UID`\")\r\n",
							"      .createOrReplaceTempView(\"SYS\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"APPFUN.ACCTRPTS\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"ACC\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_DATE_TRIGGERS\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"ZDT\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_DELD_PROC_ID\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"PDEL\")\r\n",
							"\r\n",
							"    spark.sql(\r\n",
							"      \"\"\"\r\n",
							"        |Select\r\n",
							"        |ACC.PROC_ID as ACCTRPTS_DIM_UID, ACC.PROJECT_ID as PRJCT_ID, cast(ACC.REMARKS as varchar(1024)) as ACCTRPTS_REM_TXT, \tACC.CREATED_TS as SRC_CRETD_TMS, ACC.CREATED_USERID as SRC_CRETD_USER_ID, ACC.UPDATED_TS as SRC_UPDTD_TMS, ACC.UPDATED_USERID as SRC_UPDTD_USER_ID, JOB.ETL_JOB_ID, FIL.ETL_EXCTN_ID, SYS.SRC_SYS_DIM_UID, case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\r\n",
							"        |From ACC\r\n",
							"        |inner join ZDT on ACC.PROC_ID = ZDT.PROC_ID\r\n",
							"        |left join PDEL on ACC.PROC_ID = PDEL.PROC_ID\r\n",
							"        |left join JOB on 1 = 1\r\n",
							"        |left join FIL on 1 = 1\r\n",
							"        |left join SYS on 1 = 1\r\n",
							"        |\"\"\".stripMargin)\r\n",
							"\r\n",
							"  }\r\n",
							"\r\n",
							"  def lookupData(spark: SparkSession, jdbcURL: String, jdbcProps: Properties) = {\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ACCTRPTS_DIM\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"S\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_DATE_TRIGGERS\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"ZDT\")\r\n",
							"\r\n",
							"    spark.sql(\r\n",
							"      \"\"\"\r\n",
							"        |Select\r\n",
							"        |S.ACCTRPTS_DIM_UID, S.PRJCT_ID, S.ACCTRPTS_REM_TXT, S.SRC_CRETD_TMS, S.SRC_CRETD_USER_ID, S.SRC_UPDTD_TMS, S.SRC_UPDTD_USER_ID, S.SRC_SYS_DIM_UID, S.ETL_JOB_ID\r\n",
							"        |From S\r\n",
							"        |inner join ZDT on S.ACCTRPTS_DIM_UID = ZDT.PROC_ID\r\n",
							"        |order by S.ACCTRPTS_DIM_UID asc\r\n",
							"        |\"\"\".stripMargin)\r\n",
							"  }\r\n",
							"\r\n",
							"  def changeCapture(sourceDF: DataFrame, lookupDF: DataFrame) = {\r\n",
							"    val sourceKey = sourceDF.select(\"ACCTRPTS_DIM_UID\")\r\n",
							"    val lookupKey = lookupDF.select(\"ACCTRPTS_DIM_UID\")\r\n",
							"    val insertDF = sourceDF.join(lookupDF, sourceDF.col(\"ACCTRPTS_DIM_UID\") =!= lookupDF.col(\"ACCTRPTS_DIM_UID\"), \"inner\")\r\n",
							"    val updateDF = sourceDF.join(lookupDF, sourceDF.col(\"ACCTRPTS_DIM_UID\") === lookupDF.col(\"ACCTRPTS_DIM_UID\"), \"inner\")\r\n",
							"    (insertDF,updateDF)\r\n",
							"  }\r\n",
							"\r\n",
							"  def loadTarget(insertDF: DataFrame, updateDF: DataFrame) = {\r\n",
							"    // inner join ZDT on ACC.PROC_ID = ZDT.PROC_ID\r\n",
							"    // inner join ZDT on S.ACCTRPTS_DIM_UID = ZDT.PROC_ID\r\n",
							"  }\r\n",
							"\r\n",
							"}\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/copy_xfrm_BP_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a0227500-03ce-4222-9fcf-53f57ab1e0bf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/BUSPARTNER_DIM.csv\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'BUSPARTNER_DIM.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.business_partner\"\r\n",
							"natural_key=\"BUSINESS_PARTNER_ID\"\r\n",
							"keycolumn=\"BUSINESS_PARTNER_KEY\"\r\n",
							"#columns1 = [\"BUS_PARTNER_NM\"]\r\n",
							"#columnsDF=spark.createDataFrame(incrementalData_DF)\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"#incrementalData_DF.show()\r\n",
							"#print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"#existingDataDF1.show()\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.BUSINESS_PARTNER_ID == existingDataDF1.existing_BUSINESS_PARTNER_ID, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"fullJoin2.show()\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get column list for creating Rec_Checksum\r\n",
							"\r\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\r\n",
							"# 2. Dups in target already\r\n",
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    #print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"#existingDataDF=spark.sql(\"SELECT * from $table_name tgt WHERE CURRENT_IND='Y'\")\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \r\n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\r\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND B.REC_CHECKSUM <> A.column_hash\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \r\n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\r\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND B.REC_CHECKSUM <> A.column_hash\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"qry3=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"df3=spark.sql(qry3)\r\n",
							"cnt1=df3.count()\r\n",
							"\r\n",
							"print (cnt1)\r\n",
							"if cnt1 == 0:\r\n",
							"    print(\"No Duplicates in source data\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates:\")\r\n",
							"    df3.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(1)\r\n",
							"    print(\"This will not be printed\")\r\n",
							"print (\"this will not be printed either\")\r\n",
							"\r\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\r\n",
							"#try:\r\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\r\n",
							"#except:\r\n",
							"#   status = 'fail'\r\n",
							"\r\n",
							"#assert status == 'success', 'status should be success'\r\n",
							"\r\n",
							"#a=spark.sql(qry)\r\n",
							"\r\n",
							"#print( df3. || ' Duplicate found ')\r\n",
							"\r\n",
							"#a.num_affected_rows\r\n",
							"#print(numOutputRows)\r\n",
							"\r\n",
							"\r\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\r\n",
							"\r\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\r\n",
							"\r\n",
							"\r\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\r\n",
							"\r\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\r\n",
							"\r\n",
							"#print(lastOperationDF.operationMetrics)\r\n",
							"\r\n",
							"#lastOperationDF.show()\r\n",
							"\r\n",
							"#fullHistoryDF.show()\r\n",
							"\r\n",
							"#print(num_affected_rows)\r\n",
							"\r\n",
							"#print(num_inserted_rows)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datalake_joins')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5588cce5-b44f-4914-9126-60a4673aebba"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import *\r\n",
							"\r\n",
							"#set the data lake file location:\r\n",
							"file_location = \"abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/sell_cycle_2022.csv\"\r\n",
							" \r\n",
							"#read in the data to dataframe df\r\n",
							"sellcyclesrcdf = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"delimiter\",\",\").load(file_location)\r\n",
							"#display the dataframe\r\n",
							"#display(sellcyclesrcdf)\r\n",
							"#sellcyclesrcdf.printSchema()\r\n",
							"\r\n",
							"\r\n",
							"sellcycletgtdf=spark.sql(\"SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME from etlhubConfirmed.dht_sell_cycle\")\r\n",
							"sellcycletgtdf.createOrReplaceTempView('sellcycletgtdfview')\r\n",
							"#sellcycletgtdf.show()\r\n",
							"\r\n",
							"#sellcyclefulljoindf=sellcycletgtdf.join(sellcyclesrcdf,sellcycletgtdf.SIEBEL_SALES_STAGE_CODE == sellcyclesrcdf.SIEBEL_SALES_STAGE_CODE, \"fullouter\") \r\n",
							"#sellcyclefulljoindf.show()\r\n",
							"#sellcyclesrcdf.SIEBEL_SALES_STAGE_CODE,sellcyclesrcdf.SIEBEL_SALES_STAGE_NAME,sellcyclesrcdf.SSM_STEP_NO,sellcyclesrcdf.SSM_STEP_NAME\r\n",
							"\r\n",
							"#sellcycletgtdf.join(sellcyclesrcdf,sellcycletgtdf.SIEBEL_SALES_STAGE_CODE == sellcyclesrcdf.SIEBEL_SALES_STAGE_CODE, \"fullouter\")\\\r\n",
							"#.select(sellcyclesrcdf.SIEBEL_SALES_STAGE_CODE,sellcyclesrcdf.SIEBEL_SALES_STAGE_NAME,sellcyclesrcdf.SSM_STEP_NO,sellcyclesrcdf.SSM_STEP_NAME)\\\r\n",
							"#.show(truncate=False)\r\n",
							"\r\n",
							"sellcycletgtdf.createOrReplaceTempView(\"tgt\")\r\n",
							"sellcyclesrcdf.createOrReplaceTempView(\"src\")\r\n",
							"\r\n",
							"joinDF = spark.sql(\"select t.SIEBEL_SALES_STAGE_CODE as SIEBEL_SALES_STAGE_CODE_tgt,t.SIEBEL_SALES_STAGE_NAME as SIEBEL_SALES_STAGE_NAME_tgt,t.SSM_STEP_NO as SSM_STEP_NO_tgt,t.SSM_STEP_NAME as SSM_STEP_NAME_tgt,s.SIEBEL_SALES_STAGE_CODE as SIEBEL_SALES_STAGE_CODE_src,s.SIEBEL_SALES_STAGE_NAME as SIEBEL_SALES_STAGE_NAME_src,s.SSM_STEP_NO as SSM_STEP_NO_src,s.SSM_STEP_NAME as SSM_STEP_NAME_src from tgt t FULL JOIN src s on t.SIEBEL_SALES_STAGE_CODE == s.SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							"#.show(truncate=False)\r\n",
							"joinDF.toDF\r\n",
							"joinDF.show()\r\n",
							"joinDF.createOrReplaceTempView(\"fulljoinDF\")\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--select SIEBEL_SALES_STAGE_CODE_src,SIEBEL_SALES_STAGE_NAME_src,SSM_STEP_NO_src,SSM_STEP_NAME_src from fulljoinDF\r\n",
							"--INSERT INTO etlhubConfirmed.sell_cycle select SIEBEL_SALES_STAGE_CODE_src,SIEBEL_SALES_STAGE_NAME_src,SSM_STEP_NO_src,SSM_STEP_NAME_src from fulljoinDF ;\r\n",
							"select * from etlhubConfirmed.dht_sell_cycle"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"update table etlhubConfirmed.sell_cycle set SIEBEL_SALES_STAGE_NAME = SIEBEL_SALES_STAGE_NAME_src\r\n",
							"where SIEBEL_SALES_STAGE_CODE_src = 1000"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"update etlhubConfirmed.dht_sell_cycle set SIEBEL_SALES_STAGE_NAME = '1200 newly added'\r\n",
							"where SIEBEL_SALES_STAGE_CODE = '1000'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/deltaLake_Customer_SCD_Type2_V1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5a9c270f-4324-4ba7-97ff-b254200d3fee"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"#import os\n",
							"import sys\n",
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'customer_data.csv' \n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) existing_MAX_KEY from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3)\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"print (\"this will not be printed either\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"#try:\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\n",
							"#except:\n",
							"#   status = 'fail'\n",
							"\n",
							"#assert status == 'success', 'status should be success'\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3. || ' Duplicate found ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"\n",
							"/*\n",
							"\n",
							"select * from incrementalData_DF2;\n",
							"\n",
							"select * from existingDataDF1;\n",
							"\n",
							"select * from fullJoin;\n",
							"*/\n",
							"\n",
							"--select * from incrementalData_DF2;\n",
							"\n",
							"--select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"SELECT 'All Rows' as Title, a.* FROM fullJoin a;\n",
							"\n",
							"\n",
							"--No change records, ignore --7ROWS\n",
							"select 'No Change Rows' as Title, a.*  from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert2' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert2' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--select * from etlhubConfirmed.customer_dimension where rec_checksum='c475b27f1b384e1d2289948edad59d84'\n",
							"/*\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;\n",
							"\n",
							"UPDATE etlhubConfirmed.customer_dimension\n",
							"SET GBG_ID='GB302S66'\n",
							"    ,REC_CHECKSUM='c475b27f1b384e1d2289948edad59d86'\n",
							"    WHERE CUSTOMER_KEY=5\n",
							"    ;\n",
							"\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;    \n",
							"*/\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"\n",
							"--select * from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"/*\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='N'\n",
							"    ,REC_END_DT=current_timestamp \n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"WHERE (CUSTOMER_NO ) =  (select CUSTOMER_NO from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum) )\n",
							"AND CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select existing_CUSTOMER_KEY,1+existing_VERSION as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert after insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert after insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.CUSTOMER_NO\n",
							"AND LOWER(B.CUSTOMER_NO) = LOWER(B.existing_CUSTOMER_NO) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"--Soft deletes or no longer active in source\n",
							"\n",
							"\n",
							"--SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.existing_CUSTOMER_NO\n",
							"AND B.CUSTOMER_NO is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"select 'Final rows in SCD Type2' as Title,a.* from etlhubconfirmed.customer_dimension a;\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension ;\n",
							"--CUSTOMER_NO='C0000216';\n",
							"--group by VERSION having count(*)>1;\n",
							"/*\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='Y'\n",
							"    WHERE CURRENT_IND='N' and VERSION=2.0;\n",
							"--DELETE FROM etlhubconfirmed.customer_dimension WHERE customer_key in (3.0,4.0);\n",
							"\n",
							"\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set FINANCIAL_COUNTRY_CD='896'\n",
							"    ,REC_CHECKSUM='3145dfee7cc94e4483b4b0c7244a9949'\n",
							"    WHERE customer_key=7.0;\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set GBG_ID='GB302S60'\n",
							"    ,customer_name='WALPOLE CO-OPERATIVE BANK1'\n",
							"    ,REC_CHECKSUM='0520612ce8718d5df3b8bb4b165a6548'\n",
							"    WHERE customer_key=8.0;\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET CURRENT_IND='N'\n",
							"    WHERE CUSTOMER_KEY=11.0;\n",
							"*/\n",
							"--select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"\n",
							"\n",
							"select * from fullJoin;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select customer_key,count(*) from etlhubconfirmed.customer_dimension where current_ind='Y' group by customer_key having count(*)>1;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/deltaLake_Project_SCD_Type2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dc62399a-be03-49ef-8d02-bceb41df069f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"#import os\n",
							"import sys\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"#Define Parameters\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'project' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"file_name='project_data.csv' \n",
							"natural_key=\"PROJECT_ID\"\n",
							"tablename=\"etlhubConfirmed.dht_project\"\n",
							"#natural_key=\"BUSINESS_PARTNER_ID\"\n",
							"keycolumn=\"PROJECT_KEY\"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"\n",
							"\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) existing_MAX_KEY from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"#existingMaxKeyDF.show()\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(natural_key)\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"#existingDataDF1.show()\n",
							"\n",
							"key1='existing_' + natural_key\n",
							"print(key1)\n",
							"\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.PROJECT_ID==existingDataDF1.existing_PROJECT_ID,\"fullouter\")\n",
							"var1=\"incrementalData_DF2.PROJECT_ID\"\n",
							"var2=\"existingDataDF1.existing_PROJECT_ID\"\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,({} == {}).format(var1,var2), \"fullouter\") \n",
							"\n",
							"#fullJoin1=incrementalData_DF2.join(existingDataDF1,{},'fullouter').format(natural_key)\n",
							"#firstDf.join(secondDf, [column], 'inner')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data validation Checks\n",
							"\n",
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3).format(natural_key,natural_key)\n",
							"df4=spark.sql(qry4).format(natural_key,tablename,natural_key)\n",
							"\n",
							"cnt1=df3.count()\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"\n",
							"if cnt2 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"#print (\"this will not be printed either\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"#try:\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\n",
							"#except:\n",
							"#   status = 'fail'\n",
							"\n",
							"#assert status == 'success', 'status should be success'\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3. || ' Duplicate found ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"\n",
							"/*\n",
							"\n",
							"select * from incrementalData_DF2;\n",
							"\n",
							"select * from existingDataDF1;\n",
							"\n",
							"select * from fullJoin;\n",
							"*/\n",
							"\n",
							"--select * from incrementalData_DF2;\n",
							"\n",
							"--select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"SELECT 'All Rows' as Title, a.* FROM fullJoin a;\n",
							"\n",
							"\n",
							"--No change records, ignore --7ROWS\n",
							"select 'No Change Rows' as Title, a.*  from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert2' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert2' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--select * from etlhubConfirmed.customer_dimension where rec_checksum='c475b27f1b384e1d2289948edad59d84'\n",
							"/*\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;\n",
							"\n",
							"UPDATE etlhubConfirmed.customer_dimension\n",
							"SET GBG_ID='GB302S66'\n",
							"    ,REC_CHECKSUM='c475b27f1b384e1d2289948edad59d86'\n",
							"    WHERE CUSTOMER_KEY=5\n",
							"    ;\n",
							"\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;    \n",
							"*/\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"\n",
							"--select * from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"/*\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='N'\n",
							"    ,REC_END_DT=current_timestamp \n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"WHERE (CUSTOMER_NO ) =  (select CUSTOMER_NO from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum) )\n",
							"AND CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select existing_CUSTOMER_KEY,1+existing_VERSION as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert after insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert after insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.CUSTOMER_NO\n",
							"AND LOWER(B.CUSTOMER_NO) = LOWER(B.existing_CUSTOMER_NO) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"--Soft deletes or no longer active in source\n",
							"\n",
							"\n",
							"--SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.existing_CUSTOMER_NO\n",
							"AND B.CUSTOMER_NO is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"select 'Final rows in SCD Type2' as Title,a.* from etlhubconfirmed.customer_dimension a;\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension ;\n",
							"--CUSTOMER_NO='C0000216';\n",
							"--group by VERSION having count(*)>1;\n",
							"/*\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='Y'\n",
							"    WHERE CURRENT_IND='N' and VERSION=2.0;\n",
							"--DELETE FROM etlhubconfirmed.customer_dimension WHERE customer_key in (3.0,4.0);\n",
							"\n",
							"\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set FINANCIAL_COUNTRY_CD='896'\n",
							"    ,REC_CHECKSUM='3145dfee7cc94e4483b4b0c7244a9949'\n",
							"    WHERE customer_key=7.0;\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set GBG_ID='GB302S60'\n",
							"    ,customer_name='WALPOLE CO-OPERATIVE BANK1'\n",
							"    ,REC_CHECKSUM='0520612ce8718d5df3b8bb4b165a6548'\n",
							"    WHERE customer_key=8.0;\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET CURRENT_IND='N'\n",
							"    WHERE CUSTOMER_KEY=11.0;\n",
							"*/\n",
							"--select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"\n",
							"\n",
							"select * from fullJoin;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select customer_key,count(*) from etlhubconfirmed.customer_dimension where current_ind='Y' group by customer_key having count(*)>1;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/deltaLake_SCD_Type2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "142d48b3-2f79-443e-a7b5-091984259f3d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'customer_data.csv' \n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"df3=spark.sql(qry1).count()\n",
							"\n",
							"a=spark.sql(qry)\n",
							"\n",
							"print(df3 || ' rows affected ')\n",
							"\n",
							"a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"print(lastOperationDF.operationMetrics)\n",
							"\n",
							"lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/proj_dataframe')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "24323e3d-22b0-46e0-91ba-c3af8a273514"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"dbtable\", \"DBXDH.DHT_PROJECT_SIV\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jdbcDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/python_update')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "da86b2f5-6733-4b59-8303-2b54d39642f0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pyodbc \r\n",
							"import sqlalchemy\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"import pyspark\r\n",
							"import sqlparse\r\n",
							"\r\n",
							"conn = pyodbc.connect('Driver={SQL Server};'\r\n",
							"                      'Server=sqlserver-kyn-001494-dev-eus-001.database.windows.net;'\r\n",
							"                      'Database=sqldb-etlhub-confirmed;'\r\n",
							"                      'user = sqladminuser;'\r\n",
							"                      'password = try2find$5;'\r\n",
							"                      )\r\n",
							"\r\n",
							"cursor = conn.cursor()\r\n",
							"cursor.execute('SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME FROM DBXDH.DHTS_SELL_CYCLE')\r\n",
							"\r\n",
							"for i in cursor:\r\n",
							"    print(i)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv?sp=r&st=2022-05-18T06:38:22Z&se=2022-05-18T14:38:22Z&spr=https&sv=2020-08-04&sr=b&sig=jctwRkZecjg%2Fz%2FlPtH1Z8FWGN0Ge4Rb84SLbNsBNCvs%3D\"\r\n",
							"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()),header = 0)\r\n",
							"#print(df)\r\n",
							"#print(df.columns)\r\n",
							"sparkDF=spark.createDataFrame(df) \r\n",
							"#sparkDF.columns\r\n",
							"sparkDF.createOrReplaceTempView('sell_cycle_df')\r\n",
							"#sparkDF.show()\r\n",
							"sellcycletgtDF.toDF\r\n",
							"#sellcycle.show()\r\n",
							"newAddressesToInsert = sparkDF \\\r\n",
							".alias(\"updates\") \\\r\n",
							".join(sellcycletgtDF.alias(\"sell_cycle\"), \"SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							".selectExpr(\"updates.SIEBEL_SALES_STAGE_CODE\",\"updates.SIEBEL_SALES_STAGE_NAME\",\"updates.SSM_STEP_NO\",\"updates.SSM_STEP_NAME\") \\\r\n",
							".where(\"sell_cycle.SIEBEL_SALES_STAGE_CODE = updates.SIEBEL_SALES_STAGE_CODE AND updates.SIEBEL_SALES_STAGE_NAME <> sell_cycle.SIEBEL_SALES_STAGE_NAME\")\r\n",
							"\r\n",
							"newAddressesToInsert.show()\r\n",
							"# Stage the update by unioning two sets of rows\r\n",
							"# 1. Rows that will be inserted in the whenNotMatched clause\r\n",
							"# 2. Rows that will either update the current addresses of existing customers or insert the new addresses of new customers\r\n",
							"stagedUpdates = (\r\n",
							"     newAddressesToInsert\r\n",
							"    .selectExpr(\"NULL as mergeKey\", \"*\")   # Rows for 1\r\n",
							"    .union(sparkDF.selectExpr(\"SIEBEL_SALES_STAGE_CODE as mergeKey\", \"*\"))\r\n",
							"    )\r\n",
							"#stagedUpdates.show()\r\n",
							"\r\n",
							"\r\n",
							"# Apply SCD Type 2 operation using merge\r\n",
							"DBXDH.DHTS_SELL_CYCLE.alias(\"sell_cycle\").merge(\r\n",
							"  stagedUpdates.alias(\"staged_updates\"),\r\n",
							"  \"sell_cycle.SIEBEL_SALES_STAGE_CODE = mergeKey\") \\\r\n",
							".whenMatchedUpdate(\r\n",
							"  condition = \"sell_cycle.SIEBEL_SALES_STAGE_CODE = staged_updates.SIEBEL_SALES_STAGE_CODE AND sell_cycle.SIEBEL_SALES_STAGE_NAME <> staged_updates.SIEBEL_SALES_STAGE_NAME\",\r\n",
							"  set = {                                      # Set current to false and endDate to source's effective date.\r\n",
							"   \"SIEBEL_SALES_STAGE_NAME\" : \"false\",\r\n",
							"   \"SSM_STEP_NO\" : \"false\"\r\n",
							"  }\r\n",
							"#).whenNotMatchedInsert(\r\n",
							"  #values = {\r\n",
							"    #\"SIEBEL_SALES_STAGE_CODE\": \"staged_updates.SIEBEL_SALES_STAGE_CODE\",\r\n",
							"    #\"SIEBEL_SALES_STAGE_NAME\": \"staged_updates.SIEBEL_SALES_STAGE_NAME\",\r\n",
							"    #\"SSM_STEP_NO\": \"staged_updates.SSM_STEP_NO\",\r\n",
							"    #\"SSM_STEP_NAME\": \"staged_updates.SSM_STEP_NAME\"  # Set current to true along with the new address and its effective date.\r\n",
							"    # }\r\n",
							").execute()\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 159
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sellcycletgtDF.show()\r\n",
							"#sparkDF.show()\r\n",
							"# Rows to INSERT new addresses of existing customers\r\n",
							"newAddressesToInsert = sparkDF \\\r\n",
							".alias(\"updates\") \\\r\n",
							".join(sellcycletgtDF.toDF().alias(\"sell_cycle\"), \"SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							".where(\"sell_cycle.SIEBEL_SALES_STAGE_CODE = true AND updates.SIEBEL_SALES_STAGE_NAME <> sell_cycle.SIEBEL_SALES_STAGE_NAME\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp11 (\r\n",
							"            [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO]  NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"            )\r\n",
							"    \"\"\")\r\n",
							"    # 2. Insert into temp table\r\n",
							"    sql_insert_products = f\"INSERT INTO #sellcycletemp11 VALUES (?,?,?,?)\"\r\n",
							"    con.execute(sql_insert_products, df.values.tolist())\r\n",
							"\r\n",
							"    # 3. Merge the #sellcycletemp with sellcycle\r\n",
							"    sql_merge = \"\"\"\r\n",
							"        MERGE DBXDH.DHTS_SELL_CYCLE  AS Target\r\n",
							"        USING #sellcycletemp11 AS Source\r\n",
							"            ON Source.SIEBEL_SALES_STAGE_CODE = Target.SIEBEL_SALES_STAGE_CODE\r\n",
							"        /* new records ('right match') */\r\n",
							"        WHEN NOT MATCHED BY Target THEN\r\n",
							"            INSERT ([SIEBEL_SALES_STAGE_CODE], [SIEBEL_SALES_STAGE_NAME], [SSM_STEP_NO], [SSM_STEP_NAME]) \r\n",
							"            VALUES (source.SIEBEL_SALES_STAGE_CODE, source.SIEBEL_SALES_STAGE_NAME, source.SSM_STEP_NO, source.SSM_STEP_NAME)\r\n",
							"        /* matching records ('inner match') */\r\n",
							"        WHEN MATCHED THEN \r\n",
							"            UPDATE SET\r\n",
							"            Target.SIEBEL_SALES_STAGE_NAME= Source.SIEBEL_SALES_STAGE_NAME\r\n",
							"            , Target.SSM_STEP_NO = Source.SSM_STEP_NO\r\n",
							"            , Target.SSM_STEP_NAME = Source.SSM_STEP_NAME\r\n",
							"        /* deprecated records ('left match') */\r\n",
							"        /* WHEN NOT MATCHED BY Source THEN\r\n",
							"    DELETE */\r\n",
							" \r\n",
							" OUTPUT \r\n",
							"\t$action\r\n",
							" , inserted.*\r\n",
							";\r\n",
							"    \"\"\"\r\n",
							"    con.execute(sql_merge)\r\n",
							"\r\n",
							"    # 4. Delete temporary table\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp5;\"\"\")\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp11;\"\"\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_dataframe')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7504924f-718e-4b87-942f-94b54fbbfd54"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"dbtable\", \"DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jdbcsqlDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"select * from DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jdbcsqlDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"select * from DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sellcycleDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"select * from DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF.show()\r\n",
							"sellcycleDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF.join(sellcycleDF,stgsellcycleDF.SIEBEL_SALES_STAGE_CODE ==  sellcycleDF.SIEBEL_SALES_STAGE_CODE,\"inner\") \\\r\n",
							"     .show(truncate=True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF.createOrReplaceTempView(\"stgsellcycle\")\r\n",
							"sellcycleDF.createOrReplaceTempView(\"sellcycle\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"joinDF = spark.sql(\"select * from stgsellcycle e, sellcycle d where e.SIEBEL_SALES_STAGE_CODE == d.SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							"  .show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"joinDF2 = spark.sql(\"select stg.* from stgsellcycle stg INNER JOIN sellcycle tgt ON stg.SIEBEL_SALES_STAGE_CODE == tgt.SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							"  .show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"joinDF2 = spark.sql(\"select stg.* from stgsellcycle stg INNER JOIN sellcycle tgt ON stg.SIEBEL_SALES_STAGE_CODE == tgt.SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							"  .show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"select SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME from DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sellcycleDF1 = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sellcycleDF1.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_df20220511')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "80a533ff-1e38-4ce6-83a4-f4ec6d52e944"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as existing_nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as existing_column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sellcycletgtDF.show()\r\n",
							"sellcycletgtDF.createOrReplaceTempView(\"sellcycletgt\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"conn = connect(':memory:')\r\n",
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle.csv?sp=r&st=2022-05-11T09:48:50Z&se=2022-05-11T17:48:50Z&spr=https&sv=2020-08-04&sr=c&sig=ScsDhYGM4HWdCD8kMSMmLfY7Pex8jvmc02LvGatfwPI%3D\"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\r\n",
							"#print(df)\r\n",
							"sell_cycleDF=spark.createDataFrame(df)\r\n",
							"col_list=[]\r\n",
							"for i in sell_cycleDF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    print (col_list)\r\n",
							"sell_cyclenkDF = sell_cycleDF.withColumn(\"nk_hash\",md5(\"SIEBEL_SALES_STAGE_CODE\"))\r\n",
							"sell_cyclecolhashDF = sell_cyclenkDF.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"sell_cyclecolhashDF.createOrReplaceTempView(\"sellcyclesrc\")\r\n",
							"#insertsellcycle=pd.read_sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME,a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a left join  sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\",conn)\r\n",
							"#insertsellcycledf=spark.createDataFrame(insertsellcycle)\r\n",
							"#insertsellcycledf.show()\r\n",
							"#%%sql\r\n",
							"#select * from sellcyclesrc"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col, desc\r\n",
							"(sell_cyclecolhashDF.select(\"SIEBEL_SALES_STAGE_CODE\",\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\")\r\n",
							" ).show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sell_cycleinsert = spark.sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME, a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a full outer join sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\") \\\r\n",
							"  .show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sell_cycleinsert.createOrReplaceTempView\r\n",
							"(\"sell_cycle_final\") \r\n",
							"#sqlContext.sql(\"insert into table DBXDH.DHTS_SELL_CYCLE select * from sell_cycleinsert\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"insertsellcycle=spark.createDataFrame(sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME,a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a left join  sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							" import sqlalchemy\r\n",
							" # 3.Insert table, merge\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp (\r\n",
							"             [nk_hash] NVARCHAR(100) UNIQUE\r\n",
							"            , [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"        )\r\n",
							"    \"\"\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_upsert_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0a6dbc01-0e2d-486b-9a4e-f1c7e4535654"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as existing_nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as existing_column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from sqlalchemy.engine import URL\r\n",
							"connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sqlserver-kyn-001494-dev-eus-001.database.windows.net;DATABASE=sqldb-etlhub-confirmed;UID=sqladminuser;PWD=try2find$5\"\r\n",
							"connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\r\n",
							"\r\n",
							"dbEngine = create_engine(connection_url)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp11 (\r\n",
							"            [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO]  NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"            )\r\n",
							"    \"\"\")\r\n",
							"    # 2. Insert into temp table\r\n",
							"    sql_insert_products = f\"INSERT INTO #sellcycletemp11 VALUES (?,?,?,?)\"\r\n",
							"    con.execute(sql_insert_products, df.values.tolist())\r\n",
							"\r\n",
							"    # 3. Merge the #sellcycletemp with sellcycle\r\n",
							"    sql_merge = \"\"\"\r\n",
							"        MERGE DBXDH.DHTS_SELL_CYCLE  AS Target\r\n",
							"        USING #sellcycletemp11 AS Source\r\n",
							"            ON Source.SIEBEL_SALES_STAGE_CODE = Target.SIEBEL_SALES_STAGE_CODE\r\n",
							"        /* new records ('right match') */\r\n",
							"        WHEN NOT MATCHED BY Target THEN\r\n",
							"            INSERT ([SIEBEL_SALES_STAGE_CODE], [SIEBEL_SALES_STAGE_NAME], [SSM_STEP_NO], [SSM_STEP_NAME]) \r\n",
							"            VALUES (source.SIEBEL_SALES_STAGE_CODE, source.SIEBEL_SALES_STAGE_NAME, source.SSM_STEP_NO, source.SSM_STEP_NAME)\r\n",
							"        /* matching records ('inner match') */\r\n",
							"        WHEN MATCHED THEN \r\n",
							"            UPDATE SET\r\n",
							"            Target.SIEBEL_SALES_STAGE_NAME= Source.SIEBEL_SALES_STAGE_NAME\r\n",
							"            , Target.SSM_STEP_NO = Source.SSM_STEP_NO\r\n",
							"            , Target.SSM_STEP_NAME = Source.SSM_STEP_NAME\r\n",
							"        /* deprecated records ('left match') */\r\n",
							"        /* WHEN NOT MATCHED BY Source THEN\r\n",
							"    DELETE */\r\n",
							" \r\n",
							" OUTPUT \r\n",
							"\t$action\r\n",
							" , inserted.*\r\n",
							";\r\n",
							"    \"\"\"\r\n",
							"    con.execute(sql_merge)\r\n",
							"\r\n",
							"    # 4. Delete temporary table\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp5;\"\"\")\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp11;\"\"\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_upsert_python')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8ed30c57-4f8c-44fa-bee2-ce8e2cdea06e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as existing_nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as existing_column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from sqlalchemy.engine import URL\r\n",
							"connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sqlserver-kyn-001494-dev-eus-001.database.windows.net;DATABASE=sqldb-etlhub-confirmed;UID=sqladminuser;PWD=try2find$5\"\r\n",
							"connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\r\n",
							"\r\n",
							"dbEngine = create_engine(connection_url)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"import csv\r\n",
							"\r\n",
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"def main():\r\n",
							"\r\n",
							"    # 1. Load the new products\r\n",
							"    df = pd.read_csv('project/sell_cycle_2022.csv', dtype={\r\n",
							"        'SIEBEL_SALES_STAGE_CODE': str,\r\n",
							"        'SIEBEL_SALES_STAGE_NAME': str,\r\n",
							"        'SSM_STEP_NO': str,\r\n",
							"        'SSM_STEP_NAME': str,\r\n",
							"        })"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp11 (\r\n",
							"            [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO]  NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"            )\r\n",
							"    \"\"\")\r\n",
							"    # 2. Insert into temp table\r\n",
							"    sql_insert_products = f\"INSERT INTO #sellcycletemp11 VALUES (?,?,?,?)\"\r\n",
							"    con.execute(sql_insert_products, df.values.tolist())\r\n",
							"\r\n",
							"    # 3. Merge the #sellcycletemp with sellcycle\r\n",
							"    sql_merge = \"\"\"\r\n",
							"        MERGE DBXDH.DHTS_SELL_CYCLE  AS Target\r\n",
							"        USING #sellcycletemp11 AS Source\r\n",
							"            ON Source.SIEBEL_SALES_STAGE_CODE = Target.SIEBEL_SALES_STAGE_CODE\r\n",
							"        /* new records ('right match') */\r\n",
							"        WHEN NOT MATCHED BY Target THEN\r\n",
							"            INSERT ([SIEBEL_SALES_STAGE_CODE], [SIEBEL_SALES_STAGE_NAME], [SSM_STEP_NO], [SSM_STEP_NAME]) \r\n",
							"            VALUES (source.SIEBEL_SALES_STAGE_CODE, source.SIEBEL_SALES_STAGE_NAME, source.SSM_STEP_NO, source.SSM_STEP_NAME)\r\n",
							"        /* matching records ('inner match') */\r\n",
							"        WHEN MATCHED THEN \r\n",
							"            UPDATE SET\r\n",
							"            Target.SIEBEL_SALES_STAGE_NAME= Source.SIEBEL_SALES_STAGE_NAME\r\n",
							"            , Target.SSM_STEP_NO = Source.SSM_STEP_NO\r\n",
							"            , Target.SSM_STEP_NAME = Source.SSM_STEP_NAME\r\n",
							"        /* deprecated records ('left match') */\r\n",
							"        /* WHEN NOT MATCHED BY Source THEN\r\n",
							"    DELETE */\r\n",
							" \r\n",
							" OUTPUT \r\n",
							"\t$action\r\n",
							" , inserted.*\r\n",
							";\r\n",
							"    \"\"\"\r\n",
							"    con.execute(sql_merge)\r\n",
							"\r\n",
							"    # 4. Delete temporary table\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp5;\"\"\")\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp11;\"\"\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_upsert_python_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dc8156f2-b57c-4665-bf0e-cc8508092b2a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 149
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv?sp=r&st=2022-05-18T06:38:22Z&se=2022-05-18T14:38:22Z&spr=https&sv=2020-08-04&sr=b&sig=jctwRkZecjg%2Fz%2FlPtH1Z8FWGN0Ge4Rb84SLbNsBNCvs%3D\"\r\n",
							"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()),header = 0)\r\n",
							"#print(df)\r\n",
							"#print(df.columns)\r\n",
							"sparkDF=spark.createDataFrame(df) \r\n",
							"#sparkDF.columns\r\n",
							"sparkDF.createOrReplaceTempView('sell_cycle_df')\r\n",
							"#sparkDF.show()\r\n",
							"sellcycletgtDF.toDF\r\n",
							"#sellcycle.show()\r\n",
							"newAddressesToInsert = sparkDF \\\r\n",
							".alias(\"updates\") \\\r\n",
							".join(sellcycletgtDF.alias(\"sell_cycle\"), \"SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							".selectExpr(\"updates.SIEBEL_SALES_STAGE_CODE\",\"updates.SIEBEL_SALES_STAGE_NAME\",\"updates.SSM_STEP_NO\",\"updates.SSM_STEP_NAME\") \\\r\n",
							".where(\"sell_cycle.SIEBEL_SALES_STAGE_CODE = updates.SIEBEL_SALES_STAGE_CODE AND updates.SIEBEL_SALES_STAGE_NAME <> sell_cycle.SIEBEL_SALES_STAGE_NAME\")\r\n",
							"\r\n",
							"newAddressesToInsert.show()\r\n",
							"# Stage the update by unioning two sets of rows\r\n",
							"# 1. Rows that will be inserted in the whenNotMatched clause\r\n",
							"# 2. Rows that will either update the current addresses of existing customers or insert the new addresses of new customers\r\n",
							"stagedUpdates = (\r\n",
							"     newAddressesToInsert\r\n",
							"    .selectExpr(\"NULL as mergeKey\", \"*\")   # Rows for 1\r\n",
							"    .union(sparkDF.selectExpr(\"SIEBEL_SALES_STAGE_CODE as mergeKey\", \"*\"))\r\n",
							"    )\r\n",
							"#stagedUpdates.show()\r\n",
							"\r\n",
							"\r\n",
							"# Apply SCD Type 2 operation using merge\r\n",
							"DBXDH.DHTS_SELL_CYCLE.alias(\"sell_cycle\").merge(\r\n",
							"  stagedUpdates.alias(\"staged_updates\"),\r\n",
							"  \"sell_cycle.SIEBEL_SALES_STAGE_CODE = mergeKey\") \\\r\n",
							".whenMatchedUpdate(\r\n",
							"  condition = \"sell_cycle.SIEBEL_SALES_STAGE_CODE = staged_updates.SIEBEL_SALES_STAGE_CODE AND sell_cycle.SIEBEL_SALES_STAGE_NAME <> staged_updates.SIEBEL_SALES_STAGE_NAME\",\r\n",
							"  set = {                                      # Set current to false and endDate to source's effective date.\r\n",
							"   \"SIEBEL_SALES_STAGE_NAME\" : \"false\",\r\n",
							"   \"SSM_STEP_NO\" : \"false\"\r\n",
							"  }\r\n",
							"#).whenNotMatchedInsert(\r\n",
							"  #values = {\r\n",
							"    #\"SIEBEL_SALES_STAGE_CODE\": \"staged_updates.SIEBEL_SALES_STAGE_CODE\",\r\n",
							"    #\"SIEBEL_SALES_STAGE_NAME\": \"staged_updates.SIEBEL_SALES_STAGE_NAME\",\r\n",
							"    #\"SSM_STEP_NO\": \"staged_updates.SSM_STEP_NO\",\r\n",
							"    #\"SSM_STEP_NAME\": \"staged_updates.SSM_STEP_NAME\"  # Set current to true along with the new address and its effective date.\r\n",
							"    # }\r\n",
							").execute()\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 159
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sellcycletgtDF.show()\r\n",
							"#sparkDF.show()\r\n",
							"# Rows to INSERT new addresses of existing customers\r\n",
							"newAddressesToInsert = sparkDF \\\r\n",
							".alias(\"updates\") \\\r\n",
							".join(sellcycletgtDF.toDF().alias(\"sell_cycle\"), \"SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							".where(\"sell_cycle.SIEBEL_SALES_STAGE_CODE = true AND updates.SIEBEL_SALES_STAGE_NAME <> sell_cycle.SIEBEL_SALES_STAGE_NAME\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp11 (\r\n",
							"            [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO]  NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"            )\r\n",
							"    \"\"\")\r\n",
							"    # 2. Insert into temp table\r\n",
							"    sql_insert_products = f\"INSERT INTO #sellcycletemp11 VALUES (?,?,?,?)\"\r\n",
							"    con.execute(sql_insert_products, df.values.tolist())\r\n",
							"\r\n",
							"    # 3. Merge the #sellcycletemp with sellcycle\r\n",
							"    sql_merge = \"\"\"\r\n",
							"        MERGE DBXDH.DHTS_SELL_CYCLE  AS Target\r\n",
							"        USING #sellcycletemp11 AS Source\r\n",
							"            ON Source.SIEBEL_SALES_STAGE_CODE = Target.SIEBEL_SALES_STAGE_CODE\r\n",
							"        /* new records ('right match') */\r\n",
							"        WHEN NOT MATCHED BY Target THEN\r\n",
							"            INSERT ([SIEBEL_SALES_STAGE_CODE], [SIEBEL_SALES_STAGE_NAME], [SSM_STEP_NO], [SSM_STEP_NAME]) \r\n",
							"            VALUES (source.SIEBEL_SALES_STAGE_CODE, source.SIEBEL_SALES_STAGE_NAME, source.SSM_STEP_NO, source.SSM_STEP_NAME)\r\n",
							"        /* matching records ('inner match') */\r\n",
							"        WHEN MATCHED THEN \r\n",
							"            UPDATE SET\r\n",
							"            Target.SIEBEL_SALES_STAGE_NAME= Source.SIEBEL_SALES_STAGE_NAME\r\n",
							"            , Target.SSM_STEP_NO = Source.SSM_STEP_NO\r\n",
							"            , Target.SSM_STEP_NAME = Source.SSM_STEP_NAME\r\n",
							"        /* deprecated records ('left match') */\r\n",
							"        /* WHEN NOT MATCHED BY Source THEN\r\n",
							"    DELETE */\r\n",
							" \r\n",
							" OUTPUT \r\n",
							"\t$action\r\n",
							" , inserted.*\r\n",
							";\r\n",
							"    \"\"\"\r\n",
							"    con.execute(sql_merge)\r\n",
							"\r\n",
							"    # 4. Delete temporary table\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp5;\"\"\")\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp11;\"\"\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_BPFACT_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a33f4bcd-8b71-4390-924e-80fc6ab5eed5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.functions import trim,ltrim,rtrim\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/BUSINESSPARTNER_FACT.csv\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'BUSINESSPARTNER_FACT.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.opportunity_business_partner_daily_ft\"\r\n",
							"natural_key=['BUSINESS_PARTNER_KEY','OPPORTUNITY_KEY','INFLUENCER_ROLE_CODE']\r\n",
							"keycolumn=\"BUSINESS_PARTNER_FACT_KEY\"\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"#joining with dimension tables\r\n",
							"DIMDataDF=spark.sql(\"SELECT OPPORTUNITY_KEY,OPPORTUNITY_VERSION,OPPORTUNITY_NUM FROM etlhubConfirmed.opportunity WHERE CURRENT_IND='Y'\")\r\n",
							"DIMDataDF1=spark.sql(\"SELECT BUSINESS_PARTNER_KEY,BUSINESS_PARTNER_VERSION,BUSINESS_PARTNER_ID FROM etlhubConfirmed.business_partner WHERE CURRENT_IND='Y'\")\r\n",
							"DIMJoin=incrementalData_DF.join(DIMDataDF,(trim(incrementalData_DF.OPPORTUNITY_NUM) == trim(DIMDataDF.OPPORTUNITY_NUM)), \"fullouter\") \r\n",
							"DIMJoin1=DIMJoin.join(DIMDataDF1,DIMJoin.BUSINESS_PARTNER_ID == DIMDataDF1.BUSINESS_PARTNER_ID, \"fullouter\") \r\n",
							"DIMJoin1.createOrReplaceTempView('fullJoin')\r\n",
							"DIMFinalJoinDF=spark.sql(\"select OPPORTUNITY_KEY,BUSINESS_PARTNER_KEY,INFLUENCER_ROLE_CODE from fulljoin\")\r\n",
							"#DIMFinalJoinDF.show()\r\n",
							"incrementalData_DF1 = DIMFinalJoinDF.withColumn(\"nk_hash\", md5(concat_ws(\"\", *natural_key)))\r\n",
							"col_list=[]\r\n",
							"for i in DIMFinalJoinDF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"# Add a checsum column to help identify the changed rows    \r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,((incrementalData_DF2.OPPORTUNITY_KEY == existingDataDF1.existing_OPPORTUNITY_KEY) & (incrementalData_DF2.BUSINESS_PARTNER_KEY == existingDataDF1.existing_BUSINESS_PARTNER_KEY) & (incrementalData_DF2.INFLUENCER_ROLE_CODE == existingDataDF1.existing_INFLUENCER_ROLE_CODE)), \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"#fullJoin2.show()\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"#fullJoin2.show()\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO {}\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_FACT_KEY,1 as BUSINESS_PARTNER_FACT_VERSION , \r\n",
							"A.OPPORTUNITY_KEY,A.BUSINESS_PARTNER_KEY,A.INFLUENCER_ROLE_CODE,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.opportunity_business_partner_daily_ft B\r\n",
							"ON A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.BUSINESS_PARTNER_KEY=B.BUSINESS_PARTNER_KEY AND A.INFLUENCER_ROLE_CODE=B.INFLUENCER_ROLE_CODE\r\n",
							"AND B.CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'')\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry.format(tablename))\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"INSERT INTO {} \r\n",
							"select existing_BUSINESS_PARTNER_FACT_KEY,1+BUSINESS_PARTNER_FACT_VERSION as BUSINESS_PARTNER_FACT_VERSION , \r\n",
							"A.OPPORTUNITY_KEY,A.BUSINESS_PARTNER_KEY,A.INFLUENCER_ROLE_CODE,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT ,\r\n",
							"'LG' AS DATA_IND, 'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE OPPORTUNITY_KEY = existing_OPPORTUNITY_KEY AND BUSINESS_PARTNER_KEY=existing_BUSINESS_PARTNER_KEY AND TRIM(INFLUENCER_ROLE_CODE)=TRIM(existing_INFLUENCER_ROLE_CODE) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.opportunity_business_partner_daily_ft B\r\n",
							"WHERE A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.BUSINESS_PARTNER_KEY=B.BUSINESS_PARTNER_KEY AND A.INFLUENCER_ROLE_CODE=B.INFLUENCER_ROLE_CODE\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND COALESCE(A.column_hash,'')=COALESCE(B.REC_CHECKSUM,''))\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry1.format(tablename))\r\n",
							"\r\n",
							"qry2=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON --A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.BUSINESS_PARTNER_KEY=B.BUSINESS_PARTNER_KEY AND A.INFLUENCER_ROLE_CODE=B.INFLUENCER_ROLE_CODE\r\n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry2.format(tablename))\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"qry3=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON OPPORTUNITY_KEY = existing_OPPORTUNITY_KEY AND BUSINESS_PARTNER_KEY=existing_BUSINESS_PARTNER_KEY AND TRIM(INFLUENCER_ROLE_CODE)=TRIM(existing_INFLUENCER_ROLE_CODE)\r\n",
							"AND B.BUSINESS_PARTNER_FACT_KEY is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\r\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry3.format(tablename))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.opportunity_business_partner_daily_ft"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyspark.sql.functions as f\r\n",
							"natural_key=[BUSINESS_PARTNER_KEY,OPPORTUNITY_KEY,INFLUENCER_ROLE_CODE]\r\n",
							"logical_gate = f.lit()\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"existingDataDF1.show()\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.OPPORTUNITY_NUM == existingDataDF1.existing_OPPORTUNITY_NUM, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"#fullJoin2.show()\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO {}\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_KEY,1 as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.opportunity B\r\n",
							"ON A.OPPORTUNITY_NUM=B.OPPORTUNITY_NUM\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'')\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry.format(tablename))\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"INSERT INTO {} \r\n",
							"select existing_OPPORTUNITY_KEY,1+existing_OPPORTUNITY_VERSION as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT ,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,\r\n",
							"'LG' AS DATA_IND, 'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE LOWER(OPPORTUNITY_NUM) = LOWER(existing_OPPORTUNITY_NUM) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.opportunity B\r\n",
							"WHERE A.OPPORTUNITY_NUM=B.OPPORTUNITY_NUM\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND A.column_hash=B.REC_CHECKSUM)\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry1.format(tablename))\r\n",
							"\r\n",
							"qry2=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.OPPORTUNITY_NUM = B.OPPORTUNITY_NUM\r\n",
							"AND LOWER(B.OPPORTUNITY_NUM) = LOWER(B.existing_OPPORTUNITY_NUM) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry2.format(tablename))\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"qry3=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.OPPORTUNITY_NUM = B.existing_OPPORTUNITY_NUM\r\n",
							"AND B.OPPORTUNITY_NUM is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\r\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry3.format(tablename))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.opportunity where OPPORTUNITY_NUM = '00-0JWAXDR'\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--INSERTS OR NEW ROWS\r\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_KEY1, A.* from fullJoin A WHERE existing_BUSINESS_PARTNER_KEY is null;\r\n",
							"--UPDATE ROWS\r\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(BUSINESS_PARTNER_ID) = LOWER(existing_BUSINESS_PARTNER_ID) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_BP_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "56ac7253-7a46-47ec-b49f-f3cd1ec994f1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/BUSPARTNER_DIM.csv\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'BUSPARTNER_DIM_UPD.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.business_partner\"\r\n",
							"natural_key=\"BUSINESS_PARTNER_ID\"\r\n",
							"keycolumn=\"BUSINESS_PARTNER_KEY\"\r\n",
							"#columns1 = [\"BUS_PARTNER_NM\"]\r\n",
							"#columnsDF=spark.createDataFrame(incrementalData_DF)\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"#incrementalData_DF.show()\r\n",
							"#print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"#existingDataDF1.show()\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.BUSINESS_PARTNER_ID == existingDataDF1.existing_BUSINESS_PARTNER_ID, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"#fullJoin2.show()\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO {}\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_KEY,1 as BUSINESS_PARTNER_VERSION , \r\n",
							"A.BUSINESS_PARTNER_ID,A.BUS_PARTNER_NM,'' as AGR_BUS_PARTNER_ID,'' as AGR_BUS_PARTNER_NM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'BMSIW' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.BUSINESS_PARTNER B\r\n",
							"ON A.BUSINESS_PARTNER_ID=B.BUSINESS_PARTNER_ID\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'')\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry.format(tablename))\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"INSERT INTO {} \r\n",
							"select existing_BUSINESS_PARTNER_KEY,1+existing_BUSINESS_PARTNER_VERSION as BUSINESS_PARTNER_VERSION , \r\n",
							"A.BUSINESS_PARTNER_ID,A.BUS_PARTNER_NM,'' as AGR_BUS_PARTNER_ID,'' as AGR_BUS_PARTNER_NM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'BMSIW' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT , 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE LOWER(BUSINESS_PARTNER_ID) = LOWER(existing_BUSINESS_PARTNER_ID) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.BUSINESS_PARTNER B\r\n",
							"WHERE A.BUSINESS_PARTNER_ID=B.BUSINESS_PARTNER_ID\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND A.column_hash=B.REC_CHECKSUM)\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry1.format(tablename))\r\n",
							"\r\n",
							"qry2=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.BUSINESS_PARTNER_ID = B.BUSINESS_PARTNER_ID\r\n",
							"AND LOWER(B.BUSINESS_PARTNER_ID) = LOWER(B.existing_BUSINESS_PARTNER_ID) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry2.format(tablename))\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"qry3=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.BUSINESS_PARTNER_ID = B.existing_BUSINESS_PARTNER_ID\r\n",
							"AND B.BUSINESS_PARTNER_ID is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\r\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry3.format(tablename))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--INSERTS OR NEW ROWS\r\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_KEY1, A.* from fullJoin A WHERE existing_BUSINESS_PARTNER_KEY is null;\r\n",
							"--UPDATE ROWS\r\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(BUSINESS_PARTNER_ID) = LOWER(existing_BUSINESS_PARTNER_ID) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"select * from etlhubConfirmed.BUSINESS_PARTNER\r\n",
							"where current_ind = 'N'    --BUSINESS_PARTNER_ID = 'RAUE000167'\r\n",
							"--update etlhubConfirmed.BUSINESS_PARTNER\r\n",
							"--set BUS_PARTNER_NM = 'Updated with Azure'\r\n",
							"--where BUSINESS_PARTNER_ID = 'SUSE087302'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_OPPORTUNITY_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "231a7467-f64c-45d1-9193-7483f607d81e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/OPPORTUNITY.csv\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'OPPORTUNITY_UPD.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.opportunity\"\r\n",
							"natural_key=\"OPPORTUNITY_NUM\"\r\n",
							"keycolumn=\"OPPORTUNITY_KEY\"\r\n",
							"\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"#incrementalData_DF.show()\r\n",
							"#print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"#existingDataDF1.show()\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.OPPORTUNITY_NUM == existingDataDF1.existing_OPPORTUNITY_NUM, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"#fullJoin2.show()\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO {}\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_KEY,1 as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.opportunity B\r\n",
							"ON A.OPPORTUNITY_NUM=B.OPPORTUNITY_NUM\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'')\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry.format(tablename))\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"INSERT INTO {} \r\n",
							"select existing_OPPORTUNITY_KEY,1+existing_OPPORTUNITY_VERSION as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT ,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,\r\n",
							"'LG' AS DATA_IND, 'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE LOWER(OPPORTUNITY_NUM) = LOWER(existing_OPPORTUNITY_NUM) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.opportunity B\r\n",
							"WHERE A.OPPORTUNITY_NUM=B.OPPORTUNITY_NUM\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND A.column_hash=B.REC_CHECKSUM)\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry1.format(tablename))\r\n",
							"\r\n",
							"qry2=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.OPPORTUNITY_NUM = B.OPPORTUNITY_NUM\r\n",
							"AND LOWER(B.OPPORTUNITY_NUM) = LOWER(B.existing_OPPORTUNITY_NUM) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry2.format(tablename))\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"qry3=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.OPPORTUNITY_NUM = B.existing_OPPORTUNITY_NUM\r\n",
							"AND B.OPPORTUNITY_NUM is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\r\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry3.format(tablename))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.opportunity where OPPORTUNITY_NUM = '00-0JWAXDR'\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--INSERTS OR NEW ROWS\r\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_KEY1, A.* from fullJoin A WHERE existing_BUSINESS_PARTNER_KEY is null;\r\n",
							"--UPDATE ROWS\r\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(BUSINESS_PARTNER_ID) = LOWER(existing_BUSINESS_PARTNER_ID) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_deltalake_Customer_Dimension')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7b237b58-edc8-4cb6-a618-04e511f508bd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"#import os\n",
							"import sys\n",
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'customer_data.csv' \n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) existing_MAX_KEY from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3)\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"print (\"this will not be printed either\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"#try:\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\n",
							"#except:\n",
							"#   status = 'fail'\n",
							"\n",
							"#assert status == 'success', 'status should be success'\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3. || ' Duplicate found ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "741bc7ae-a6b3-4c2f-9c88-e8060c51b585"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'customer_data.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.customer_dimension\"\r\n",
							"natural_key=\"CUSTOMER_NO\"\r\n",
							"keycolumn=\"CUSTOMER_KEY\"\r\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\r\n",
							"\r\n",
							"# Get column list for creating Rec_Checksum\r\n",
							"\r\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\r\n",
							"# 2. Dups in target already\r\n",
							"\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    #print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"#existingDataDF=spark.sql(\"SELECT * from $table_name tgt WHERE CURRENT_IND='Y'\")\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \r\n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\r\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND B.REC_CHECKSUM <> A.column_hash\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \r\n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\r\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND B.REC_CHECKSUM <> A.column_hash\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"qry3=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"df3=spark.sql(qry3)\r\n",
							"cnt1=df3.count()\r\n",
							"\r\n",
							"print (cnt1)\r\n",
							"if cnt1 == 0:\r\n",
							"    print(\"No Duplicates in source data\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates:\")\r\n",
							"    df3.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(1)\r\n",
							"    print(\"This will not be printed\")\r\n",
							"print (\"this will not be printed either\")\r\n",
							"\r\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\r\n",
							"#try:\r\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\r\n",
							"#except:\r\n",
							"#   status = 'fail'\r\n",
							"\r\n",
							"#assert status == 'success', 'status should be success'\r\n",
							"\r\n",
							"#a=spark.sql(qry)\r\n",
							"\r\n",
							"#print( df3. || ' Duplicate found ')\r\n",
							"\r\n",
							"#a.num_affected_rows\r\n",
							"#print(numOutputRows)\r\n",
							"\r\n",
							"\r\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\r\n",
							"\r\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\r\n",
							"\r\n",
							"\r\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\r\n",
							"\r\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\r\n",
							"\r\n",
							"#print(lastOperationDF.operationMetrics)\r\n",
							"\r\n",
							"#lastOperationDF.show()\r\n",
							"\r\n",
							"#fullHistoryDF.show()\r\n",
							"\r\n",
							"#print(num_affected_rows)\r\n",
							"\r\n",
							"#print(num_inserted_rows)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sPoolKyn001494')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 30
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsqlpoolKyn001494DevEtlHubEUS001')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}